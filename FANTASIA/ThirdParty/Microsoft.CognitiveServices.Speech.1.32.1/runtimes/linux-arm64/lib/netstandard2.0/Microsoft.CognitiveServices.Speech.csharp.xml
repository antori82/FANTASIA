<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.CognitiveServices.Speech.csharp</name>
    </assembly>
    <members>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioConfig">
            <summary>
            Represents audio input or output configuration. Audio input can be from a microphone, file, or input stream. Audio output can be to a speaker, audio file output in WAV format, or output stream.
            </summary>
              
            <remarks>
            See also: [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text?tabs=script%2Cwindowsinstall&amp;pivots=programming-language-csharp)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultMicrophoneInput">
            <summary>
            Creates an AudioConfig object that receives speech from the default microphone on the computer.
            </summary>
            <remarks>
            See also: [Recognize from microphone](/azure/cognitive-services/speech-service/get-started-speech-to-text#recognize-from-microphone)
            </remarks>
            <returns>An audio input configuration object set to provide speech from the default microphone.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultMicrophoneInput(Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions)">
            <summary>
            Creates an AudioConfig object that receives speech from the default microphone on the computer.
            </summary>
            <remarks>
            See also: [Recognize from microphone](/azure/cognitive-services/speech-service/get-started-speech-to-text#recognize-from-microphone)
            </remarks>
            <param name="audioProcessingOptions">Specifies the audio processing options.</param>
            <returns>An audio input configuration object set to provide speech from the default microphone.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromWavFileInput(System.String)">
            <summary>
            Creates an AudioConfig object that receives speech from an audio file in WAV format.
            </summary>
            <remarks>
            See also: [Continuous recognition](/azure/cognitive-services/speech-service/get-started-speech-to-text#continuous-recognition)
            </remarks>
            <param name="fileName">Specifies the audio input file.</param>
            <returns>An audio input configuration object set to provide speech from a .WAV file.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromWavFileInput(System.String,Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions)">
            <summary>
            Creates an AudioConfig object that receives speech from an audio file in WAV format.
            </summary>
            <remarks>
            See also: [Continuous recognition](/azure/cognitive-services/speech-service/get-started-speech-to-text#continuous-recognition)
            </remarks>
            <param name="fileName">Specifies the audio input file.</param>
            <param name="audioProcessingOptions">Specifies the audio processing options.</param>
            <returns>An audio input configuration object set to provide speech from a .WAV file.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.AudioInputStream)">
            <summary>
            Creates an AudioConfig object that receives speech from a stream.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="audioStream">Specifies the custom audio input stream.</param>
            <returns>An audio input configuration object set to provide speech from a stream.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.AudioInputStream,Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions)">
            <summary>
            Creates an AudioConfig object that receives speech from a stream.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="audioStream">Specifies the custom audio input stream.</param>
            <param name="audioProcessingOptions">Specifies the audio processing options.</param>
            <returns>An audio input configuration object set to provide speech from a stream.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback)">
            <summary>
            Creates an AudioConfig object that receives speech from a stream using an audio input stream callback.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="callback">Specifies the pull audio input stream callback.</param>
            <returns>An audio input configuration object set to provide speech from a stream using a callback.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions)">
            <summary>
            Creates an AudioConfig object that receives speech from a stream using an audio input stream callback.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="callback">Specifies the pull audio input stream callback.</param>
            <param name="audioProcessingOptions">Specifies the audio processing options.</param>
            <returns>An audio input configuration object set to provide speech from a stream using a callback.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates an AudioConfig object that receives speech from a stream in a specified format using an audio input stream callback.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="callback">Specifies the pull audio input stream callback.</param>
            <param name="format">Data format of audio provided to the stream.</param>
            <returns>An audio input configuration object set to provide speech from a stream using a callback.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamInput(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat,Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions)">
            <summary>
            Creates an AudioConfig object that receives speech from a stream in a specified format using an audio input stream callback.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="callback">Specifies the pull audio input stream callback.</param>
            <param name="format">Data format of audio provided to the stream.</param>
            <param name="audioProcessingOptions">Specifies the audio processing options.</param>
            <returns>An audio input configuration object set to provide speech from a stream using a callback.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromMicrophoneInput(System.String)">
            <summary>
            Creates an AudioConfig object that receives speech from a specific microphone on the computer. Added in 1.3.0
            </summary>
            <param name="deviceName">Specifies the device name. To retrieve platform-specific microphone names, see [How to: Select an audio input device with the Speech SDK](/azure/cognitive-services/speech-service/how-to-select-audio-input-devices).</param>
            <returns>An audio input configuration object set to provide speech from a particular microphone.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromMicrophoneInput(System.String,Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions)">
            <summary>
            Creates an AudioConfig object that receives speech from a specific microphone on the computer.
            </summary>
            <param name="deviceName">Specifies the device name. To retrieve platform-specific microphone names, see [How to: Select an audio input device with the Speech SDK](/azure/cognitive-services/speech-service/how-to-select-audio-input-devices).</param>
            <param name="audioProcessingOptions">Specifies the audio processing options.</param>
            <returns>An audio input configuration object set to provide speech from a particular microphone.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultSpeakerOutput">
            <summary>
            Creates an AudioConfig object that produces speech on the computer's default speaker. Added in 1.4.0
            </summary>
            <returns>An audio output configuration object set to provide speech to the default device speaker.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromSpeakerOutput(System.String)">
            <summary>
            Creates an AudioConfig object that produces speech to to the specified speaker.
            Added in 1.14.0
            </summary>
            <param name="deviceName">Specifies the device name. To retrieve platform-specific audio device names, see [How to: Select an audio input device with the Speech SDK](/azure/cognitive-services/speech-service/how-to-select-audio-input-devices).</param>
            <returns>An audio output configuration object set to provide speech to a particular device speaker.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromWavFileOutput(System.String)">
            <summary>
            Creates an AudioConfig object that produces speech to the specified WAV file.
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Synthesize speech to a file](/azure/cognitive-services/speech-service/get-started-text-to-speech#synthesize-speech-to-a-file)
            </remarks>
            <param name="fileName">Specifies the audio output file. The parent directory must already exist.</param>
            <returns>An audio output configuration object set to provide speech to a .WAV file.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamOutput(Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Added in 1.4.0
            </summary>
            <param name="audioStream">Specifies the custom audio output stream.</param>
            <returns>An audio output configuration object.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromStreamOutput(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback)">
            <summary>
            Creates an AudioConfig object representing the specified stream.
            Added in 1.4.0
            </summary>
            <param name="callback">Specifies the push audio output stream callback.</param>
            <returns>An audio output configuration object.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.AudioProcessingOptions">
            <summary>
            Audio processing options used by Speech SDK.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.Dispose">
            <summary>
            Dispose of associated resources held by the object.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.SetProperty(System.String,System.String)">
            <summary>
            Sets a property using a string name.
            Added in 1.10.0
            </summary>
            <remarks>
            A small number of properties are stored using string names.
            In most cases, you will use <see cref="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)"/> instead of this method.
            </remarks>
            <param name="name">Name of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Sets a property using a PropertyId value.
            Added in 1.10.0
            </summary>
            <param name="id">PropertyId of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.GetProperty(System.String)">
            <summary>
            Searches for the property that has the given string name.
            Added in 1.10.0
            </summary>
            <remarks>
            A small number of properties are stored using string names.
            In most cases, you will use <see cref="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)"/> instead of this method.
            </remarks>
            <param name="name">Name of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Searches for the property named with the given PropertyId value.
            Added in 1.10.0
            </summary>
            <param name="id">PropertyId of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream">
            <summary>
            Represents audio input stream used for custom audio input configurations.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePushStream">
            <summary>
            Creates a memory backed PushAudioInputStream using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <returns>The push audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePushStream(Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a memory backed PushAudioInputStream with the specified audio format.
            </summary>
            <param name="format">The data format of the audio to be written to the push audio stream's write() method.</param>
            <returns>The push audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePullStream(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods, using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback</param>
            <returns>The pull audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.CreatePullStream(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods.
            </summary>
            <remarks>
            See also: [Asynchronous Conversation Transcription](/azure/cognitive-services/speech-service/how-to-async-conversation-transcription?pivots=programming-language-csharp)
            </remarks>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback.</param>
            <param name="format">The data format of the audio returned from the callback's read() method.</param>
            <returns>The pull audio input stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioInputStream.isDisposing">
            <summary>
            Indicates whether the object is currently being disposed.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream">
            <summary>
            Represents memory backed push audio input stream used for custom audio input configurations.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.#ctor">
            <summary>
            Creates a memory backed PushAudioInputStream using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a memory backed PushAudioInputStream with the specified audio format.
            </summary>
            <param name="format">The data format of the audio written to the push audio stream's write() method.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Write(System.Byte[])">
            <summary>
            Writes the audio data specified by making an internal copy of the data.
            Note: The dataBuffer must not contain an audio header.
            </summary>
            <param name="dataBuffer">The audio buffer this method will copy.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Write(System.Byte[],System.Int32)">
            <summary>
            Writes the audio data specified by making an internal copy of the data.
            </summary>
            <param name="dataBuffer">The audio buffer this method will copy.</param>
            <param name="size">The size of the data in the audio buffer. Note the size could be smaller than dataBuffer.Length</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Set value of a property associated to data buffer. 
            The properties of the audio data must be set before writing the audio data.
            Added in 1.5.0
            </summary>
            <param name="id">A property identifier.</param>
            <param name="value">The value of the property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.SetProperty(System.String,System.String)">
            <summary>
            Set value of a property associated to data buffer. 
            The properties of the audio data mustt be set before writing the audio data.
            Added in 1.5.0
            </summary>
            <param name="name">The name of the property.</param>
            <param name="value">The value of the property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Close">
            <summary>
            Closes the stream.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioInputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream">
            <summary>
            Represents audio input stream used for custom audio input configurations.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods using the default format (16 kHz, 16 bit, mono PCM).
            </summary>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback,Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat)">
            <summary>
            Creates a PullAudioInputStream that delegates to the specified callback interface for read() and close() methods.
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="callback">The custom audio input object, derived from PullAudioInputStreamCallback.</param>
            <param name="format">The data format of the audio returned from the callback's read() method.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback">
            <summary>
            Defines callback methods (Read() and Close()) for custom audio input streams).
            </summary>
            <remarks>
            See also: [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Read(System.Byte[],System.UInt32)">
            <summary>
            Reads binary data from the stream.
            Note: The dataBuffer returned by Read() cannot contain any audio header.
            </summary>
            <param name="dataBuffer">The buffer to fill</param>
            <param name="size">The size of the buffer.</param>
            <returns>The number of bytes filled, or 0 in case the stream hits its end and there is no more data available.
            If there is no data immediately available, Read() blocks until the next data becomes available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
             <summary>
             Gets a property of the data buffer. If the property isn’t found, an empty string is returned.
             Added in 1.5.0
             </summary>
             <remarks>
             Use these properties to get details about this data buffer:
            
             * PropertyId.ConversationTranscribingService_DataBufferTimeStamp
             * PropertyId.ConversationTranscribingService_DataBufferUserId
             </remarks>
             <param name="id">A property id.</param>
             <returns>The value of the property </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Close">
            <summary>
            Closes the audio input stream.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioInputStreamCallback.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream">
            <summary>
            Represents audio output stream used for custom audio output configurations.
            Updated in 1.7.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.CreatePullStream">
            <summary>
            Creates a memory backed PullAudioOutputStream.
            </summary>
            <returns>The pull audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.CreatePushStream(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback)">
            <summary>
            Creates a PushAudioOutputStream that delegates to the specified callback interface for write() and close() methods.
            </summary>
            <remarks>
            See also: [Recognize from in-memory stream](/azure/cognitive-services/speech-service/get-started-speech-to-text#recognize-from-in-memory-stream)
            </remarks>
            <param name="callback">The custom audio output object, derived from PushAudioOutputStreamCallback</param>
            <returns>The push audio output stream being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioOutputStream.isDisposing">
            <summary>
            Indicates whether the object is currently being disposed.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream">
            <summary>
            Represents memory backed pull audio output stream used for custom audio output configurations.
            Updated in 1.7.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.#ctor">
            <summary>
            Creates a memory backed PullAudioOutputStream.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.Read(System.Byte[])">
            <summary>
            Read audio from the stream.
            The maximal number of bytes to be read is determined by the size of dataBuffer.
            If there is no data immediately available, read() blocks until the next data becomes available.
            </summary>
            <param name="buffer">The buffer to receive the audio data</param>
            <returns>The number of bytes filled, or 0 in case the stream hits its end and there is no more data available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream">
            <summary>
            Represents audio output stream used for custom audio output configurations.
            Updated in 1.7.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream.#ctor(Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback)">
            <summary>
            Creates a PushAudioOutputStream that delegates to the specified callback interface for write() and close() methods.
            </summary>
            <param name="callback">The custom audio output object, derived from PushAudioOutputStreamCallback.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback">
            <summary>
            Abstract base class that defines callback methods (Write() and Close()) for custom audio output streams).
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Recognize from in-memory stream](/azure/cognitive-services/speech-service/get-started-speech-to-text#recognize-from-in-memory-stream)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Write(System.Byte[])">
            <summary>
            Writes binary data to the stream.
            </summary>
            <param name="dataBuffer">The buffer containing the data to be written</param>
            <returns>The number of bytes written.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Close">
            <summary>
            Closes the audio output stream.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.PushAudioOutputStreamCallback.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry">
            <summary>
            Types of preset microphone array geometries.
            Check https://docs.microsoft.com/azure/cognitive-services/speech-service/speech-devices-sdk-microphone for details.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Uninitialized">
            <summary>
            Indicates that no geometry specified. Speech SDK will determine the microphone array geometry.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Circular7">
            <summary>
            Indicates a microphone array with one microphone in the center and six microphones evenly spaced
            in a circle with radius approximately equal to 42.5 mm.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Circular4">
            <summary>
            Indicates a microphone array with one microphone in the center and three microphones evenly spaced
            in a circle with radius approximately equal to 42.5 mm.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Linear4">
            <summary>
            Indicates a microphone array with four linearly placed microphones with 40 mm spacing between them.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Linear2">
            <summary>
            Indicates a microphone array with two linearly placed microphones with 40 mm spacing between them.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Mono">
            <summary>
            Indicates a microphone array with a single microphone.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry.Custom">
            <summary>
            Indicates a microphone array with custom geometry.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayType">
            <summary>
            Types of microphone arrays.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayType.Linear">
            <summary>
            Indicates that the microphone array has microphones in a straight line.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayType.Planar">
            <summary>
            Indicates that the microphone array has microphones in a plane.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.SpeakerReferenceChannel">
            <summary>
            Defines speaker reference channel position in input audio.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.SpeakerReferenceChannel.None">
            <summary>
            Indicates that the input audio does not have a speaker reference channel.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.SpeakerReferenceChannel.LastChannel">
            <summary>
            Indicates that the last channel in the input audio corresponds to the speaker
            reference for echo cancellation.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates">
            <summary>
            Represents coordinates for a microphone.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates.X">
            <summary>
            X-coordinate of the microphone.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates.Y">
            <summary>
            Y-coordinate of the microphone.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates.Z">
            <summary>
            Z-coordinate of the microphone.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates.#ctor(System.Int32,System.Int32,System.Int32)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="x">Specifies the signed X-coordinate of the microphone, in millimeters.</param>
            <param name="y">Specifies the signed Y-coordinate of the microphone, in millimeters.</param>
            <param name="z">Specifies the signed Z-coordinate of the microphone, in millimeters.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry">
            <summary>
            Represents the geometry of a microphone array.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry.MicrophoneArrayType">
            <summary>
            Type of microphone array.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry.BeamformingStartAngle">
            <summary>
            Start angle for beamforming in degrees.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry.BeamformingEndAngle">
            <summary>
            End angle for beamforming in degrees.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry.MicrophoneCoordinates">
            <summary>
            Coordinates of microphones in the microphone array.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry.#ctor(Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayType,Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates[])">
            <summary>
            Creates a new instance of MicrophoneArrayGeometry.
            Beamforming start angle is set to zero.
            Beamforming end angle is set to 180 degrees if microphoneArrayType is Linear, otherwise it is set to 360 degrees.
            </summary>
            <param name="microphoneArrayType">Type of microphone array.</param>
            <param name="microphoneCoordinates">Coordinates of microphones in the microphone array.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry.#ctor(Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayType,System.UInt16,System.UInt16,Microsoft.CognitiveServices.Speech.Audio.MicrophoneCoordinates[])">
            <summary>
            Creates a new instance of MicrophoneArrayGeometry.
            </summary>
            <param name="microphoneArrayType">Type of microphone array.</param>
            <param name="beamformingStartAngle">Start angle for beamforming in degrees.</param>
            <param name="beamformingEndAngle">End angle for beamforming in degrees.</param>
            <param name="microphoneCoordinates">Coordinates of microphones in the microphone array.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants">
            <summary>
            Represents audio input processing constants used for specifying the processing in AudioProcessingOptions.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_NONE">
            <summary>
            Disables built-in input audio processing.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_ENABLE_DEFAULT">
            <summary>
            Enables default built-in input audio processing.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_DISABLE_DEREVERBERATION">
            <summary>
            Disables dereverberation in the audio processing pipeline.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_DISABLE_NOISE_SUPPRESSION">
            <summary>
            Disables noise suppression in the audio processing pipeline.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_DISABLE_GAIN_CONTROL">
            <summary>
            Disables automatic gain control in the audio processing pipeline.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_DISABLE_ECHO_CANCELLATION">
            <summary>
            Disables echo cancellation in the audio processing pipeline.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingConstants.AUDIO_INPUT_PROCESSING_ENABLE_VOICE_ACTIVITY_DETECTION">
            <summary>
            Enable voice activity detection in input audio processing.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions">
            <summary>
            Represents audio processing options used with audio config class.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.Create(System.Int32)">
            <summary>
            Creates an AudioProcessingOptions object with audio processing flags.
            </summary>
            <remarks>
            This method should only be used when the audio input is from a microphone array.
            On Windows, this method will try to query the microphone array geometry from the audio driver. Audio data is also read from speaker reference channel.
            On Linux, it assumes that the microphone is a single channel microphone.
            </remarks>
            <param name="audioProcessingFlags">Specifies flags to control the audio processing performed by Speech SDK. It is bitwise OR of constants from AudioProcessingConstants class.</param>
            <returns>The audio processing options object being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.Create(System.Int32,Microsoft.CognitiveServices.Speech.Audio.PresetMicrophoneArrayGeometry,Microsoft.CognitiveServices.Speech.Audio.SpeakerReferenceChannel)">
            <summary>
            Creates an AudioProcessingOptions object with audio processing flags, preset microphone array geometry and speaker reference channel position.
            </summary>
            <param name="audioProcessingFlags">Specifies flags to control the audio processing performed by Speech SDK. It is bitwise OR of constants from AudioProcessingConstants class.</param>
            <param name="microphoneArrayGeometry">Specifies the type of microphone array geometry.</param>
            <param name="speakerReferenceChannel">Specifies the speaker reference channel position in the input audio.</param>
            <returns>The audio processing options object being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.Create(System.Int32,Microsoft.CognitiveServices.Speech.Audio.MicrophoneArrayGeometry,Microsoft.CognitiveServices.Speech.Audio.SpeakerReferenceChannel)">
            <summary>
            Creates an AudioProcessingOptions object with audio processing flags, custom microphone array geometry and speaker reference channel position.
            </summary>
            <param name="audioProcessingFlags">Specifies flags to control the audio processing performed by Speech SDK. It is bitwise OR of constants from AudioProcessingConstants class.</param>
            <param name="microphoneArrayGeometry">Specifies the microphone array geometry.</param>
            <param name="speakerReferenceChannel">Specifies the speaker reference channel position in the input audio.</param>
            <returns>The audio processing options object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.AudioProcessingFlags">
            <summary>
            The type of audio processing performed by Speech SDK.
            Bitwise OR of flags from AudioProcessingConstants class indicating the audio processing performed by Speech SDK.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.PresetMicrophoneArrayGeometry">
            <summary>
            Type of microphone array geometry of the microphone used for audio input.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.MicrophoneArrayType">
            <summary>
            The microphone array type of the microphone used for audio input.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.BeamformingStartAngle">
            <summary>
            Start angle used for beamforming in degrees.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.BeamformingEndAngle">
            <summary>
            End angle used for beamforming in degrees.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.MicrophoneCoordinates">
            <summary>
            The coordinates of microphones in the microphone array used for audio input.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.SpeakerReferenceChannel">
            <summary>
            Speaker reference channel position in the audio input.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioProcessingOptions.Dispose">
            <summary>
            Dispose of associated resources held by the object.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat">
            <summary>
            Supported audio input container formats.
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Use codec compressed audio input with the Speech SDK](/azure/cognitive-services/speech-service/how-to-use-codec-compressed-audio-input-streams?tabs=debian)
            </remarks>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.OGG_OPUS">
            <summary>
            Audio stream container format definition for OGG OPUS.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.MP3">
            <summary>
            Audio stream container format definition for MP3.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.FLAC">
            <summary>
            Audio stream container format definition for FLAC. Added in 1.7.0 
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.ALAW">
            <summary>
            Audio stream container format definition for ALAW. Added in 1.7.0 
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.MULAW">
            <summary>
            Audio stream container format definition for MULAW. Added in 1.7.0 
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.AMRNB">
            <summary>
            Audio stream container format definition for AMRNB. Currently not supported. 
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.AMRWB">
            <summary>
            Audio stream container format definition for AMRWB. Currently not supported. 
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat.ANY">
            <summary>
            Audio stream container format definition for any other or unknown format.  
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioStreamWaveFormat">
            <summary>
            Represents the format specified inside WAV container which are sent directly as encoded to the speech service.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamWaveFormat.PCM">
            <summary>
            AudioStreamWaveFormat definition for PCM (pulse-code modulated) data in integer format.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamWaveFormat.ALAW">
            <summary>
            AudioStreamWaveFormat definition A-law-encoded format.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Audio.AudioStreamWaveFormat.MULAW">
            <summary>
            AudioStreamWaveFormat definition for Mu-law-encoded format.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat">
            <summary>
            Represents audio stream format used for custom audio input configurations.
            Updated in 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetDefaultInputFormat">
            <summary>
            Creates an audio stream format object that represents the default microphone input format (16 kHz, 16 bit, mono PCM).
            </summary>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetDefaultOutputFormat">
            <summary>
            Creates an audio stream format object that represents the default speaker output format (16 kHz, 16 bit, mono PCM).
            Added in 1.4.0
            </summary>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetWaveFormat(System.UInt32,System.Byte,System.Byte,Microsoft.CognitiveServices.Speech.Audio.AudioStreamWaveFormat)">
            <summary>
            Creates an audio stream format object with the specified waveformat characteristics.
            </summary>
            <remarks>
            See also: 
            * [Recognize from in-memory stream](/azure/cognitive-services/speech-service/get-started-speech-to-text#recognize-from-in-memory-stream)
            * [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="samplesPerSecond">Sample rate, in samples per second (Hertz).</param>
            <param name="bitsPerSample">Bits per sample.</param>
            <param name="channels">Number of channels in the waveform-audio data.</param>
            <param name="waveFormat">The format specified inside the WAV container.</param>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetWaveFormatPCM(System.UInt32,System.Byte,System.Byte)">
            <summary>
            Creates an audio stream format object with the specified PCM waveformat characteristics.
            </summary>
            <remarks>
            See also: 
            * [Recognize from in-memory stream](/azure/cognitive-services/speech-service/get-started-speech-to-text#recognize-from-in-memory-stream)
            * [About the Speech SDK audio input stream API](/azure/cognitive-services/speech-service/how-to-use-audio-input-streams)
            </remarks>
            <param name="samplesPerSecond">Sample rate, in samples per second (Hertz).</param>
            <param name="bitsPerSample">Bits per sample.</param>
            <param name="channels">Number of channels in the waveform-audio data.</param>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.GetCompressedFormat(Microsoft.CognitiveServices.Speech.Audio.AudioStreamContainerFormat)">
            <summary>
            Creates an audio stream format object that specifies which compressed audio container format the input uses.
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Example code using codec compressed audio input](/azure/cognitive-services/speech-service/how-to-use-codec-compressed-audio-input-streams#example-code-using-codec-compressed-audio-input)
            </remarks>
            <param name="compressedFormat">Formats are defined in AudioStreamContainerFormat enum</param>
            <returns>The audio stream format being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Audio.AudioStreamFormat.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.AudioDataStream">
            <summary>
            Provides audio data as a stream.
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Get result as an in-memory stream](/azure/cognitive-services/speech-service/get-started-text-to-speech#get-result-as-an-in-memory-stream)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.FromWavFileInput(System.String)">
            <summary>
            Creates a memory-backed audio data stream using the specified audio file.
            </summary>
            <param name="fileName">Specifies the audio input file.</param>
            <returns>An audio data stream based on the audio file.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.FromResult(Microsoft.CognitiveServices.Speech.SpeechSynthesisResult)">
            <summary>
            Creates a memory-backed audio data stream using the specified synthesized speech.
            </summary>
            <remarks>
            See also: [Get result as an in-memory stream](/azure/cognitive-services/speech-service/get-started-text-to-speech#get-result-as-an-in-memory-stream)
            </remarks>
            <param name="result">The speech synthesis result.</param>
            <returns>An audio data stream.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.FromResult(Microsoft.CognitiveServices.Speech.KeywordRecognitionResult)">
            <summary>
            Gets the memory-backed audio data stream associated with a given keyword recognition result, set to the moment before the keyword was spoken.
            </summary>
            <param name="result">The keyword recognition result.</param>
            <returns>An audio stream of speech positioned to the moment before the keyword utterance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.GetStatus">
            <summary>
            Get current status of the audio data stream.
            </summary>
            <returns>Current status.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.CanReadData(System.UInt32)">
            <summary>
            Indicates whether the audio data stream can read the specified number of bytes, starting from the current position of the stream.
            </summary>
            <param name="bytesRequested">The requested data size, in bytes.</param>
            <returns>A bool indicating whether the specified number of bytes can be read from the stream.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.CanReadData(System.UInt32,System.UInt32)">
            <summary>
            Indicates whether the audio data stream can read the specified number of bytes, starting from a position offset from the first byte.
            </summary>
            <param name="pos">The offset from the start from the of the stream where the request starts, in bytes.</param>
            <param name="bytesRequested">The requested data size, in bytes.</param>
            <returns>A bool indicating whether the specified number of bytes can be read from the stream.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.ReadData(System.Byte[])">
            <summary>
            Reads audio data from the current position in the audio data stream. Waits if no data is available.
            </summary>
            <remarks>
            The maximum number of bytes to be read is determined by the size of buffer.
            </remarks>
            <param name="buffer">The buffer to receive the audio data.</param>
            <returns>The number of bytes provided, or 0 when the stream ends and there is no more data available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.ReadData(System.UInt32,System.Byte[])">
            <summary>
            Reads audio data from a specified position in the audio data stream. Waits if no data is available.
            </summary>
            <remarks>
            The maximum number of bytes to be read is determined by the size of buffer.
            </remarks>
            <param name="pos">The offset from the start from the of the stream where the request starts, in bytes.</param>
            <param name="buffer">The buffer to receive the audio data.</param>
            <returns>The number of bytes provided, or 0 when the stream ends and there is no more data available.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.SaveToWaveFileAsync(System.String)">
            <summary>
            Saves the audio data to a file as an asynchronous operation.
            </summary>
            <remarks>
            See also: [Customize audio format](/azure/cognitive-services/speech-service/get-started-text-to-speech#customize-audio-format)
            </remarks>
            <param name="fileName">Name of the file.</param>
            <returns>An asynchronous operation that writes to the file.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.SetPosition(System.UInt32)">
            <summary>
            Sets current position of the audio data stream.
            </summary>
            <param name="pos">New offset position from the start of the stream.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.GetPosition">
            <summary>
            Get current position of the audio data stream.
            </summary>
            <returns>Current position.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.DetachInput">
            <summary>
            Stops any more data from reaching the stream.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AudioDataStream.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.AudioDataStream.Properties">
            <summary>
            Contains properties of the audio data stream.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig">
            <summary>
            Configures options for automatic detection of languages.
            Updated in 1.13.0
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig.FromOpenRange">
            <summary>
            Creates an instance of AutoDetectSourceLanguageConfig that can detect any language.
            Added in 1.13.0
            </summary>
            <remarks>
            Only <see cref="T:Microsoft.CognitiveServices.Speech.SpeechSynthesizer"/> can detect the source language automatically.
            An object inherited from <see cref="T:Microsoft.CognitiveServices.Speech.Recognizer"/> must specify possible source languages.
              
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection)
            </remarks>
            <returns>A new AutoDetectSourceLanguageConfig instance that can detect any language known to the Azure Speech service.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig.FromLanguages(System.String[])">
            <summary>
            Creates an instance of AutoDetectSourceLanguageConfig that specifies possible source languages.
            </summary>
            <param name="languages">List of source languages.</param>
            <returns>A new AutoDetectSourceLanguageConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig.FromSourceLanguageConfigs(Microsoft.CognitiveServices.Speech.SourceLanguageConfig[])">
            <summary>
            Creates an instance of AutoDetectSourceLanguageConfig that specifies possible source languages.
            </summary>
            <param name="sourceLanguageConfigs">List of source languages.</param>
            <returns>A new AutoDetectSourceLanguageConfig instance.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageResult">
            <summary>
            Contains languages detected by the Speech service.
            Added in 1.9.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageResult.Language">
            <summary>
            Detected language
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageResult.FromResult(Microsoft.CognitiveServices.Speech.SpeechRecognitionResult)">
            <summary>
            Creates an instance of AutoDetectSourceLanguageResult object from a speech recognition result.
            </summary>
            <param name="result">The speech recongition result.</param>
            <returns>A new AutoDetectSourceLanguageResult instance</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageResult.#ctor(Microsoft.CognitiveServices.Speech.RecognitionResult)">
            <summary>
            Creates an instance of AutoDetectSourceLanguageResult for any recognition result.
            </summary>
            <param name="result">The recongition result.</param>
            <returns>A new AutoDetectSourceLanguageResult instance</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Connection">
            <summary>
            A proxy class for managing connection to the speech service of the specified Recognizer. Added in 1.2.0
            </summary>
            <remarks>
            By default, a Recognizer autonomously manages connection to service when needed.
            The Connection class provides additional methods you can use to explicitly open or close a connection and
            to subscribe to connection status changes.
            The use of Connection is optional. It is intended for scenarios where fine tuning of application
            behavior based on connection status is needed. You can optionally call Open() to manually
            initiate a service connection before starting recognition on the Recognizer associated with this Connection.
            After starting a recognition, calling Open() or Close() might fail. This will not impact
            the Recognizer or the ongoing recognition. Connection might drop for various reasons, the Recognizer will
            always try to reinstitute the connection as required to guarantee ongoing operations. In all these cases
            Connected/Disconnected events will indicate the change of the connection status.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer)">
            <summary>
            Gets the Connection instance from the specified recognizer.
            </summary>
            <param name="recognizer">The recognizer associated with the connection.</param>
            <returns>The Connection instance of the recognizer.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FromConversationTranslator(Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator)">
            <summary>
            Gets the Connection instance from the conversation translator.
            </summary>
            <param name="convTrans">The conversation translator associated with the connection.</param>
            <returns>The Connection instance of the conversation translator.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FromDialogServiceConnector(Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector)">
            <summary>
            Gets the Connection instance from the specified dialog service connector, used for observing and managing
            connection and disconnection from the speech service.
            </summary>
            <param name="dialogServiceConnector">The dialog service connector associated with the connection.</param>
            <returns>The Connection instance of the dialog service connector.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FromSpeechSynthesizer(Microsoft.CognitiveServices.Speech.SpeechSynthesizer)">
            <summary>
            Gets the Connection instance from the specified speech synthesizer, used for observing and managing
            connection and disconnection from the speech service.
            Added in version 1.17.0
            </summary>
            <param name="speechSynthesizer">The speech synthesizer associated with the connection.</param>
            <returns>The Connection instance of the speech synthesizer.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.Open(System.Boolean)">
            <summary>
            Starts to set up connection to the service.
            You can optionally call Open() to manually set up a connection in advance before starting recognition on the
            Recognizer associated with this Connection. After starting recognition, calling Open() might fail, depending on
            the process state of the Recognizer. But the failure does not affect the state of the associated Recognizer.
            Note: On return, the connection might not be ready yet. Please subscribe to the Connected event to
            be notified when the connection is established.
            </summary>
            <param name="forContinuousRecognition">Indicates whether the connection is used for continuous recognition or single-shot recognition.  It takes no effect if the connection is from SpeechSynthsizer.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.Close">
            <summary>
            Closes the connection the service.
            You can optionally call Close() to manually shutdown the connection of the associated Recognizer.
            The call might fail, depending on the process state of the Recognizer. But the failure does not affect the state of the
            associated Recognizer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.SendMessageAsync(System.String,System.String)">
            <summary>
            Sends a message to the Speech service as an asynchronous operation.
            Note: This method doesn't work for the connection of SpeechSynthesizer.
            Added in 1.7.0
            </summary>
            <param name="path">The path of the message.</param>
            <param name="payload">The payload of the message. This is a JSON string.</param>
            <returns>A task representing the asynchronous operation that sends the message.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.SendMessageAsync(System.String,System.Byte[],System.UInt32)">
            <summary>
            Sends a binary message to the speech service as an asynchronous operation.
            Note: This method doesn't work for the connection of SpeechSynthesizer.
            Added in 1.10.0
            </summary>
            <param name="path">The path of the message.</param>
            <param name="payload">The binary payload of the message.</param>
            <param name="size">The size of the binary payload.</param>
            <returns>A task representing the asynchronous operation that sends the message.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.SetMessageProperty(System.String,System.String,System.String)">
            <summary>
            Appends a parameter in a message to service.
            Note: This method doesn't work for the connection of SpeechSynthesizer.
            Added in 1.7.0
            </summary>
            <param name="path">The path of the network message.</param>
            <param name="propertyName">Name of the property.</param>
            <param name="propertyValue">Value of the property. This is a JSON string.</param>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Connection.Connected">
            <summary>
            The Connected event to indicate that the recognizer is connected to service.
            In order to receive the Connected event after subscribing to it, the Connection object itself needs to be alive.
            If the Connection object owning this event is out of its life time, all subscribed events won't be delivered.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Connection.Disconnected">
            <summary>
            The Disconnected event to indicate that the recognizer is disconnected from service.
            In order to receive the Disconnected event after subscribing to it, the Connection object itself needs to be alive.
            If the Connection object owning this event is out of its life time, all subscribed events won't be delivered.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Connection.MessageReceived">
            <summary>
            The MessageReceived event indicates that the service has sent a network message to the client.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Connection.FireEvent_Connected(System.IntPtr,System.IntPtr)">
             <summary>
             Provides private methods that raise a C# event when a corresponding callback is invoked from the native layer.
             </summary>
            
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ConnectionEventArgs">
            <summary>
            Contains payload for Connected/Disconnected events
            Added in 1.2.0
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ConnectionEventType">
            <summary>
            Lists connection event types.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ConnectionMessage">
            <summary>
            Represents implementation-specific messages sent to and received from
            the speech service. For debugging only.  Added in 1.10.0
            </summary>
            <remarks>
            These messages are provided for debugging purposes and should not
            be used for production use cases with the Azure Cognitive Services Speech Service.
            Messages sent to and received from the Speech Service are subject to change without
            notice. This includes message contents, headers, payloads, ordering, etc.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.ConnectionMessage.Path">
            <summary>
            A string that specifies the message type.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ConnectionMessage.IsTextMessage">
            <summary>
            Checks to see if the ConnectionMessage is a text message.
            See also IsBinaryMessage().
            </summary>
            <returns>A bool indicating if the message payload is text.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ConnectionMessage.IsBinaryMessage">
            <summary>
            Checks to see if the ConnectionMessage is a binary message.
            See also GetBinaryMessage().
            </summary>
            <returns>A bool indicating if the message payload is binary.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ConnectionMessage.GetTextMessage">
            <summary>
            Gets the text message payload. Typically the text message content-type is
            application/json. To determine other content-types use
            Properties.GetProperty("Content-Type").
            </summary>
            <returns>A string containing the text message.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ConnectionMessage.GetBinaryMessage">
            <summary>
            Gets the binary message payload.
            </summary>
            <returns>An array of bytes containing the binary message.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.ConnectionMessage.Properties">
            <summary>
            A collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.ConnectionMessage"/>.
            Message headers can be accessed via this collection (e.g. "Content-Type").
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ConnectionMessage.ToString">
            <summary>
            Returns a string that represents the connection message.
            </summary>
            <returns>A string that represents the connection message.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ConnectionMessageEventArgs">
            <summary>
            Contains payload for MessageReceived events of a Connection instance.
            Added in 1.10.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.ConnectionMessageEventArgs.Message">
            <summary>
            Gets the <see cref="T:Microsoft.CognitiveServices.Speech.ConnectionMessage"/> associated with this <see cref="T:Microsoft.CognitiveServices.Speech.ConnectionMessageEventArgs"/>.
            </summary>
            <returns>A ConnectionMessage containing the message.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ConnectionMessageEventArgs.ToString">
            <summary>
            Returns a string that represents the connection message event.
            </summary>
            <returns>A string that represents the connection message event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.Conversation">
            <summary>
            Transcribes conversations from speech. Returns recognized text and speaker id.
            Added in 1.8.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.CreateConversationAsync(Microsoft.CognitiveServices.Speech.SpeechConfig,System.String)">
            <summary>
            Creates a new conversation asynchronously.
            </summary>
            <remarks>
            After a conversation is created by this asynchronous operation, the <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.Conversation"/>.ConversationId contains the conversation identifier.
            </remarks>
            <param name="speechConfig">The speech configuration to use.</param>
            <param name="conversationId">(Optional) The identifier for the conversation you want to join.</param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Conversation.ConversationId">
            <summary>
            Gets or sets the conversation id.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Conversation.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Conversation.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Conversation.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Conversation.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber"/>.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Conversation.NativeHandle">
            <summary>
            Gets the native handle for the conversation.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.AddParticipantAsync(System.String)">
            <summary>
            Add a participant to a conversation using the user's id as an asynchronous operation.
            </summary>
            <remarks>
            The returned participants can be used to remove. If the client changes the participant's attributes,
            the changed attributes are passed on to the service only when the participants is added again.
            </remarks>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.AddParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.User)">
            <summary>
            Add a participant to a conversation using the User object asynchronously.
            </summary>
            <param name="user">A User object.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.AddParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.Participant)">
            <summary>
            Add a participant to a conversation using the Participant object asynchronously.
            </summary>
            <param name="participant">A participant object.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.RemoveParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.Participant)">
            <summary>
            Remove a participant in a conversation using the Participant object asynchronously.
            </summary>
            <param name="participant">A Participant object.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.RemoveParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.User)">
            <summary>
            Remove a participant in a conversation using the User object asynchronously.
            </summary>
            <param name="user">A User object.</param>
            <returns>An asynchronous operation representing removing a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.RemoveParticipantAsync(System.String)">
            <summary>
            Remove a participant from a conversation using a user identifier asynchronously.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing removing a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.EndConversationAsync">
            <summary>
            End a conversation.
            </summary>
            <returns>An asynchronous operation representing ending a conversation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.StartConversationAsync">
            <summary>
            Start a conversation.
            </summary>
            <returns>An asynchronous operation representing starting a conversation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.DeleteConversationAsync">
            <summary>
            Delete a conversation. After this no one will be able to join the conversation.
            </summary>
            <returns>An asynchronous operation representing deleting a conversation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.LockConversationAsync">
            <summary>
            Lock a conversation. This will prevent new participants from joining.
            </summary>
            <returns>An asynchronous operation representing locking a conversation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.UnlockConversationAsync">
            <summary>
            Unlocks a conversation.
            </summary>
            <returns>An asynchronous operation representing unlocking a conversation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.MuteAllParticipantsAsync">
            <summary>
            Mute all other participants in the conversation. After this no other participants will
            have their speech recognitions broadcast, nor be able to send text messages.
            </summary>
            <returns>An asynchronous operation representing muting all participants.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.UnmuteAllParticipantsAsync">
            <summary>
            Unmute all other participants in the conversation.
            </summary>
            <returns>An asynchronous operation representing un-muting all participants.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.MuteParticipantAsync(System.String)">
            <summary>
            Mute a participant.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing muting a particular participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.UnmuteParticipantAsync(System.String)">
            <summary>
            Unmute a participant.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing un-muting a particular participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.Dispose(System.Boolean)">
            <summary>
            
            </summary>
            <param name="disposeManaged"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationExpirationEventArgs">
            <summary>
            Contains a payload for the <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.ConversationExpiration"/> event.
            Added in 1.9.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationExpirationEventArgs.#ctor(System.IntPtr)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="eventHandle">The event handle</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationExpirationEventArgs.ExpirationTime">
            <summary>
            How much longer until the conversation expires.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ParticipantChangedReason">
            <summary>
            Why the participant changed event was raised
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Transcription.ParticipantChangedReason.JoinedConversation">
            <summary>
            Participant has joined the conversation.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Transcription.ParticipantChangedReason.LeftConversation">
            <summary>
            Participant has left the conversation. This could be voluntary, or involuntary
            (e.g. they are experiencing networking issues).
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Transcription.ParticipantChangedReason.Updated">
            <summary>
            The participants' state has changed (e.g. they became muted, changed their nickname).
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationParticipantsChangedEventArgs">
            <summary>
            Contains a payload for the <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.ParticipantsChanged" /> event.
            Added in 1.9.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationParticipantsChangedEventArgs.#ctor(System.IntPtr)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="handle">The event handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationParticipantsChangedEventArgs.Reason">
            <summary>
            Why the participant changed event was raised (e.g. a participant joined).
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Transcription.ConversationParticipantsChangedEventArgs.Participants">
            <summary>
            The participant(s) that joined, left, or were updated.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber">
            <summary>
            Transcribes conversations from speech into text.
            </summary>
            <remarks>
            See also: [Get started with real-time Conversation Transcription](/azure/cognitive-services/speech-service/how-to-use-conversation-transcription?pivots=programming-language-csharp)
            </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Transcribing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Transcribing"/> signals that an intermediate transcription result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Transcribed">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Transcribed"/> signals that a final transcription result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Canceled"/> signals that the transcription was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of Conversation Transcriber.
            <param name="speechConfig">Speech configuration</param>
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.SourceLanguageConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="sourceLanguageConfig">The source language config</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.SourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="sourceLanguageConfig">Language of the source speech, in BCP-47 format.</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber that determines the source language from a list of options.
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration that specifies the language(s) to look for in the source speech to recognize</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of ConversationTranscriber.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">An instance that specifies possible source languages in the speech.</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber"/>.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.StartTranscribingAsync">
            <summary>
            Starts conversation trancsribing on a continuous audio stream, until StopTranscribingAsync() is called.
            You must subscribe to events to receive recognition results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.StopTranscribingAsync">
            <summary>
            Stops conversation transcribing.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
            <remarks>This is used to pause the conversation. The client can start the conversation again by calling StartTranscribingAsync.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriber.Dispose(System.Boolean)">
            <summary>
            Disposes of the object
            </summary>
            <param name="disposing">True to dispose managed resources</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionResult">
            <summary>
            Contains the result of conversation transcriber.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionResult.#ctor(System.IntPtr)">
            <summary>
            Constructor to create ConversationTranscriptionResult
            </summary>
            <param name="resultPtr">The result handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionResult.SpeakerId">
            <summary>
            A string that represents the speaker identifier.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionResult.ToString">
            <summary>
            Returns a string that represents the conversation transcription result.
            </summary>
            <returns>A string that represents the conversation transcription result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionEventArgs">
            <summary>
            Contains conversation transcriber event arguments.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionEventArgs.Result">
            <summary>
            Represents the conversation transcription result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the conversation transcription result event.
            </summary>
            <returns>A string that represents the conversation transcription event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs">
            <summary>
            Contains payload of conversation transcriber canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs.Reason">
            <summary>
            The reason the result was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranscriptionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the conversation transcription result event.
            </summary>
            <returns>A string that represents the conversation transcriber event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationEventArgs">
            <summary>
            Contains a payload for <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribing"/>, and
            <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribed"/> events.
            Added in 1.9.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationEventArgs.#ctor(System.IntPtr)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="handle">The event handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationEventArgs.Result">
            <summary>
            Gets the conversation translation result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationEventArgs.ToString">
            <summary>
            Returns a string that represents the conversation translation event.
            </summary>
            <returns>The string representation.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs">
            <summary>
            Contains a payload for the <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Canceled"/> event.
            Added in 1.9.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs.#ctor(System.IntPtr)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="handle">The event handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationResult">
            <summary>
            Contains a conversation translation result.
            Added in 1.9.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationResult.#ctor(System.IntPtr)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="resultPtr">The result handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationResult.ParticipantId">
            <summary>
            The unique identifier for the participant this result is for.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationResult.OriginalLang">
            <summary>
            The original language this result was in.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslationResult.ToString">
            <summary>
            Returns a string that represents the conversation translation result.
            </summary>
            <returns>A string representation.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator">
            <summary>
            Creates a speech or text conversation where participants
            can see recognized speech and typed messages in their own languages.
            </summary>
            <remarks>
            See also: [Quickstart: Multi-device Conversation](/azure/cognitive-services/speech-service/quickstarts/multi-device-conversation)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.#ctor">
            <summary>
            Creates a new instance of the Conversation Translator using the default microphone input.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.#ctor(Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of the Conversation Translator.
            </summary>
            <param name="audioConfig">Audio configuration.</param>
            <exception cref="T:System.ArgumentNullException">If the audioConfig is null.</exception>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.#ctor(Microsoft.CognitiveServices.Speech.Internal.InteropSafeHandle)">
            <summary>
            Creates a new instance.
            </summary>
            <param name="nativeHandle">The handle to the native conversation translator.</param>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.SessionStarted">
            <summary>
            Event that signals the start of a conversation translation session.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.SessionStopped">
            <summary>
            Event that signals the end of a conversation translation session.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Canceled">
            <summary>
            Event that signals an error with the conversation transcription, or the end of the
            audio stream has been reached.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.ParticipantsChanged">
            <summary>
            Event that signals participants in the room have changed (e.g. a new participant joined).
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.ConversationExpiration">
            <summary>
            Event that signals how many more minutes are left before the conversation expires.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribing">
            <summary>
            Event that signals an intermediate conversation translation result is available for a
            conversation participant.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribed">
            <summary>
            Event that signals a final conversation translation result is available for a conversation
            participant.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.TextMessageReceived">
            <summary>
            Event that signals a translated text message from a conversation participant.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.ParticipantId">
            <summary>
            Gets your participant identifier
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.AuthorizationToken">
            <summary>
            Gets or sets the authorization token used to connect to the conversation service
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator"/>.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.JoinConversationAsync(Microsoft.CognitiveServices.Speech.Transcription.Conversation,System.String)">
            <summary>
            Joins an existing conversation. You should use this method if you have created a conversation
            using <see cref="M:Microsoft.CognitiveServices.Speech.Transcription.Conversation.CreateConversationAsync(Microsoft.CognitiveServices.Speech.SpeechConfig,System.String)"/>.
            </summary>
            <param name="conversation">The conversation to join.</param>
            <param name="nickname">The display name to use for the current participant.</param>
            <returns>An asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.JoinConversationAsync(System.String,System.String,System.String)">
            <summary>
            Joins an existing conversation.
            </summary>
            <param name="conversationId">The unique identifier for the conversation to join.</param>
            <param name="nickname">The display name to use for the current participant.</param>
            <param name="lang">The speech language to use for the current participant.</param>
            <returns>An asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.StartTranscribingAsync">
            <summary>
            Starts sending audio to the conversation service for speech recognition and translation. You
            should subscribe to the <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribing"/>, and <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribed"/> events to
            receive conversation translation results for yourself, and other participants in the
            conversation.
            </summary>
            <returns>An asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.StopTranscribingAsync">
            <summary>
            Stops sending audio to the conversation service. You will still receive <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribing"/>,
            and <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Transcribed"/> events for other participants in the conversation.
            </summary>
            <returns>An asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.SendTextMessageAsync(System.String)">
            <summary>
            Sends an instant message to all participants in the conversation. This instant message
            will be translated into each participant's text language.
            </summary>
            <param name="message">The message to send.</param>
            <returns>An asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.SetAuthorizationToken(System.String,System.String)">
            <summary>
            Sets the Cognitive Speech authorization token that will be used for connecting to the server.
            </summary>
            <param name="authToken">The authorization token.</param>
            <param name="region">The Azure region for the given token.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.LeaveConversationAsync">
            <summary>
            Leave the current conversation. After this is called, you will no longer receive any events.
            </summary>
            <returns>An asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.ConversationTranslator.Dispose(System.Boolean)">
            <summary>
            Disposes of the object
            </summary>
            <param name="disposeManaged">True to dispose managed resources</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.Meeting">
            <summary>
            Transcribes meetings. Returns recognized text and speaker id.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.CreateMeetingAsync(Microsoft.CognitiveServices.Speech.SpeechConfig,System.String)">
            <summary>
            Creates a new meeting asynchronously.
            </summary>
            <remarks>
            After a meeting is created by this asynchronous operation, the <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.Meeting"/>.MeetingId contains the meeting identifier.
            </remarks>
            <param name="speechConfig">The speech configuration to use.</param>
            <param name="meetingId">The identifier for the meeting you want to join.</param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Meeting.MeetingId">
            <summary>
            Gets or sets the meeting id.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Meeting.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            </summary>
            <remarks>
            The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Meeting.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Meeting.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Meeting.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber"/>.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Meeting.NativeHandle">
            <summary>
            Gets the native handle for the meeting.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.AddParticipantAsync(System.String)">
            <summary>
            Add a participant to a meeting using the user's id as an asynchronous operation.
            </summary>
            <remarks>
            The returned participants can be used to remove. If the client changes the participant's attributes,
            the changed attributes are passed on to the service only when the participants is added again.
            </remarks>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.AddParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.User)">
            <summary>
            Add a participant to a meeting using the User object asynchronously.
            </summary>
            <param name="user">A User object.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.AddParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.Participant)">
            <summary>
            Add a participant to a meeting using the Participant object asynchronously.
            </summary>
            <param name="participant">A participant object.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.RemoveParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.Participant)">
            <summary>
            Remove a participant in a meeting using the Participant object asynchronously.
            </summary>
            <param name="participant">A Participant object.</param>
            <returns>An asynchronous operation representing adding a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.RemoveParticipantAsync(Microsoft.CognitiveServices.Speech.Transcription.User)">
            <summary>
            Remove a participant in a meeting using the User object asynchronously.
            </summary>
            <param name="user">A User object.</param>
            <returns>An asynchronous operation representing removing a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.RemoveParticipantAsync(System.String)">
            <summary>
            Remove a participant from a meeting using a user identifier asynchronously.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing removing a participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.EndMeetingAsync">
            <summary>
            End a meeting.
            </summary>
            <returns>An asynchronous operation representing ending a meeting.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.StartMeetingAsync">
            <summary>
            Start a meeting.
            </summary>
            <returns>An asynchronous operation representing starting a meeting.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.DeleteMeetingAsync">
            <summary>
            Delete a meeting. After this no one will be able to join the meeting.
            </summary>
            <returns>An asynchronous operation representing deleting a meeting.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.LockMeetingAsync">
            <summary>
            Lock a meeting. This will prevent new participants from joining.
            </summary>
            <returns>An asynchronous operation representing locking a meeting.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.UnlockMeetingAsync">
            <summary>
            Unlocks a meeting.
            </summary>
            <returns>An asynchronous operation representing unlocking a meeting.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.MuteAllParticipantsAsync">
            <summary>
            Mute all other participants in the meeting. After this no other participants will
            have their speech recognitions broadcast, nor be able to send text messages.
            </summary>
            <returns>An asynchronous operation representing muting all participants.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.UnmuteAllParticipantsAsync">
            <summary>
            Unmute all other participants in the meeting.
            </summary>
            <returns>An asynchronous operation representing un-muting all participants.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.MuteParticipantAsync(System.String)">
            <summary>
            Mute a participant.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing muting a particular participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.UnmuteParticipantAsync(System.String)">
            <summary>
            Unmute a participant.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>An asynchronous operation representing un-muting a particular participant.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Meeting.Dispose(System.Boolean)">
            <summary>
            
            </summary>
            <param name="disposeManaged"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber">
            <summary>
            Transcribes meetings from speech into text, with the ability to add, remove, and identify multiple participants.
            </summary>
            <remarks>
            See also: [Get started with real-time Meeting Transcription](/azure/cognitive-services/speech-service/how-to-use-meeting-transcription?pivots=programming-language-csharp)
            </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Transcribing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Transcribing"/> signals that an intermediate transcription result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Transcribed">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Transcribed"/> signals that a final transcription result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Canceled"/> signals that the speech recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.#ctor">
            <summary>
            Creates a new instance of MeetingTranscriber.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.#ctor(Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of MeetingTranscriber.
            </summary>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.JoinMeetingAsync(Microsoft.CognitiveServices.Speech.Transcription.Meeting)">
            <summary>
            Join a meeting.
            </summary>
            <param name="meeting">The meeting to be joined.</param>
            <returns>An asynchronous operation representing joining a meeting.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.LeaveMeetingAsync">
            <summary>
            Leave a meeting.
            </summary>
            <returns>An asynchronous operation representing leaving a meeting.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber"/>.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.StartTranscribingAsync">
            <summary>
            Starts meeting trancsribing on a continuous audio stream, until StopTranscribingAsync() is called.
            You must subscribe to events to receive recognition results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.StopTranscribingAsync">
            <summary>
            Stops meeting transcribing.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
            <remarks>This is used to pause the meeting. The client can start the meeting again by calling StartTranscribingAsync.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriber.Dispose(System.Boolean)">
            <summary>
            Disposes of the object
            </summary>
            <param name="disposing">True to dispose managed resources</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionResult">
            <summary>
            Contains the result of meeting transcriber.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionResult.#ctor(System.IntPtr)">
            <summary>
            Constructor to create MeetingTranscriptionResult
            </summary>
            <param name="resultPtr">The result handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionResult.UserId">
            <summary>
            A string that represents the user identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionResult.UtteranceId">
            <summary>
            A string that represents the utterance. This id is consistence for intermediates and final speech recognition result from one speaker.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionResult.ToString">
            <summary>
            Returns a string that represents the meeting transcription result.
            </summary>
            <returns>A string that represents the meeting transcription result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionEventArgs">
            <summary>
            Contains meeting transcriber event arguments.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionEventArgs.Result">
            <summary>
            Represents the meeting transcription result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the meeting transcription result event.
            </summary>
            <returns>A string that represents the meeting transcription event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs">
            <summary>
            Contains payload of meeting transcriber canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs.Reason">
            <summary>
            The reason the result was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.MeetingTranscriptionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the meeting transcription result event.
            </summary>
            <returns>A string that represents the meeting transcriber event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.Participant">
            <summary>
            Represents a participant in a conversation.
            Changed in 1.9.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Participant.From(System.String,System.String,System.String)">
            <summary>
            Creates a Participant using a user identifier, including the user's preferred language and voice signature.
            If voice signature is empty then the user will not be identified.
            </summary>
            <param name="userId">A user identifier.</param>
            <param name="preferredLanguage">A preferred language.</param>
            <param name="voiceSignature">A voice signature of the user.</param>
            <returns>A Participant object</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Participant.#ctor(System.IntPtr)">
            <summary>
            Internal constructor
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.Id">
            <summary>
            The unique identifier for the participant.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.Avatar">
            <summary>
            Gets the colour of the user's avatar as an HTML hex string (e.g. FF0000 for red).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.DisplayName">
            <summary>
            The participant's display name. Please note that each participant within the same conversation must
            have a different display name. Duplicate names within the same conversation are not allowed. You can
            use the <see cref="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.Id"/> property as another way to refer to each participant.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.IsMuted">
            <summary>
            Gets whether or not this participant is muted.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.IsUsingTts">
            <summary>
            Gets whether or not the participant is using Text To Speech (TTS).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.IsHost">
            <summary>
            Gets whether or not this participant is the host.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.PreferredLanguage">
            <summary>
            The participant's preferred spoken language.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.VoiceSignature">
            <summary>
            The participant's voice signature.
            If voice signature is empty then user will not be identified.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.Properties">
            <summary>
            Contains properties of the participant.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.Participant.ParticipantHandle">
            <summary>
            Gets the native handle.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.Participant.Dispose(System.Boolean)">
            <summary>
            Disposes of the object.
            </summary>
            <param name="disposeManaged">True to dispose managed resources.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Transcription.User">
            <summary>
            Represents a user in a conversation.
            Added in 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.User.FromUserId(System.String)">
            <summary>
            Create a user using a user identifier.
            </summary>
            <param name="userId">A user identifier.</param>
            <returns>A user object</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Transcription.User.#ctor(System.IntPtr)">
            <summary>
            Internal constructor
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Transcription.User.UserId">
            <summary>
            Gets the user identifier.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel">
            <summary>
            Represents a pattern matching model used for intent recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel.ModelId">
            <summary>
            Unique Id for this model.
            Note for ConversationalLanguageUnderstandingModels this will be set to the ProjectName-DeploymentName.
            </summary>
            <returns>A string representing the id of this model.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel.LanguageResourceKey">
            <summary>
            This is the Azure language resource key to be used with this model.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel.Endpoint">
            <summary>
            Conversational Language Understanding deployment endpoint to contact.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel.ProjectName">
            <summary>
            Conversational Language Understanding project name.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel.DeploymentName">
            <summary>
            Conversational Language Understanding deployment name.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.ConversationalLanguageUnderstandingModel.#ctor(System.String,System.String,System.String,System.String)">
            <summary>
            Creates a Conversational Language Understanding (CLU) model using the specified model ID.
            </summary>
            <param name="languageResourceKey">The Azure Language resource key.</param>
            <param name="endpoint">The Azure Language resource endpoint.</param>
            <param name="projectName">The Conversational Language Understanding project name.</param>
            <param name="deploymentName">The Conversational Language Understanding deployment name.</param>
            <returns>A shared pointer to the Conversational Language Understanding model.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult">
            <summary>
            Contains result of intent recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.#ctor(System.IntPtr)">
            <summary>
            Creates a new instance
            </summary>
            <param name="resultHandle">The native result handle</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.IntentId">
            <summary>
            A string that represents the intent identifier being recognized.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.Entities">
            <summary>
            Gets the entities found in the utterance.
            </summary>
            <remarks>This does not currently support LUIS entities.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.ToString">
            <summary>
            Returns a string that represents the intent recognition result.
            </summary>
            <returns>A string that represents the intent recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionEventArgs">
            <summary>
            Contains payload of intent recognizing/recognized events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionEventArgs.Result">
            <summary>
            Represents the intent recognition result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the intent recognition result event.
            </summary>
            <returns>A string that represents the intent recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs">
            <summary>
            Contains payload of intent recognition canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.Reason">
            <summary>
            The reason the result was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in 1.1.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the session id and the intent recognition result event.
            </summary>
            <returns>A string that represents the intent recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer">
             <summary>
             Recognizes intents using a language understanding (LUIS) model or phrase. 
             </summary>
             <remarks>
             Intents indicate what the user wishes to initiate or do based on options you define.
             Successful intent recognition returns both recognized text and recognized intent.
            
             See also: [Get started with intent recognition](/azure/cognitive-services/speech-service/quickstarts/intent-recognition)
             </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Canceled"/> signals that the intent recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of IntentRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig)">
            <summary>
            Creates a new instance of IntentRecognizer using EmbeddedSpeechConfig,
            configured to receive speech from the default microphone.
            Added in 1.20.0
            </summary>
            <param name="speechConfig">Embedded speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of IntentRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of IntentRecognizer using EmbeddedSpeechConfig,
            configured to receive speech from an audio source specified in an AudioConfig object.
            Added in 1.20.0
            </summary>
            <param name="speechConfig">Embedded speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that is used for recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.ApplyLanguageModels(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModelCollection)">
            <summary>
            Takes a collection of language understanding models, makes a copy of them, and applies them to the recognizer. This application
            takes effect at different times depending on the LanguageUnderstandingModel type.
            PatternMatchingModels will become active immediately whereas
            LanguageUnderstandingModels utilizing the LUIS service will become active immediately
            unless the recognizer is in the middle of intent recognition in which case it will
            take effect after the next Recognized event.
            </summary>
            <param name="collection">A LanguageUnderstandingModelCollection of shared pointers to LanguageUnderstandingModels.</param>
            <remarks>This replaces any previously applied models.</remarks>
            <returns>True if the application of the models takes effect immediately. Otherwise false.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
              
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Properties">
             <summary>
             Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer"/>.
             </summary>
             <remarks>
             The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            
             For a code example that displays the raw JSON results of an intent recognition, 
             see [How to recognize intents from speech using the Speech SDK for C#](/azure/cognitive-services/speech-service/how-to-recognize-intents-from-speech-csharp).
             </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.RecognizeOnceAsync">
             <summary>
             Starts speech recognition with intent recognition as an asynchronous operation.
             </summary>
             <remarks>
             The end of a single utterance is determined by listening for silence at the end, or until a timeout period has elapsed.
             The task returns the intent in <see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.IntentId" /> 
             and the recognized speech in <see cref="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult" />.Text.
             Intents are defined in the LUIS model or through an <see cref="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(System.String)" /> or <see cref="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(System.String,System.String)" />method.
               
             You can call <see cref="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StopContinuousRecognitionAsync" />
             to stop recognition before an intent has been recognized.
               
             Since this method returns only a single utterance, it is suitable only for single shot recognition like command or query. 
             For long-running multi-utterance recognition, use <see cref="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StartContinuousRecognitionAsync" /> instead.
            
             See also: [Get started with intent recognition](/azure/cognitive-services/speech-service/quickstarts/intent-recognition)
             </remarks>
             <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult"/></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.RecognizeOnceAsync(System.String)">
            <summary>
            Performs intent recognition, and generates a result from the text passed in. This is useful for testing and other times when the speech input
            is not tied to the IntentRecognizer.
            Note: The Intent Service does not currently support this so it is only valid for offline pattern matching or exact matching intents.
            </summary>
            <param name="text">The text to be recognized for intent.</param>
            <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult"/></returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts speech recognition on a continuous audio stream asynchronously, 
            until <see cref="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StopContinuousRecognitionAsync" /> is called.
            </summary>
            <remarks>
            To receive recognition results, you must subscribe to events.
            </remarks>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops a running recognition operation as soon as possible and immediately requests a result based on the
            the input that has been processed so far. This works for all recognition operations, not just continuous
            ones, and facilitates the use of push-to-talk or "finish now" buttons for manual audio endpointing.
            </summary>
            <returns>
            A task that will complete when input processing has been stopped. Result generation, if applicable for the
            input provided, may happen after this task completes and should be handled with the appropriate event.
            </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Configures the recognizer with the given keyword model. After calling this method, the recognizer is listening 
            for the keyword to start the recognition. Call StopKeywordRecognitionAsync() to end the keyword initiated recognition.
            You must subscribe to events to receive recognition results.
            </summary>
            <param name="model">The keyword recognition model that specifies the keyword to be recognized.</param>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.StopKeywordRecognitionAsync">
            <summary>
            Ends the keyword initiated recognition.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(System.String)">
            <summary>
            Adds a simple phrase that may be spoken by the user, indicating a specific user intent.
            </summary>
            <param name="simplePhrase">The phrase corresponding to the intent.</param>
            <remarks>Once recognized, the IntentRecognitionResult's IntentId property will match the simplePhrase specified here.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(System.String,System.String)">
            <summary>
            Adds a simple phrase that may be spoken by the user, indicating a specific user intent.
            </summary>
            <param name="simplePhrase">The phrase corresponding to the intent.</param>
            <param name="intentId">A custom id string to be returned in the <see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.IntentId" /> property.</param>
            <remarks>Once recognized, the result's intent id will match the id supplied here.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel,System.String)">
            <summary>
            Adds a single intent by name from the specified Language Understanding Model.
            For PatternMatchingModel and ConversationalLanguageUnderstandingModel types, this will clear
            any existing models before enabling it. For these types, the intentName is ignored.
            </summary>
            <param name="model">The language understanding model containing the intent.</param>
            <param name="intentName">The name of the single intent to be included from the language understanding model.</param>
            <remarks>Once recognized, the <see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.IntentId" /> property will contain the intentName specified here.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddIntent(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel,System.String,System.String)">
            <summary>
            Adds a single intent by name from the specified Language Understanding Model.
            For PatternMatchingModel and ConversationalLanguageUnderstandingModel types, this will clear
            any existing models before enabling it. For these types, the intentName and intentId are ignored.
            </summary>
            <param name="model">The language understanding model containing the intent.</param>
            <param name="intentName">The name of the single intent to be included from the language understanding model.</param>
            <param name="intentId">A custom id string to be returned in the IntentRecognitionResult's IntentId property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddAllIntents(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel,System.String)">
            <summary>
            Adds a single intent by name from the specified Language Understanding Model.
            For PatternMatchingModel and ConversationalLanguageUnderstandingModel types, this will clear
            any existing models before enabling it. For these types, the intentName is ignored.
            </summary>
            <param name="model">The language understanding model from Language Understanding service.</param>
            <param name="intentId">A custom string id to be returned in the <see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognitionResult.IntentId" /> property.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AddAllIntents(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel)">
            <summary>
            Adds all intents from the specified Language Understanding Model.
            For PatternMatchingModel and ConversationalLanguageUnderstandingModel types, this will clear
            any existing models before enabling it.
            </summary>
            <param name="model">The language understanding model containing the intents.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel">
            <summary>
            Represents language understanding model used for intent recognition.
            </summary>
            <remarks>
            See also: [Add a LanguageUnderstandingModel and intents](/azure/cognitive-services/speech-service/quickstarts/intent-recognition)
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.ModelId">
            <summary>
            Unique Id for this model.
            </summary>
            <returns>A string representing the id of this model.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.FromEndpoint(System.String)">
            <summary>
            Creates a language understanding model using the specified endpoint URL.
            </summary>
            <remarks>
            The Speech SDK supports LUIS v2.0 and v3.0 endpoints. For details, see [Get started with intent recognition](https://docs.microsoft.com/azure/cognitive-services/speech-service/quickstarts/intent-recognition).
            </remarks>
            <param name="uri">Endpoint of the language understanding model.</param>
            <returns>The language understanding model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.FromAppId(System.String)">
            <summary>
            Creates a language understanding model using the application id of Language Understanding service.
            </summary>
            <remarks>
            See also: [Add a LanguageUnderstandingModel and intents](/azure/cognitive-services/speech-service/quickstarts/intent-recognition)
            </remarks>
            <param name="appId">The app ID in the LUIS portal.</param>
            <returns>The language understanding model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel.FromSubscription(System.String,System.String,System.String)">
            <summary>
            Creates a language understanding model using hostname, subscription key and application id of Language Understanding service.
            </summary>
            <param name="subscriptionKey">The subscription key of Language Understanding service.</param>
            <param name="appId">The app ID in the LUIS portal.</param>
            <param name="region">The region identifier for the given subscription key.</param>
            <returns>The language understanding model being created.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModelCollection">
            <summary>
            Represents a collection of LanguageUnderstanding models used for intent recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModelCollection.GetKeyForItem(Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel)">
            <summary>
            Gets key for item/
            </summary>
            <param name="item">Item to get key of.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity">
            <summary>
            Represents a pattern matching entity used for intent recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.EntityId">
            <summary>
            An Id used to define this entity if it is matched. This id must appear in an intent phrase
            or the entity will never be matched.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.Mode">
            <summary>
            The EntityMatchMode of this Entity.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.Type">
            <summary>
            The EntityType of this Entity.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.Phrases">
            <summary>
            A list of strings used to match the entity for List type entities. Strict Mode means the entity must appear in the list.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.#ctor(System.String,Microsoft.CognitiveServices.Speech.Intent.EntityType,Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Protected constructor that sets the properties with the input parameters.
            </summary>
            <param name="entityId">The entityId.</param>
            <param name="type">The EntityType.</param>
            <param name="mode">The EntityMatchMode.</param>
            <param name="phrases"></param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.CreateAnyEntity(System.String)">
            <summary>
            Creates a pattern matching entity using the specified intent ID.
            </summary>
            <param name="entityId">A string that represents a unique Id for this entity.</param>
            <returns>The Pattern Matching Entity being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.CreateIntegerEntity(System.String)">
            <summary>
            Creates a pattern matching entity using the specified intent ID. The PrebuiltInteger Entity will match words representing numbers in lexical, digit, and ordinal formats.
            </summary>
            <param name="entityId">A string that represents a unique Id for this entity.</param>
            <returns>The Pattern Matching Entity being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.CreateListEntity(System.String,Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Creates a pattern matching entity using the specified intent ID, EntityMatchMode, phrases. This entity type will match based on the phrases provided.
            </summary>
            <param name="entityId">A string that represents a unique Id for this entity.</param>
            <param name="mode">The EntityMatchMode for the List entity. Strict means the captured entity must appear in the phrases list.</param>
            <param name="phrases">A list of phrases used to match the list entity.</param>
            <returns>The Pattern Matching Entity being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity.CreateListEntity(System.String,Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode,System.String[])">
            <summary>
            Creates a pattern matching entity using the specified intent ID, EntityMatchMode, phrases. This entity type will match based on the phrases provided.
            </summary>
            <param name="entityId">A string that represents a unique Id for this entity.</param>
            <param name="mode">The EntityMatchMode for the List entity. Strict means the captured entity must appear in the phrases list.</param>
            <param name="phrases">A list of phrases used to match the list entity.</param>
            <returns>The Pattern Matching Entity being created.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode">
            <summary>
            Used to define the match mode of an entity used for intent recognition. Currently this only affects List entities as all other entities only have one mode.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode.Basic">
            <summary>
            This is the basic or default mode of matching based on the EntityType.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode.Strict">
            <summary>
            This will match only exact matches within the entities phrases.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Intent.EntityMatchMode.Fuzzy">
            <summary>
            This will match text within the slot the entity is in, but not require anything from that text.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.EntityType">
            <summary>
            Used to define the type of entity used for intent recognition.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Intent.EntityType.Any">
            <summary>
            This will match any text that fills the slot.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Intent.EntityType.List">
            <summary>
            This will match text that is contained within the list or any text if the mode is set to EntityMatchMode.Fuzzy.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Intent.EntityType.PrebuiltInteger">
            <summary>
            This will match cardinal and ordinal integers.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntityCollection">
            <summary>
            Represents a PatternMatchingEntity collection used for intent recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntityCollection.GetKeyForItem(Microsoft.CognitiveServices.Speech.Intent.PatternMatchingEntity)">
            <summary>
            Gets key for item/
            </summary>
            <param name="item">Item to get key of.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent">
            <summary>
            Represents a pattern matching intent used for intent recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent.IntentId">
            <summary>
            An Id used to define this PatternMatchingIntent. This Id will be used in the IntentRecognitionResult returned from the IntentRecognizer.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent.Phrases">
            <summary>
            Phrases and patterns that will trigger this intent. At least one phrase must exist to be able to
            apply this intent to an IntentRecognizer.
            </summary> 
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent.#ctor(System.String)">
            <summary>
            Creates a pattern matching intent using the specified intent Id.
            </summary>
            <param name="intentId">A string that represents a unique Id for this intent.</param>
            <returns>The Pattern Matching Intent being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent.#ctor(System.String,System.Collections.Generic.IEnumerable{System.String})">
            <summary>
            Creates a pattern matching intent using the specified intent Id and collection of phrases.
            </summary>
            <param name="intentId">A string that represents a unique Id for this intent.</param>
            <param name="phrases">A list of phrases to be use to match the intent.</param>
            <returns>The Pattern Matching Intent being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent.#ctor(System.String,System.String[])">
            <summary>
            Creates a pattern matching intent using the specified intent Id.
            </summary>
            <param name="intentId">A string that represents a unique Id for this intent.</param>
            <param name="phrases">A list of phrases to be use to match the intent.</param>
            <returns>The Pattern Matching Intent being created.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntentCollection">
            <summary>
            Represents a pattern matching model intent collection used for intent recognition.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntentCollection.GetKeyForItem(Microsoft.CognitiveServices.Speech.Intent.PatternMatchingIntent)">
            <summary>
            Gets key for PatternMatchingIntent item.
            </summary>
            <param name="item">Item to get key of.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel">
            <summary>
            Represents a pattern matching model used for intent recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel.Intents">
            <summary>
            This container of PatternMatchingIntents is used to define all the PatternMatchingIntents this model will look for.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel.Entities">
            <summary>
            This container of PatternMatchingEntitys is used to define all the PatternMatchingEntitys this model will look for.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel.ModelId">
            <summary>
            Unique Id for this model.
            </summary>
            <returns>A string representing the id of this model.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel.#ctor(System.String)">
            <summary>
            Creates a pattern matching model using the specified model ID.
            </summary>
            <param name="modelId">A string that represents a unique Id for this model.</param>
            <returns>The Pattern Matching Model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel.FromJSONFile(System.String)">
            <summary>
            Creates a pattern matching model using the specified .json file. This should follow the Microsoft LUIS JSON export schema.
            </summary>
            <param name="filepath">A string that representing the path to a '.json' file.</param>
            <returns>A shared pointer to pattern matching model.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Intent.PatternMatchingModel.FromJSONFileStream(System.IO.Stream)">
            <summary>
            Creates a PatternMatchingModel using the specified istream pointing to an .json file in the LUIS json format.
            This assumes the stream is already open and has permission to read.
            </summary>
            <param name="iStream">A stream that represents a '.json' file.</param>
            <returns>A shared pointer to pattern matching model.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResultCollection">
            <summary>
            Collection of best recognitions.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResultCollection.NBest">
            <summary>
            Enumerable of alternative interpretations of the same speech recognition result.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult">
             <summary>
             Contains recognition details including confidence score, recognized text, raw lexical form, normalized form,
             and normalized form with masked profanity.
             Changed in 1.7.0
             </summary>
             <remarks>
             The following example shows differences between display forms:
            
             * Text: Can we meet at 2:40 PM today?
             * LexicalForm: can we meet at two forty pm today
             * NormalizedForm: can we meet at 2:40 PM today
             </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.Confidence">
            <summary>
            Confidence of recognition, from 0.0 (no confidence) to 1.0 (full confidence)
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.Text">
            <summary>
            Recognized text.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.LexicalForm">
            <summary>
            Raw lexical form of recognized text (the actual words recognized)
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.NormalizedForm">
            <summary>
            Inverse-text-normalized ("canonical") form of the recognized text, with phone numbers, numbers, abbreviations ("doctor smith" to "dr smith"), and other transformations applied
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.MaskedNormalizedForm">
            <summary>
            Normalized form with profanity masked
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.PronunciationAssessment">
            <summary>
            Sentence level pronunciation assessment result, available when pronunciation assessment is enabled.
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.Words">
            <summary>
            Word level timing result list.
            Added in 1.7.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult.DisplayWords">
            <summary>
            Word level timing result list for the DisplayText.
            Added in 1.24.0
            </summary>
            <remarks>Note: Word level timing information for DisplayText is not typically available.</remarks>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.WordLevelTimingResult">
            <summary>
            For a recognized word in speech audio, contains the offset to the start and the duration, in ticks. 1 tick = 100 ns.
            Added in 1.7.0
            </summary>
            <remarks>
            One tick is 100 nanoseconds.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.Confidence">
            <summary>
            The confidence score associated with this word-level entry.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.Duration">
            <summary>
            Duration of word articulation, in ticks.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.Offset">
            <summary>
            Offset from start of speech audio, in ticks.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.Word">
            <summary>
            Recognized word.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.PronunciationAssessment">
            <summary>
            Word level pronunciation assessment result, available when pronunciation assessment is enabled.
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.Phonemes">
            <summary>
            Phoneme level timing result list.
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelTimingResult.Syllables">
            <summary>
            Syllable level timing result list.
            Added in 1.20.0
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult">
            <summary>
            Phoneme level timing result.
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult.Duration">
            <summary>
            Duration in ticks.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult.Offset">
            <summary>
            Offset in ticks.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult.Phoneme">
            <summary>
            Recognized Phoneme.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult.AccuracyScore">
            <summary>
            Accuracy Score.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult.NBestPhonemes">
            <summary>
            NBest phoneme list.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PhonemeLevelTimingResult.PronunciationAssessment">
            <summary>
            Phoneme level pronunciation assessment result, available when pronunciation assessment is enabled.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult">
            <summary>
            Syllable level timing result.
            Added in 1.20.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult.Duration">
            <summary>
            Duration in ticks.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult.Offset">
            <summary>
            Offset in ticks.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult.Syllable">
            <summary>
            Syllable.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult.Grapheme">
            <summary>
            Grapheme.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult.AccuracyScore">
            <summary>
            Accuracy Score.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SyllableLevelTimingResult.PronunciationAssessment">
            <summary>
            Phoneme level pronunciation assessment result, available when pronunciation assessment is enabled.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SentenceLevelPronunciationAssessmentResult">
            <summary>
            Sentence level pronunciation assessment results
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SentenceLevelPronunciationAssessmentResult.AccuracyScore">
            <summary>
            Pronunciation accuracy of the speech. Accuracy indicates how closely the phonemes match a native speaker's pronunciation. Word and full text level accuracy score is aggregated from phoneme level accuracy score.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SentenceLevelPronunciationAssessmentResult.PronunciationScore">
            <summary>
            Overall score indicating the pronunciation quality of the given speech. This is aggregated from AccuracyScore, FluencyScore and CompletenessScore with weight.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SentenceLevelPronunciationAssessmentResult.CompletenessScore">
            <summary>
            Completeness of the speech, determined by calculating the ratio of pronounced words to reference text input.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SentenceLevelPronunciationAssessmentResult.FluencyScore">
            <summary>
            Fluency of the given speech. Fluency indicates how closely the speech matches a native speaker's use of silent breaks between words.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.WordLevelPronunciationAssessmentResult">
            <summary>
            Word level pronunciation assessment results
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelPronunciationAssessmentResult.AccuracyScore">
            <summary>
            Pronunciation accuracy of the speech. Accuracy indicates how closely the phonemes match a native speaker's pronunciation. Aggregated from phoneme level accuracy score.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.WordLevelPronunciationAssessmentResult.ErrorType">
            <summary>
            Error that indicates whether a word is omitted, inserted or badly pronounced, compared to ReferenceText. Possible values are `None` (meaning no error on this word), `Omission`, `Insertion` and `Mispronunciation`.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessmentAccuracyScore">
            <summary>
            Pronunciation assessment result with accuracy score
            Added in 1.20.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessmentAccuracyScore.AccuracyScore">
            <summary>
            Pronunciation accuracy of the speech. Accuracy indicates how closely the speech match a native speaker's pronunciation.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessmentNBestPhoneme">
            <summary>
            Pronunciation assessment nbest phoneme result
            Added in 1.20.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessmentNBestPhoneme.Phoneme">
            <summary>
            Phoneme name.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessmentNBestPhoneme.Score">
            <summary>
            Score of the phoneme
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessmentPhonemeResult">
            <summary>
            Phoneme level pronunciation assessment results
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessmentPhonemeResult.NBestPhonemes">
            <summary>
            NBest phoneme list.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.Diagnostics">
            <summary>
            Diagnostics access for internal objects.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.DisposableBase">
            <summary>
            Base class that implements IDisposable.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.DisposableBase.Finalize">
            <summary>
            The base destructor
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Internal.DisposableBase.IsDisposed">
            <summary>
            Indicates whether this object has been disposed.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.DisposableBase.Dispose">
            <summary>
            Disposes the current object. Safe to call multiple times.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.DisposableBase.Dispose(System.Boolean)">
            <summary>
            Method that actually disposes of resources.
            </summary>
            <param name="disposeManaged">True if we should dispose both managed and native
            resources, false if we should only dispose native resources</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.DisposableBase.CheckDisposed">
            <summary>
            Throws an exception if the object has been disposed.
            </summary>
            <exception cref="T:System.ObjectDisposedException">If the object has been disposed</exception>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator">
            <summary>
            P/Invokes for the conversation translator
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.SPXHANDLE_INVALID">
            <summary>
            An invalid handle
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadString`1">
            <summary>
            Delegate to read a string value from native code. This will return the length if you pass
            <see cref="F:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.Null"/> as the native string handle. Otherwise it copy the
            native string into the passed in native string handle.
            </summary>
            <typeparam name="THandle">The handle type.</typeparam>
            <param name="handle">The native handle.</param>
            <param name="nativeString">The native string handle.</param>
            <param name="length">The length of the native string handle. If native string handle is
            null, this will be set to the native string length. Otherwise this will be used to determine
            the length of the passed in native string handle.</param>
            <returns>An SPXHR code indicating success, or failure.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadValue`2">
            <summary>
            Delegate to read a value from native code
            </summary>
            <typeparam name="THandle">The type of the handle.</typeparam>
            <typeparam name="TValue">The type of the value to read.</typeparam>
            <param name="handle">The native handle.</param>
            <param name="value">The value to set.</param>
            <returns>SPXHR code</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.GetString``1(``0,Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadString{``0})">
            <summary>
            Helper method to read a UTF8 string from native code
            </summary>
            <typeparam name="THandle">The native handle type.</typeparam>
            <param name="handle">The native handle.</param>
            <param name="nativeMethod">The native method to read</param>
            <returns>The string value, or null. Throws an exception on errors</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.TryGetString``1(``0,Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadString{``0},System.String@)">
            <summary>
            Helper method to read a UTF8 string from native code
            </summary>
            <typeparam name="THandle">The native handle type.</typeparam>
            <param name="handle">The native handle.</param>
            <param name="nativeMethod">The native method to read</param>
            <param name="stringValue">If the function succeeds, this parameter will contain the string value</param>
            <returns>true if the string was successfully retrieved, false otherwise</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.GetValue``1(Microsoft.CognitiveServices.Speech.Internal.InteropSafeHandle,Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadValue{Microsoft.CognitiveServices.Speech.Internal.InteropSafeHandle,``0})">
            <summary>
            Helper method to read a value from native code
            </summary>
            <typeparam name="TValue">The value type to read</typeparam>
            <param name="handle">The native handle</param>
            <param name="method">The method to read from native code</param>
            <returns>The value or throws an exception on failure</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.TryGetValue``1(Microsoft.CognitiveServices.Speech.Internal.InteropSafeHandle,Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadValue{Microsoft.CognitiveServices.Speech.Internal.InteropSafeHandle,``0},``0@)">
            <summary>
            Helper method to read a value from native code
            </summary>
            <typeparam name="TValue">The value type to read</typeparam>
            <param name="handle">The native handle</param>
            <param name="method">The method to read from native code</param>
            <param name="value">If the function succeeds, this parameter will contain the requested value. In case of failure, this parameter will contains the default value of the requested type</param>
            <returns>true if the value was successfully retrieved, false otherwise</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.GetValue``1(System.IntPtr,Microsoft.CognitiveServices.Speech.Internal.ConversationTranslator.NativeReadValue{System.IntPtr,``0})">
            <summary>
            Helper method to read a value from native code
            </summary>
            <typeparam name="TValue">The value type to read</typeparam>
            <param name="handle">The native handle</param>
            <param name="method">The method to read from native code</param>
            <returns>The value or throws an exception on failure</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1">
            <summary>
            Helper class to simplify working with native events.
            </summary>
            <typeparam name="TEventArgs">The type of the event arguments</typeparam>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1.SetNativeCallback">
            <summary>
            Delegate used to set the event callback in the native code
            </summary>
            <param name="instanceHandle">The handle to the native instance</param>
            <param name="callback">The managed callback to invoke when the event is fired</param>
            <param name="context">The context to pass to the native code. This will be set to the managed
            instance GC handle</param>
            <returns>An SPXHR code. Non-zero values indicate failures.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1.#ctor(System.Object,Microsoft.CognitiveServices.Speech.Internal.InteropSafeHandle,Microsoft.CognitiveServices.Speech.Internal.InteropEvent{`0}.SetNativeCallback)">
            <summary>
            Creates a new instance
            </summary>
            <param name="sender">The parent instance so events have the right sender</param>
            <param name="senderHandle">The handle to native version of the instance</param>
            <param name="setter">The delegate used to set the handler in native</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1.Add(System.EventHandler{`0})">
            <summary>
            Adds a new managed event handler
            </summary>
            <param name="handler">The handler</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1.Remove(System.EventHandler{`0})">
            <summary>
            Removes a managed handler
            </summary>
            <param name="handler">The handler</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1.Dispose(System.Boolean)">
            <summary>
            Disposes the object
            </summary>
            <param name="disposeManaged">True to dispose managed resources</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.InteropEvent`1.FromNativeCallback(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            The method called from native code to handle event.
            </summary>
            <param name="instanceHandle">The handle to the object in native code.</param>
            <param name="eventHandle">The handle to event object in native code.</param>
            <param name="context">The native pointer used to retrieve the corresponding managed instance.</param>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Internal.SpeakerRecognition.SPXHANDLE_INVALID">
            <summary>
            An invalid handle
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler">
            <summary>
            UTF-8 string marshaler.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler.MarshalNativeToManaged(System.IntPtr)">
            <summary>Copies a null terminated UTF-8 encoded string into a managed string.</summary>
            <param name="native">A pointer to null terminated UTF-8 encoded native string.</param>
            <returns>The managed string equivalent,</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler.MarshalNativeToManaged(System.IntPtr,System.Int32)">
            <summary>Copies a native UTF-8 encoded string into a managed string.</summary>
            <param name="native">A pointer to UTF-8 encoded string.</param>
            <param name="lengthInBytes">The number of bytes in the string excluding any null terminators.</param>
            <returns>The managed string equivalent.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringMarshaler.MarshalManagedToNative(System.String)">
            <summary>Converts the managed data to unmanaged data.</summary>
            <param name="str">The managed string to be converted.</param>
            <returns>A pointer to the COM view of the managed object.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle">
            <summary>
            Helper class to simplify marshaling a UTF8 string to native and back. You should use
            this in a using() block.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.Null">
            <summary>
            Returns a null UTF8 native string handle
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.#ctor(System.String)">
            <summary>
            Creates a new instance
            </summary>
            <param name="str">The string to marshal</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.#ctor(System.UInt32)">
            <summary>
            Creates a new instance
            </summary>
            <param name="maxLength">The maximum string length including the terminating \0</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.#ctor">
            <summary>
            This is deliberately private to prevent using as an out parameter. You cannot free
            native code in managed unless it was explicitly allocated using LocalAlloc or
            CoTaskMemAlloc(). Using this as an out parameter could result in trying to free
            memory allocated using malloc or new().
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.Length">
            <summary>
            Gets the length of the native string handle (including the trailing \0).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.ToString">
            <summary>
            Gets the managed string representation of the native string
            </summary>
            <returns>The managed string representation</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.IsInvalid">
            <summary>Gets a value that indicates whether the handle is invalid.</summary>
            <returns>
                <see langword="true" /> if the handle is not valid; otherwise, <see langword="false" />.
            </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Internal.Utf8StringHandle.ReleaseHandle">
            <summary>
            Executes the code required to free the handle.
            </summary>
            <returns>
                <see langword="true" /> if the handle is released successfully; otherwise, in the event of a catastrophic failure, <see langword=" false" />.
            </returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig">
            <summary>
            Contains base configurations for dialog service connector.
            Added in 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.SetProperty(System.String,System.String)">
            <summary>
            Sets the property by name.
            </summary>
            <param name="name">Name of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Sets a property using a PropertyId value.
            </summary>
            <param name="id">PropertyId of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.GetProperty(System.String)">
            <summary>
            Searches for the property based on the given string name.
            </summary>
            <remarks>
            A small number of properties are stored using string names.
            In most cases, you will use <see cref="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)"/> instead of this method.
            </remarks>
            <param name="name">Name of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Searches for the property named with the given PropertyId value.
            </summary>
            <param name="id">PropertyId of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.SetServiceProperty(System.String,System.String,Microsoft.CognitiveServices.Speech.ServicePropertyChannel)">
            <summary>
            Enables preview of new service features.
            </summary>
            <remarks>
            Used to configure services that are in Preview, and are not yet Generally Available. 
            This method might appear in some samples, where it can typically be ignored.
            </remarks>
            <param name="name">The property name.</param>
            <param name="value">The property value.</param>
            <param name="channel">The channel used to pass the specified property to service.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.SetProxy(System.String,System.Int32,System.String,System.String)">
             <summary>
             Sets proxy configuration.
            
             Note: Proxy functionality is not available on macOS. This method will have no effect on the macOS platform.
             </summary>
             <param name="proxyHostName">The host name of the proxy server, without the protocol scheme (http://)</param>
             <param name="proxyPort">The port number of the proxy server.</param>
             <param name="proxyUserName">The user name of the proxy server.</param>
             <param name="proxyPassword">The password of the proxy server.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.SetProxy(System.String,System.Int32)">
            <summary>
            Sets proxy configuration.
            </summary>
            <param name="proxyHostName">The host name of the proxy server.</param>
            <param name="proxyPort">The port number of the proxy server.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig.Language">
            <summary>
            Specifies the name of the language to be used, in BCP-47 format.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.BotFrameworkConfig">
            <summary>
            Contains configurations for the dialog service connector object for using a Bot Framework backend.
            </summary>
            <remarks>
            See also: [Quickstart: Create a Voice Assistant with the Speech SDK in C# UWP](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/quickstart/csharp/uwp/virtual-assistant)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.BotFrameworkConfig.FromSubscription(System.String,System.String)">
            <summary>
            Creates an instance of the bot framework config with the specified subscription and region.
            </summary>
            <param name="subscription">Subscription key associated with the bot</param>
            <param name="region">Region identifier for the subscription key associated with the bot.</param>
            <returns>A new bot framework config.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.BotFrameworkConfig.FromSubscription(System.String,System.String,System.String)">
            <summary>
            Creates an instance of the bot framework config with the specified subscription and region.
            </summary>
            <param name="subscription">Subscription key associated with the bot</param>
            <param name="region">Region identifier for the subscription key associated with the bot</param>
            <param name="botId">The bot ID (aka bot secret) used to select a bot associated with the given subscription</param>
            <returns>A new bot framework config.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.BotFrameworkConfig.FromAuthorizationToken(System.String,System.String,System.String)">
            <summary>
            Creates an instance of the bot framework config with the specified authorization token and region.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            As configuration values are copied when creating a new recognizer, the new token value will not apply to recognizers that have already been created.
            For recognizers that have been created before, you need to set authorization token of the corresponding recognizer
            to refresh the token. Otherwise, the recognizers will encounter errors during recognition.
            </summary>
            <param name="authorizationToken">The authorization token associated with the bot</param>
            <param name="region">Region identifier for the authorization token associated with the bot</param>
            <param name="botId">The optional bot ID (aka bot secret) used to select a bot associated with the given subscription</param>
            <returns>A new bot framework config.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.CustomCommandsConfig">
            <summary>
            Contains configurations for a dialog service connector.
            </summary>
            <remarks>
            See also: [Integrate with a client application using Speech SDK](/azure/cognitive-services/speech-service/how-to-custom-commands-setup-speech-sdk)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.CustomCommandsConfig.FromSubscription(System.String,System.String,System.String)">
            <summary>
            Creates an instance of the dialog service configuration with the specified Custom Commands application id, subscription and region.
            </summary>
            <param name="applicationId">Custom Commands application id.</param>
            <param name="subscription">Subscription key associated with the application.</param>
            <param name="region">Region identifier for the subscription key associated with the application.</param>
            <returns>A new Custom Commands config.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.CustomCommandsConfig.FromAuthorizationToken(System.String,System.String,System.String)">
            <summary>
            Creates an instance of the dialog service config with the specified Custom Commands application id, authorization token and region.
            </summary>
            <remarks>
            The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            As configuration values are copied when creating a new recognizer, the new token value will not apply to recognizers that have already been created.
            For recognizers that have been created before, you need to set authorization token of the corresponding recognizer
            to refresh the token. Otherwise, the recognizers will encounter errors during recognition.
            </remarks>
            <param name="applicationId">Custom Commands application id.</param>
            <param name="authorizationToken">The authorization token associated with the application.</param>
            <param name="region">Region identifier for the authorization token associated with the application.</param>
            <returns>A new Custom Commands config.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.CustomCommandsConfig.ApplicationId">
            <summary>
            Custom Commands application identifier.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector">
            <summary>
            Connects to a speech enabled dialog.
            Added in 1.5.0
            </summary>
            <remarks>
            See also: 
            * [Integrate with a client application using Speech SDK](/azure/cognitive-services/speech-service/how-to-custom-commands-setup-speech-sdk)
            * [Voice assistants frequently asked questions](/azure/cognitive-services/speech-service/faq-voice-assistants)
            * [What is Custom Commands?](/azure/cognitive-services/speech-service/custom-commands)
            </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SessionStarted">
            <summary>
            Signal that indicates the start of a listening session. See also <see cref="T:Microsoft.CognitiveServices.Speech.SessionEventArgs"/>.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SessionStopped">
            <summary>
            Signal that indicates the end of a listening session. See also <see cref="T:Microsoft.CognitiveServices.Speech.SessionEventArgs"/>.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SpeechStartDetected">
            <summary>
            Occurs when speech data is first detected in the input audio for current phrase.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SpeechEndDetected">
            <summary>
            Occurs when the end of speech data is detected for the current phrase.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.Recognized">
            <summary>
            Signal for events containing speech recognition results. See also <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs"/>.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.Recognizing">
            <summary>
            Signal for events containing intermediate recognition results. See also <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs"/>.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.Canceled">
            <summary>
            Signal for events relating to the cancellation of an interaction. See also <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs"/>.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.ActivityReceived">
            <summary>
            Signal that an activity was received from the backing dialog. See also <see cref="T:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs"/>.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.TurnStatusReceived">
            <summary>
            Signal raised when a turn status update is received.
            </summary>
            <remarks>
            This event indicates success or failure of execution
            on the dialog backend, including in instances where communication with the backend fails due to
            network routing or other conditions that prevent a normal response from the backend.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.#ctor(Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig)">
            <summary>
            Creates a dialog service connector using the default microphone input for a specified dialog service configuration.
            </summary>
            <param name="config">Dialog service config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.#ctor(Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a dialog service connector using the specified dialog and audio configuration.
            </summary>
            <param name="config">Dialog service config.</param>
            <param name="audioConfig">Audio config.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
            </summary>
            <remarks>
            The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SpeechActivityTemplate">
            <summary>
            Gets or sets the JSON template that will be provided to the speech service for the next conversation. The
            service will attempt to merge this template into all activities sent to the dialog backend, whether
            originated by the client with SendActivityAsync or generated by the service, as is the case with
            speech-to-text results.
            </summary>
            <remarks>
            Properties from the template will be stamped on each generated activity.
            This property can be set to null, an empty string, or a syntactically valid JSON object.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector"/> instance.
            </summary>
            <remarks>
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            </summary>
            <param name="disposing">Flag to request disposal.</param>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.gch">
            <summary>
            GC handle for callbacks for context.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.disposed">
            <summary>
            disposed is a flag used to indicate if object is disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.isDisposing">
            <summary>
            Indicates whether the object is currently being disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.audioInputKeepAlive">
            <summary>
            audioInputKeepAlive is reference to AudioConfig object, which is kept alive during DialogServiceConnector lifetime.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_SessionStarted(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a SessionStarted C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_SessionStopped(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a SessionStopped C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_SpeechStartDetected(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a SpeechStartDetected C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_SpeechEndDetected(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a SpeechEndDetected C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_Recognizing(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a Recognizing C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_Recognized(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a Recognized C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_Canceled(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a Canceled C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.FireEvent_ActivityReceived(System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            Method to raise a ActivityReceived C# event when a corresponding callback is invoked from the native layer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.ConnectAsync">
            <summary>
            Connects with the Speech service as an asynchronous operation.
            </summary>
            <returns>An asynchronous operation that starts the connection.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.DisconnectAsync">
            <summary>
            Disconnects from the Speech service as an asynchronous operation.
            </summary>
            <returns>An asynchronous operation that starts the disconnection.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SendActivityAsync(System.String)">
             <summary>
             Sends an activity to the backing dialog as an asynchronous operation.
             </summary>
             <remarks>
             The task completes when the client receives a confirmation from Direct Line Speech service 
             that the Direct Line Speech service received the Activity string.
             This does _not_ indicate that the Bot received the Activity string.
            
             When the task completes, the return string value contains the Session ID GUID. 
             You can use this Session ID GUID to connect client application events with Bot traffic.
             This Session ID GUID is part of the Activity payload the Bot gets from the Direct Line Speech service.
             </remarks>
             <param name="activityJSON">Activity to send as a serialized JSON object</param>
             <returns>A task object representing the asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Starts keyword recognition asynchronously.
            </summary>
            <param name="model">Specifies the keyword model to be used.</param>
            <returns>A task object representing the asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.StopKeywordRecognitionAsync">
            <summary>
            Stops keyword recognition asynchronously.
            </summary>
            <returns>A task object representing the asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.ListenOnceAsync">
             <summary>
             Starts a listening session as an asynchronous operation. 
             </summary>
             <remarks>
             The end of a single utterance is determined by listening for silence at the end, or until a timeout period has elapsed. 
             The task returns the recognized speech in <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResult" />.Text.
            
             See also: [Integrate with a client application using Speech SDK](/azure/cognitive-services/speech-service/how-to-custom-commands-setup-speech-sdk)
             </remarks>
             <returns>A task object representing the asynchronous operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.StartContinuousListeningAsync">
             <summary>
             Begins a continuous listening session as an asynchronous operation.
             </summary>
             <remarks>
             In contrast to ListenOnceAsync, continuous listening will not stop after a single final recognition result
             is received and will instead continue to provide audio for future utterances.
            
             SpeechRecognitionResults will arrive asynchronously and must be retrieved from the <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognized" />
             event.
             </remarks>
             <returns> A task that will complete once continuous listening has successfully begun. </returns> 
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.StopListeningAsync">
            <summary>
            Requests that an active listening operation stop immediately. This interrupts
            any ongoing speaking, and provides a result that only reflects the audio data captured so far.
            </summary>
            <returns> A task representing the asynchronous operation that stops an active listening session. </returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs">
            <summary>
            Class for activity received event arguments.
            Added in 1.5.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs.Activity">
            <summary>
            Gets the activity associated with the event as a serialized JSON object.
            </summary>
            <returns>The activity.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs.HasAudio">
            <summary>
            Checks if the event contains audio.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.ActivityReceivedEventArgs.Audio">
            <summary>
            Gets the audio associated with the event.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Dialog.TurnStatusReceivedEventArgs">
            <summary>
            Class for event arguments raised when a turn status event is generated by the dialog service.
            Added in 1.15.0
            </summary>
            <remarks>
            These arguments indicate execution success or failure for a dialog backend turn, correlated
            to a specific interaction and conversation, even in cases where that backend cannot be reached
            due to network routing or other failure conditions.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.TurnStatusReceivedEventArgs.InteractionId">
            <summary>
            Retrieves the interaction ID associated with this turn status event. Interaction ID generally corresponds
            to a single input signal (e.g. voice utterance) or data/activity transaction and will correlate to
            replyToId fields in Bot Framework activities.
            </summary>
            <returns> The interaction ID associated with the turn status. </returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.TurnStatusReceivedEventArgs.ConversationId">
            <summary>
            Retrieves the conversation ID associated with this turn status event. Conversations may span multiple
            interactions. A client can try to resume or retry a conversation.
            </summary>
            <returns> The conversation ID associated with the turn status. </returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Dialog.TurnStatusReceivedEventArgs.StatusCode">
            <summary>
            Retrieves the numeric status code associated with this turn status event. These typically correspond to
            standard HTTP status codes such as 200 (OK), 400 (Failure/Bad Request), and 429 (Timeout/Throttled).
            </summary>
            <returns> The status code associated with this event, analolgous to standard HTTP codes. </returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig">
            <summary>
            Class that defines embedded (offline) speech configuration.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.FromPath(System.String)">
            <summary>
            Creates an instance of the embedded speech config with a specified offline model path.
            </summary>
            <param name="path">The folder path to search for offline models.
            This can be a root path under which several models are located in subfolders,
            or a direct path to a specific model folder.
            </param>
            <returns>A new embedded speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.FromPaths(System.String[])">
            <summary>
            Creates an instance of the embedded speech config with specified offline model paths.
            </summary>
            <param name="paths">The folder paths to search for offline models.
            These can be root paths under which several models are located in subfolders,
            or direct paths to specific model folders.
            </param>
            <returns>A new embedded speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.GetSpeechRecognitionModels">
            <summary>
            Gets a list of available speech recognition models.
            </summary>
            <returns>Speech recognition model info.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetProperty(System.String,System.String)">
            <summary>
            Sets a property value by name.
            </summary>
            <param name="name">The property name.</param>
            <param name="value">The property value.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Sets a property value by ID.
            </summary>
            <param name="id">The property id.</param>
            <param name="value">The property value.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.GetProperty(System.String)">
            <summary>
            Gets a property value by name.
            </summary>
            <param name="name">The parameter name.</param>
            <returns>The property value.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Gets a property value by ID.
            </summary>
            <param name="id">The parameter id.</param>
            <returns>The property value.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetSpeechRecognitionModel(System.String,System.String)">
            <summary>
            Sets the model for speech recognition.
            </summary>
            <param name="name">The model name.</param>
            <param name="key">The model decryption key.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SpeechRecognitionModelName">
            <summary>
            Gets the model name for speech recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SpeechRecognitionOutputFormat">
            <summary>
            Gets/sets the speech recognition output format (simple or detailed).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetProfanity(Microsoft.CognitiveServices.Speech.ProfanityOption)">
            <summary>
            Sets the profanity option. This can be used to remove profane words or mask them.
            </summary>
            <param name="profanity">Profanity option value.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetSpeechSynthesisVoice(System.String,System.String)">
            <summary>
            Sets the voice for embedded speech synthesis.
            </summary>
            <param name="name">The voice name for embedded speech synthesis.</param>
            <param name="key">The decryption key.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SpeechSynthesisVoiceName">
            <summary>
            Gets the voice name for embedded speech synthesis.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetSpeechSynthesisOutputFormat(Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat)">
            <summary>
            Sets the speech synthesis audio output format.
            </summary>
            <param name="format">The synthesis output format ID (e.g. Riff16Khz16BitMonoPcm).</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SpeechSynthesisOutputFormat">
            <summary>
            Gets the output format of synthesized speech.
            Example value: `riff-16khz-16bit-mono-pcm`
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.GetSpeechTranslationModels">
            <summary>
            Gets a list of available speech translation models.
            </summary>
            <returns>Speech translation model info.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetSpeechTranslationModel(System.String,System.String)">
            <summary>
            Sets the model for speech translation.
            </summary>
            <param name="name">Model name.</param>
            <param name="key">Model decryption key.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SpeechTranslationModelName">
            <summary>
            Gets the model name for speech translation.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.EventLogger">
            <summary>
            A static class to control event-based SDK logging.
            Turning on logging while running your Speech SDK scenario provides
            detailed information from the SDK's core native components. If you
            report an issue to Microsoft, you may be asked to provide logs to help
            Microsoft diagnose the issue. Your application should not take dependency
            on particular log strings, as they may change from one SDK release to another
            without notice.
            Use EventLogger when you want to get access to new log strings as soon
            as they are available, and you need to further process them. For example,
            integrating Speech SDK logs with your existing logging collection system.
            Added in version 1.20.0
            </summary>
            <remarks>Event logging is a process wide construct. That means that if (for example)
            you have multiple speech recognizer objects running in parallel, you can only register
            one callback function to receive interleaved logs from all recognizers. You cannot register
            a separate callback for each recognizer.</remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.EventLogger.OnMessage">
            <summary>
            Event that gets invoked for each new log messages.
            </summary>
            <remarks>You can only subscribe once to the event. The event thread is a working thread of the SDK,
            so the log string should be copied somewhere for further processing by another thread, and return immediately
            from the method. No heavy processing or network calls should be done in method subscribed to this event.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.EventLogger.SetFilters(System.String[])">
            <summary>
            Sets or clears filters for log messages.
            Once filters are set, the event will be invoked only if the log message
            contains at least one of the strings specified by the filters. The match is case sensitive.
            </summary>
            <param name="filters">Filters to use, or an empty list to clear previously set filters</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.EventLogger.SetLevel(Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level)">
            <summary>
            Sets the level of the messages to be captured by the logger
            </summary>
            <param name="level">Maximum level of detail to be capture by the logger.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.FileLogger">
            <summary>
            A static class to control file-based SDK logging.
            Turning on logging while running your Speech SDK scenario provides
            detailed information from the SDK's core native components. If you
            report an issue to Microsoft, you may be asked to provide logs to help
            Microsoft diagnose the issue. Your application should not take dependency
            on particular log strings, as they may change from one SDK release to another
            without notice.
            FileLogger is the simplest logging solution and suitable for diagnosing
            most on-device issues when running Speech SDK.
            Added in version 1.20.0
            </summary>
            <remarks>File logging is a process wide construct. That means that if (for example)
            you have multiple speech recognizer objects running in parallel, there will be one
            log file containing interleaved logs lines from all recognizers. You cannot get a
            separate log file for each recognizer.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.FileLogger.Start(System.String,System.Boolean)">
            <summary>
            Starts logging to a file.
            </summary>
            <param name="filePath">Path to a log file on local disk</param>
            <param name="append">Optional. If true, appends to existing log file. If false, creates a new log file</param>
            <remarks>Note that each write operation to the file is immediately followed by a flush to disk.
            For typical usage (e.g. one Speech Recognizer and a Solid State Drive (SSD)) this should not
            cause performace issues. You may however want to avoid file logging when running many Speech
            SDK recognizers or other SDK objects simultaneously. Use MemoryLogger or EventLogger instead.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.FileLogger.Stop">
            <summary>
            Stops logging to a file.
            </summary>
            <remarks>This call is optional. If logging as been started,
            the log file will be written when the process exists normally.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.FileLogger.SetFilters(System.String[])">
            <summary>
            Sets or clears the filters that apply to file logging.
            Once filters are set, the event will be invoked only if the log string
            contains at least one of the strings specified by the filters. The match is case sensitive.
            </summary>
            <param name="filters">Filters to use, or an empty list to remove previously set filters.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.FileLogger.SetLevel(Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level)">
            <summary>
            Sets the level of the messages to be captured by the logger
            </summary>
            <param name="level">Maximum level of detail to be captured by the logger.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level">
            <summary>
            Defines the different available log levels.
            </summary>
            <remarks>
            This is used by different loggers to set the maximum level of detail they will output.
            <see cref="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.SetLevel(Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level)" />
            <see cref="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.EventLogger.SetLevel(Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level)" />
            <see cref="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.FileLogger.SetLevel(Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level)" />
            </remarks>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger">
            <summary>
            A static class to control SDK logging into an in-memory buffer.
            Turning on logging while running your Speech SDK scenario provides
            detailed information from the SDK's core native components. If you
            report an issue to Microsoft, you may be asked to provide logs to help
            Microsoft diagnose the issue. Your application should not take dependency
            on particular log strings, as they may change from one SDK release to another
            without notice.
            MemoryLogger is designed for the case where you want to get access to logs
            that were taken in the short duration before some unexpected event happens.
            For example, if you are running a Speech Recognizer, you may want to dump the MemoryLogger
            after getting an event indicating recognition was canceled due to some error.
            The size of the memory buffer is fixed at 2MB and cannot be changed. This is
            a "ring" buffer, that is, new log strings written replace the oldest ones
            in the buffer.
            Added in version 1.20.0
            </summary>
            <remarks>Memory logging is a process wide construct. That means that if (for example)
            you have multiple speech recognizer objects running in parallel, there will be one
            memory buffer containing interleaved logs from all recognizers. You cannot get a
            separate logs for each recognizer.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.Start">
            <summary>
            Starts logging into the internal memory buffer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.Stop">
            <summary>
            Stops logging into the internal memory buffer.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.SetFilters(System.String[])">
            <summary>
            Sets or clears filters for memory logging.
            Once filters are set, memory logger will only be updated with log strings
            containing at least one of the strings specified by the filters. The match is case sensitive.
            </summary>
            <param name="filters">Filters to use, or an empty list to remove previously set filters.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.SetLevel(Microsoft.CognitiveServices.Speech.Diagnostics.Logging.Level)">
            <summary>
            Sets the level of the messages to be captured by the logger
            </summary>
            <param name="level">Maximum level of detail to be captured by the logger.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.Dump(System.String)">
            <summary>
            Writes the content of the whole memory buffer to the specified file.
            It does not block other SDK threads from continuing to log into the buffer.
            </summary>
            <param name="filePath">Path to a log file on local disk.</param>
            <remarks>This does not reset (clear) the memory buffer.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.Dump(System.IO.TextWriter)">
            <summary>
            Writes the content of the whole memory buffer to an object that implements System.IO.TextWriter.
            For example, System.Console.Out (for console output).
            It does not block other SDK threads from continuing to log into the buffer.
            </summary>
            <param name="writer">TextWriter object to write to.</param>
            <remarks>This does not reset (clear) the memory buffer.</remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Diagnostics.Logging.MemoryLogger.Dump">
            <summary>
            Returns the content of the whole memory buffer as a string enumerable.
            For example, you can access it as a string list by calling MemoryLogger.Dump().ToList&lt;string&gt;().
            It does not block other SDK threads from continuing to log into the buffer.
            </summary>
            <returns>A string enumerable of the contents of the memory buffer copied into it.</returns>
            <remarks>This does not reset (clear) the memory buffer.</remarks>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.HybridSpeechConfig">
            <summary>
            Class that defines hybrid (cloud and embedded) configurations for speech recognition and speech synthesis.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.FromConfigs(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig)">
            <summary>
            Creates an instance of the hybrid speech config with specified cloud and embedded speech configs.
            </summary>
            <param name="cloudSpeechConfig">Cloud speech configuration</param>
            <param name="embeddedSpeechConfig">Embedded speech configuration</param>
            <returns>A new hybrid speech config instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.SpeechRecognitionOutputFormat">
            <summary>
            Gets/sets the speech recognition output format (simple or detailed).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.SetSpeechSynthesisOutputFormat(Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat)">
            <summary>
            Sets the speech synthesis audio output format.
            </summary>
            <param name="format">The synthesis output format ID (e.g. Riff16Khz16BitMonoPcm).</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.SpeechSynthesisOutputFormat">
            <summary>
            Gets the output format of synthesized speech.
            Example value: `riff-16khz-16bit-mono-pcm`
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.SetProperty(System.String,System.String)">
            <summary>
            Sets a property value by name.
            </summary>
            <param name="name">The property name.</param>
            <param name="value">The property value.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Sets a property value by ID.
            </summary>
            <param name="id">The property id.</param>
            <param name="value">The property value.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.GetProperty(System.String)">
            <summary>
            Gets a property value by name.
            </summary>
            <param name="name">The parameter name.</param>
            <returns>The property value.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.HybridSpeechConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Gets a property value by ID.
            </summary>
            <param name="id">The parameter id.</param>
            <returns>The property value.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel">
            <summary>
            Represents keyword recognition model that can trigger an event when pre-defined keywords are spoken.
            </summary>
            <remarks>
            See also: [Get started with Custom Keyword](/azure/cognitive-services/speech-service/custom-keyword-basics)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel.FromFile(System.String)">
            <summary>
            Creates a keyword recognition model using the specified `.table` file.
            </summary>
            <remarks>
            See also: [Get started with Custom Keyword](/azure/cognitive-services/speech-service/custom-keyword-basics)
            </remarks>
            <param name="fileName">A string that represents file name for the keyword recognition model.</param>
            <returns>The keyword recognition model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionEventArgs">
            <summary>
            Class for the events emitted by the <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognizer" />.
            </summary>
            <remarks>
            Due to a limitation within keyword detection,
            the KeywordRecognitionEventArgs.<see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionEventArgs"/>.Offset property
            does not contain an accurate value.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.KeywordRecognitionEventArgs.Result">
            <summary>
            Keyword recognition event result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the keyword recognition result event.
            </summary>
            <returns>A string that represents the keyword recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionResult">
            <summary>
            Contains the results emitted by the <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognizer" />.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.KeywordRecognizer">
            <summary>
            Recognizes a word or short phrase using a keyword model.
            </summary>
            <remarks>
            You create a keyword model in Speech Studio, which saves it in a `.table` file.
            
            See also:
            * [Get started with Custom Keyword](/azure/cognitive-services/speech-service/custom-keyword-basics)
            * <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionResult" />
            * <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionEventArgs" />
            * <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel" />
            * <see cref="T:Microsoft.CognitiveServices.Speech.Audio.AudioConfig" />
            </remarks>
            <example>
            First, the object needs to be instantiated:
            <code language="c#">
            // (This sample uses the microphone. You can use any input source.)
            var audioConfig = Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultMicrophoneInput();
            var recognizer = new KeywordRecognizer (audioConfig);
            </code>
            (optional) Then, the events need to be wired in order to receive notifications:
            <code language="c#">
            recognizer.Recognized += (s, e) =&gt;
            {
                // Keyword detected!
            };
            </code>
            All set up. Start recognition.
            <code language="c#">
            // for .table, see:
            // https://docs.microsoft.com/azure/cognitive-services/speech-service/custom-keyword-basics
            var keywordModel = KeywordRecognitionModel.FromFile(@"C:\path\to\your\tablefile.table");
            var result = recognizer.RecognizeOnceAsync(keywordModel);
            result.Wait();
            </code>
            </example>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.KeywordRecognizer.gch">
            <summary>
            GC handle for callbacks for context.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognizer.#ctor(Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a KeywordRecognizer from an <see cref="T:Microsoft.CognitiveServices.Speech.Audio.AudioConfig" />. The config
            defines the audio input to be used by the recognizer object.
            </summary>
            <param name="audioConfig">Defines the audio input to be used by the recognizer.</param>
            <returns>A new KeywordRecognizer that will consume audio from the specified input.</returns>
            <remarks>
            If no audio config is provided, the result is the same as a call with a config constructed with
            <see cref="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromDefaultMicrophoneInput" />.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognizer.RecognizeOnceAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Starts a keyword recognition session as an asynchronous operation.
            </summary>
            <param name="model">The <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionModel" /> that describes the keyword we want to detect.</param>
            <returns>A future that resolves to a <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognitionResult" /> that resolves once a keyword is detected.</returns>
            <remarks>
            The keyword recognition session lasts until the first keyword is recognized, or **StopRecognitionAsync** is called. 
              
            When a keyword is recognized, a Recognized event fires and the task completes. To detect another keyword, call the method again.
            
            If no keyword is detected in the input, the task will never resolve unless <see cref="M:Microsoft.CognitiveServices.Speech.KeywordRecognizer.StopRecognitionAsync" /> is called.
              
            See also: [Get started with Custom Keyword](/azure/cognitive-services/speech-service/custom-keyword-basics)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognizer.StopRecognitionAsync">
            <summary>
            Stops a currently active keyword recognition session asynchronously.
            </summary>
            <returns>A future that resolves when the active session (if any) is stopped.</returns>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.KeywordRecognizer.Recognized">
            <summary>
            Signal for events related to the recognition of keywords.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.KeywordRecognizer.Canceled">
            <summary>
            Signal for events relating to the cancellation of an interaction. The event indicates if the reason is a direct cancellation or an error.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.KeywordRecognizer.Properties">
            <summary>
            A collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.KeywordRecognizer"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.KeywordRecognizer.isDisposing">
            <summary>
            Indicates whether the object is currently being disposed.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.KeywordRecognizer.Dispose">
            <summary>
            This method performs cleanup of resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.NoMatchReason">
            <summary>
            Lists the possible reasons a recognition result was not recognized.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.NotRecognized">
            <summary>
            Indicates that speech was detected, but not recognized.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.InitialSilenceTimeout">
            <summary>
            Indicates that the start of the audio stream only contained silence, and the service timed out waiting for speech.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.InitialBabbleTimeout">
            <summary>
            Indicates that the start of the audio stream only contained noise, and the service timed out waiting for speech.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.KeywordNotRecognized">
            <summary>
            Indicates that the recognized keyword was rejected by the keyword verification service.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.NoMatchReason.EndSilenceTimeout">
            <summary>
            Indicates that the audio stream contained only silence after the last recognized phrase.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.OutputFormat">
            <summary>
            Output format.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ProfanityOption">
            <summary>
            Removes profanity (swearing), or replaces letters of profane words with stars.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ProfanityOption.Masked">
            <summary>
            Replaces letters in profane words with star characters.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ProfanityOption.Removed">
            <summary>
            Removes profane words.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ProfanityOption.Raw">
            <summary>
            Does nothing to profane words.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig">
            <summary>
            Configures a pronunciation assessment.
            Added in 1.14.0
            </summary>
            <remarks>
            See also: [Pronunciation assessment](/azure/cognitive-services/speech-service/how-to-pronunciation-assessment?pivots=programming-language-csharp#pronunciation-assessment-with-the-speech-sdk)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.#ctor(System.String,Microsoft.CognitiveServices.Speech.PronunciationAssessment.GradingSystem,Microsoft.CognitiveServices.Speech.PronunciationAssessment.Granularity,System.Boolean)">
            <summary>
            Creates an instance of the PronunciationAssessmentConfig.
            </summary>
            <remarks>
            See also: [Pronunciation assessment](/azure/cognitive-services/speech-service/how-to-pronunciation-assessment?pivots=programming-language-csharp#pronunciation-assessment-with-the-speech-sdk)
            </remarks>
            <param name="referenceText">The reference text to evaluate against</param>
            <param name="gradingSystem">The point system for score calibration</param>
            <param name="granularity">The evaluation granularity</param>
            <param name="enableMiscue">Miscue option. When true, the pronounced words are compared to the reference text, and are marked with omission/insertion based on the comparison; when false, the recognized text will always be reference text.</param>
            <returns>A new PronunciationAssessmentConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.FromJson(System.String)">
             <summary>
             Creates an instance of the PronunciationAssessmentConfig from JSON-formatted text.
             </summary>
             <remarks>
             You can use JSON-formatted text rather than constructor parameters to configure the pronunciation assessment.
            
             See also: [Pronunciation assessment parameters](/azure/cognitive-services/speech-service/rest-speech-to-text#pronunciation-assessment-parameters)
             </remarks>
             <param name="json">The JSON-formatted string containing the pronunciation assessment parameters.</param>
             <returns>A new PronunciationAssessmentConfig instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.ReferenceText">
            <summary>
            Contains text used to compare with a pronunciation.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.PhonemeAlphabet">
            <summary>
            The phoneme alphabet. Valid values are: "SAPI" (default) and "IPA".
            Added in 1.20.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.NBestPhonemeCount">
            <summary>
            The nbest phoneme count.
            Added in 1.20.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.ToJson">
            <summary>
            Gets JSON-formatted string of pronunciation assessment parameters.
            </summary>
            <returns>JSON string of pronunciation assessment parameters.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.ApplyTo(Microsoft.CognitiveServices.Speech.Recognizer)">
            <summary>
            Applies the settings in this config to a recognizer.
            </summary>
            <param name="recognizer">The target recognizer.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessment.GradingSystem">
            <summary>
            Lists point systems for pronunciation score calibration.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PronunciationAssessment.GradingSystem.FivePoint">
            <summary>
            Provides a 0-5 floating point score.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PronunciationAssessment.GradingSystem.HundredMark">
            <summary>
            Provides a 0-100 floating point score.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessment.Granularity">
            <summary>
            Lists pronunciation evaluation granularities.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PronunciationAssessment.Granularity.Phoneme">
            <summary>
            Shows the score on the full text, word and phoneme level.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PronunciationAssessment.Granularity.Word">
            <summary>
            Shows the score on the full text and word level.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PronunciationAssessment.Granularity.FullText">
            <summary>
            Shows the score on the full text level only.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult">
            <summary>
            Contains pronunciation assessment results.
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.AccuracyScore">
            <summary>
            Score indicating how closely the phonemes match a native speaker's pronunciation.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.PronunciationScore">
            <summary>
            Overall score indicating the pronunciation quality of the given speech.
            This is calculated from AccuracyScore, FluencyScore and CompletenessScore with weight.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.CompletenessScore">
            <summary>
            Score indicating the completeness of the given speech as a ratio of pronounced words to total words.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.FluencyScore">
            <summary>
            Score indicating the fluency of the given speech.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.Words">
            <summary>
            Word level pronunciation assessment result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.FromResult(Microsoft.CognitiveServices.Speech.SpeechRecognitionResult)">
            <summary>
            Creates an instance of PronunciationAssessmentResult object for the speech recognition result.
            </summary>
            <param name="result">The speech recongition result.</param>
            <returns>A new PronunciationAssessmentResult instance</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentResult.#ctor(Microsoft.CognitiveServices.Speech.DetailedSpeechRecognitionResult)">
            <summary>
            Creates an instance of PronunciationAssessmentResult object for a DetailedSpeechRecognitionResult result.
            </summary>
            <param name="result">The DetailedSpeechRecognitionResult result.</param>
            <returns>A new PronunciationAssessmentResult instance</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentWordResult">
            <summary>
            Contains word level pronunciation assessment result.
            Added in 1.14.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentWordResult.Word">
            <summary>
            Word text.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentWordResult.AccuracyScore">
            <summary>
            Score that indicates how closely the phonemes match a native speaker's pronunciation.
            </summary>
            <remarks>
            Note: The accuracy score is invalid if the ErrorType of this word is Omission.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentWordResult.ErrorType">
            <summary>
            Value that indicates whether a word is omitted, inserted or badly pronounced, compared to ReferenceText.
            Possible values are None (meaning no error on this word), Omission, Insertion and Mispronunciation.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentWordResult.Syllables">
            <summary>
            Syllable level pronunciation assessment result.
            Added in 1.20.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentWordResult.Phonemes">
            <summary>
            Phoneme level pronunciation assessment result
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PropertyCollection">
            <summary>
            Class to retrieve or set a property value from a property collection.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.#ctor(System.IntPtr)">
            <summary>
            Constructor to create PropertyCollection
            </summary>
            <param name="propertyBagPtr">The property bag handle.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.Close">
            <summary>
            Dispose the property bag
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Searches for the property named with this PropertyId value.
            </summary>
            <remarks>
            If the property value is not defined, an empty string is returned.
            </remarks>
            <param name="id">The ID of property. See <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/></param>
            <returns>value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(System.String)">
            <summary>
            Searches for the property that has this string name.
            </summary>
            <remarks>
            If the property value is not defined, an empty string is returned.
            In most cases, you will use <see cref="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)" /> instead of this method.
            </remarks>
            <param name="propertyName">The name of property</param>
            <returns>value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Searches for the property named with this PropertyId value.
            </summary>
            <remarks>
            If the property value is not defined, the specified default value is returned.
            </remarks>
            <param name="id">The id of property. See <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/></param>
            <param name="defaultValue">The default value which is returned if no value is defined for the property.</param>
            <returns>value of the property.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(System.String,System.String)">
            <summary>
            Searches for the property that has this string name.
            </summary>
            <remarks>
            A small number of properties are stored using string names.
            If the property value is not defined, the specified default value is returned.
            </remarks>
            <param name="propertyName">The name of property.</param>
            <param name="defaultValue">The default value which is returned if no value is defined for the property.</param>
            <returns>value of the property.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Set value of a property.
            </summary>
            <param name="id">The id of property. See <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/></param>
            <param name="value">value to set</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PropertyCollection.SetProperty(System.String,System.String)">
            <summary>
            Set value of a property.
            </summary>
            <param name="propertyName">The name of property.</param>
            <param name="value">value to set</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PropertyId">
            <summary>
            Lists speech property IDs.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Key">
            <summary>
            The subscription key used with Speech service endpoints. If you are using an intent recognizer, you need
            to specify the LUIS endpoint key for your particular LUIS app. Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Endpoint">
            <summary>
            The Speech service endpoint, a URL. Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead, use  <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)"/>, or <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)"/>.
            NOTE: This endpoint is not the same as the endpoint used to obtain an access token.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Region">
            <summary>
            The Speech service region associated with the subscription key. Under normal circumstances, you shouldn't have to
            use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)"/>, <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)"/>,
            <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)"/>, <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri,System.String)"/>,
            <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri)"/>, <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromAuthorizationToken(System.String,System.String)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceAuthorization_Token">
            <summary>
            The Speech service authorization token (aka access token). Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromAuthorizationToken(System.String,System.String)"/>,
            <see cref="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.AuthorizationToken"/>, <see cref="P:Microsoft.CognitiveServices.Speech.Intent.IntentRecognizer.AuthorizationToken"/>,
            <see cref="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.AuthorizationToken"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceAuthorization_Type">
            <summary>
            Unused. The Speech service authorization type.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_EndpointId">
            <summary>
            The Custom Speech or Custom Voice Service endpoint id. Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)"/>, or <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)"/>.
            NOTE: The endpoint id is available in the Custom Speech Portal, listed under Endpoint Details.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Host">
            <summary>
            The Speech service host (url). Under normal circumstances, you shouldn't
            have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri,System.String)"/>, or <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyHostName">
            <summary>
            The host name of the proxy server used to connect to the Speech service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            Added in 1.1.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyPort">
            <summary>
            The port of the proxy server used to connect to the Speech service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            Added in 1.1.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyUserName">
            <summary>
            The user name of the proxy server used to connect to the Speech service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            Added in 1.1.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_ProxyPassword">
            <summary>
            The password of the proxy server used to connect to the Speech service. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)"/>.
            Added in 1.1.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_Url">
            <summary>
            The URL string built from speech configuration.
            This property is read-only. The SDK uses this value internally.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_TranslationToLanguages">
            <summary>
            The list of comma separated languages (in BCP-47 format) used as target translation languages. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.AddTargetLanguage(System.String)"/> and the read-only <see cref="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.TargetLanguages"/> collection.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_TranslationVoice">
            <summary>
            The name of the voice used for Text-to-speech. Under normal circumstances, you shouldn't have to use this
            property directly. Instead use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.VoiceName"/>.
            Find valid voice names <a href="/azure/cognitive-services/speech-service/language-support#text-to-speech">here</a>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_TranslationFeatures">
            <summary>
            Translation features. For internal use.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_IntentRegion">
            <summary>
            The Language Understanding Service Region. Under normal circumstances, you shouldn't have to use this property directly.
            Instead use <see cref="T:Microsoft.CognitiveServices.Speech.Intent.LanguageUnderstandingModel"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoMode">
            <summary>
            The Speech service recognition mode. Can be INTERACTIVE, CONVERSATION, DICTATION.
            This property is read-only. The SDK uses it internally.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoLanguage">
            <summary>
            The spoken language to be recognized (in BCP-47 format). Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechRecognitionLanguage"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoBackend">
            <summary>
            The string to specify the backend to be used for speech recognition;
            allowed options are online and offline.
            Under normal circumstances, you shouldn't use this property directly.
            Currently the offline option is only valid when EmbeddedSpeechConfig is used.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoModelName">
            <summary>
            The name of the model to be used for speech recognition.
            Under normal circumstances, you shouldn't use this property directly.
            Currently this is only valid when EmbeddedSpeechConfig is used.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_RecoModelKey">
            <summary>
            The decryption key of the model to be used for speech recognition.
            Under normal circumstances, you shouldn't use this property directly.
            Currently this is only valid when EmbeddedSpeechConfig is used.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Speech_SessionId">
            <summary>
            The session id. This id is a universally unique identifier (aka UUID) representing a specific binding of an audio input stream
            and the underlying speech recognition instance to which it is bound. Under normal circumstances,
            you shouldn't have to use this property directly.
            Instead use <see cref="P:Microsoft.CognitiveServices.Speech.SessionEventArgs.SessionId"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthLanguage">
            <summary>
            The spoken language to be synthesized (e.g. en-US).
            Added in 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthVoice">
            <summary>
            The name of the voice to be used for Text-to-speech.
            Added in 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthOutputFormat">
            <summary>
            The string to specify speech synthesis output audio format (e.g. riff-16khz-16bit-mono-pcm)
            Added in 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthEnableCompressedAudioTransmission">
            <summary>
            Indicates whether to use compressed audio format for speech synthesis audio transmission.
            This property only matters when SpeechServiceConnection_SynthOutputFormat is set to a pcm format.
            If this property is not set to true and GStreamer is available, SDK will use compressed format for synthesized audio transmission,
            and decode it. You can set this property to false to use raw pcm format for transmission on wire.
            Added in 1.16.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthBackend">
            <summary>
            The string to specify TTS backend; valid options are online and offline.
            Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.FromPath(System.String)"/> or <see cref="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.FromPaths(System.String[])"/>.
            to set the synthesis backend to offline.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthOfflineDataPath">
            <summary>
            The data file path(s) for offline synthesis engine; only valid when synthesis backend is offline.
            Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.FromPath(System.String)"/> or <see cref="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.FromPaths(System.String[])"/>.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthOfflineVoice">
            <summary>
            The name of the offline TTS voice to be used for speech synthesis.
            Under normal circumstances, you shouldn't use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetSpeechSynthesisVoice(System.String,System.String)"/>.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_SynthModelKey">
            <summary>
            The decryption key of the model to be used for speech synthesis.
            Under normal circumstances, you shouldn't use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig.SetSpeechSynthesisVoice(System.String,System.String)"/>.
            Added in version 1.19.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_VoicesListEndpoint">
            <summary>
            The Cognitive Services Speech Service voices list API endpoint (url). Under normal circumstances,
            you don't need to specify this property, SDK will construct it based on the region/host/endpoint of <see cref="T:Microsoft.CognitiveServices.Speech.SpeechConfig"/>.
            Added in 1.16.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs">
            <summary>
            The initial silence timeout value (in milliseconds) used by the service.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_EndSilenceTimeoutMs">
            <summary>
            The end silence timeout value (in milliseconds) used by the service.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_EnableAudioLogging">
            <summary>
            A boolean value specifying whether audio logging is enabled in the service or not.
            Audio and content logs are stored either in Microsoft-owned storage, or in your own storage account linked
            to your Cognitive Services subscription (Bring Your Own Storage (BYOS) enabled Speech resource).
            Added in 1.5.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_AutoDetectSourceLanguages">
            <summary>
            The auto detect source languages.
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_AutoDetectSourceLanguageResult">
            <summary>
            The auto detect source language result.
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceConnection_LanguageIdMode">
            <summary>
            The speech service connection language identifier mode.
            Can be "AtStart" (the default), or "Continuous". See [Language
            Identification](https://aka.ms/speech/lid?pivots=programming-language-csharp) document.
            Added in 1.25.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestDetailedResultTrueFalse">
            <summary>
            The requested Speech service response output format (**OutputFormat.Simple** or **OutputFormat.Detailed**).
            Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechConfig.OutputFormat"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestProfanityFilterTrueFalse">
            <summary>
            Unused. The requested Speech service response output profanity level.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_ProfanityOption">
            <summary>
            The requested Speech service response output profanity setting.
            Allowed values are masked, removed, and raw.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_PostProcessingOption">
            <summary>
            A string value specifying which post processing option should be used by service.
            Allowed value: TrueText.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestWordLevelTimestamps">
            <summary>
            A boolean value specifying whether to include word-level timestamps in the response result.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_StablePartialResultThreshold">
            <summary>
            The number of times a word has to be in partial results to be returned.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_OutputFormatOption">
            <summary>
            A string value specifying the output format option in the response result. Internal use only.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestSnr">
            <summary>
            A boolean value specifying whether to include SNR (signal to noise ratio) in the response result.
            Added in version 1.18.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_TranslationRequestStablePartialResult">
            <summary>
            A boolean value to request for stabilizing translation partial results by omitting words in the end.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestWordBoundary">
            <summary>
            A boolean value specifying whether to request WordBoundary events.
            Added in version 1.21.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestPunctuationBoundary">
            <summary>
            A boolean value specifying whether to request punctuation boundary in WordBoundary Events. Default is true.
            Added in version 1.21.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RequestSentenceBoundary">
            <summary>
            A boolean value specifying whether to request sentence boundary in WordBoundary Events. Default is false.
            Added in version 1.21.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisEventsSyncToAudio">
            <summary>
            A boolean value specifying whether the SDK should synchronize synthesis metadata events,
            (e.g. word boundary, viseme, etc.) to the audio playback. This only takes effect when the audio is played through the SDK.
            Default is true.
            If set to false, the SDK will fire the events as they come from the service, which may be out of sync with the audio playback.
            Added in version 1.31.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_JsonResult">
            <summary>
            The Speech service response output (in JSON format). This property is available on
            recognition result objects only.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_JsonErrorDetails">
            <summary>
            The Speech service error details (in JSON format). Under normal circumstances, you shouldn't have to
            use this property directly. Instead use <see cref="P:Microsoft.CognitiveServices.Speech.CancellationDetails.ErrorDetails"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RecognitionLatencyMs">
            <summary>
            The recognition latency in milliseconds. Read-only, available on final speech/translation/intent results.
            This measures the latency between when an audio input is received by the SDK, and the moment the final result is received from the service.
            The SDK computes the time difference between the last audio fragment from the audio input that is contributing to the final result, and the time the final result is received from the speech service.
            Added in 1.3.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_RecognitionBackend">
            <summary>
            The recognition backend. Read-only, available on speech recognition results.
            This indicates whether cloud (online) or embedded (offline) recognition was used to produce the result.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisFirstByteLatencyMs">
            <summary>
            The speech synthesis first byte latency in milliseconds. Read-only, available on final speech synthesis results.
            This measures the latency between when the synthesis is started to be processed, and the moment the first byte audio is available.
            Added in version 1.17.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisFinishLatencyMs">
            <summary>
            The speech synthesis all bytes latency in milliseconds. Read-only, available on final speech synthesis results.
            This measures the latency between when the synthesis is started to be processed, and the moment the whole audio is synthesized.
            Added in version 1.17.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisUnderrunTimeMs">
            <summary>
            The underrun time for speech synthesis in milliseconds. Read-only, available on results in SynthesisCompleted events.
            This measures the total underrun time from <see cref="F:Microsoft.CognitiveServices.Speech.PropertyId.AudioConfig_PlaybackBufferLengthInMs"/> is filled to synthesis completed.
            Added in version 1.17.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisConnectionLatencyMs">
            <summary>
            The speech synthesis connection latency in milliseconds. Read-only, available on final speech synthesis results.
            This measures the latency between when the synthesis is started to be processed, and the moment the HTTP/WebSocket connection is established.
            Added in version 1.26.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisNetworkLatencyMs">
            <summary>
            The speech synthesis network latency in milliseconds. Read-only, available on final speech synthesis results.
            This measures the network round trip time.
            Added in version 1.26.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisServiceLatencyMs">
            <summary>
            The speech synthesis service latency in milliseconds. Read-only, available on final speech synthesis results.
            This measures the service processing time to synthesize the first byte of audio.
            Added in version 1.26.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechServiceResponse_SynthesisBackend">
            <summary>
            Indicates which backend the synthesis is finished by.
            Read-only, available on speech synthesis results, except for the result in SynthesisStarted event.
            Added in version 1.19.0.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.CancellationDetails_Reason">
            <summary>
            Unused. The cancellation reason.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.CancellationDetails_ReasonText">
            <summary>
            Unused. The cancellation text.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.CancellationDetails_ReasonDetailedText">
            <summary>
            Unused. The cancellation detailed text.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.LanguageUnderstandingServiceResponse_JsonResult">
            <summary>
            The Language Understanding Service response output (in JSON format). Available via <see cref="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Properties"/>.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.AudioConfig_DeviceNameForRender">
            <summary>
            The device name for audio render. Under normal circumstances, you shouldn't have to
            use this property directly.
            Instead, use <see cref="M:Microsoft.CognitiveServices.Speech.Audio.AudioConfig.FromSpeakerOutput(System.String)"/>.
            Added in version 1.17.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.AudioConfig_PlaybackBufferLengthInMs">
            <summary>
            Playback buffer length in milliseconds, default is 50 milliseconds.
            Added in version 1.17.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Speech_LogFilename">
            <summary>
            The file name to write logs.
            Added in 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Speech_SegmentationSilenceTimeoutMs">
             <summary>
             A duration of detected silence, measured in milliseconds, after which speech-to-text will determine a spoken
             phrase has ended and generate a final Recognized result. Configuring this timeout may be helpful in situations
             where spoken input is significantly faster or slower than usual and default segmentation behavior consistently
             yields results that are too long or too short. Segmentation timeout values that are inappropriately high or low
             can negatively affect speech-to-text accuracy; this property should be carefully configured and the resulting
             behavior should be thoroughly validated as intended.
            
             For more information about timeout configuration that includes discussion of default behaviors, please visit
             https://aka.ms/csspeech/timeouts.
             </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_ApplicationId">
            <summary>
            Identifier used to connect to the backend service.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_DialogType">
            <summary>
            Type of dialog backend to connect to.
            Added in 1.7.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Initial_Silence_Timeout">
            <summary>
            Silence timeout for listening.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_From_Id">
            <summary>
            The from identifier to add to speech recognition activities.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Conversation_Id">
            <summary>
            ConversationId for the session.
            Added in 1.8.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Custom_Voice_Deployment_Ids">
            <summary>
            Comma separated list of custom voice deployment ids.
            Added in 1.8.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Speech_Activity_Template">
            <summary>
            Speech activity template, stamp properties from the template on the activity generated by the service for speech. See <see cref="P:Microsoft.CognitiveServices.Speech.Dialog.DialogServiceConnector.SpeechActivityTemplate"/>
            Added in 1.10.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Request_Bot_Status_Messages">
            <summary>
            A boolean value that specifies whether or not the client should receive turn status messages and generate
            corresponding TurnStatusReceived events. Defaults to true.
            Added in 1.15.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_Connection_Id">
            <summary>
            Additional identifying information, such as a Direct Line token, used to authenticate with the backend service.
            Added in 1.16.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.Conversation_ParticipantId">
            <summary>
            Gets your identifier in the conversation.
            Added in 1.13.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.ConversationTranscribingService_DataBufferUserId">
            <summary>
            The user identifier associated to data buffer written by client when using Pull/Push audio mode streams.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.ConversationTranscribingService_DataBufferTimeStamp">
            <summary>
            The time stamp associated to data buffer written by client when using Pull/Push audio mode streams.
            The time stamp is a 64-bit value with a resolution of 90 kHz. The same as the presentation timestamp in an MPEG transport stream.
            See https://en.wikipedia.org/wiki/Presentation_timestamp.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_ReferenceText">
            <summary>
            The reference text of the audio for pronunciation evaluation.
            For this and the following pronunciation assessment parameters, see
            [Pronunciation assessment parameters](/azure/cognitive-services/speech-service/rest-speech-to-text#pronunciation-assessment-parameters) for details.
            Under normal circumstances, you shouldn't have to use this property directly.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_GradingSystem">
            <summary>
            The point system for pronunciation score calibration (FivePoint or HundredMark).
            Under normal circumstances, you shouldn't have to use this property directly.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_Granularity">
            <summary>
            The pronunciation evaluation granularity (Phoneme, Word, or FullText).
            Under normal circumstances, you shouldn't have to use this property directly.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_EnableMiscue">
            <summary>
            Indicates miscue calculation state.
            When enabled, the pronounced words will be compared to the reference text,
            and will be marked with omission/insertion based on the comparison. The default setting is false.
            Under normal circumstances, you shouldn't have to use this property directly.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_PhonemeAlphabet">
            <summary>
            The pronunciation evaluation phoneme alphabet. The valid values are "SAPI" (default) and "IPA"
            Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.PhonemeAlphabet"/>.
            Added in version 1.20.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_NBestPhonemeCount">
            <summary>
            The pronunciation evaluation nbest phoneme count.
            Under normal circumstances, you shouldn't have to use this property directly.
            Instead, use <see cref="P:Microsoft.CognitiveServices.Speech.PronunciationAssessment.PronunciationAssessmentConfig.NBestPhonemeCount"/>.
            Added in version 1.20.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_Json">
            <summary>
            The JSON string of pronunciation assessment parameters.
            Under normal circumstances, you shouldn't have to use this property directly.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.PronunciationAssessment_Params">
            <summary>
            Pronunciation assessment parameters.
            This property is read-only.
            Added in 1.14.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeakerRecognition_Api_Version">
            <summary>
            Speaker recognition API version.
            Added in 1.18.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechTranslation_ModelName">
            <summary>
            The name of a model to be used for speech translation.
            Do not use this property directly.
            Currently this is only valid when EmbeddedSpeechConfig is used.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.PropertyId.SpeechTranslation_ModelKey">
            <summary>
            The decryption key of a model to be used for speech translation.
            Do not use this property directly.
            Currently this is only valid when EmbeddedSpeechConfig is used.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ResultReason">
            <summary>
            Describes a recognition result.
            </summary>
            <remarks>
            See also:
            * [Text-dependent verification](/azure/cognitive-services/speech-service/get-started-speaker-recognition#text-dependent-verification)
            * [Error handling](/azure/cognitive-services/speech-service/get-started-speech-to-text#error-handling)
            </remarks>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.NoMatch">
            <summary>
            Indicates speech could not be recognized. Use <see cref="T:Microsoft.CognitiveServices.Speech.NoMatchDetails" />  for details.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.Canceled">
            <summary>
            Indicates that the recognition was canceled. Use <see cref="T:Microsoft.CognitiveServices.Speech.CancellationDetails" /> for details.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizingSpeech">
            <summary>
            Indicates the speech result contains hypothesis text.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedSpeech">
            <summary>
            Indicates the speech result contains final text that has been recognized.
            Speech recognition is now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizingIntent">
            <summary>
            Indicates the intent result contains hypothesis text and intent.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedIntent">
            <summary>
            Indicates the intent result contains final text and intent.
            Speech recognition and intent determination are now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatingSpeech">
            <summary>
            Indicates the translation result contains hypothesis text and its translation(s).
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatedSpeech">
            <summary>
            Indicates the translation result contains final text and corresponding translation(s).
            Speech recognition and translation are now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.SynthesizingAudio">
            <summary>
            Indicates the synthesized audio result contains a non-zero amount of audio data.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.SynthesizingAudioCompleted">
            <summary>
            Indicates the synthesized audio is now complete for this phrase.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizingKeyword">
            <summary>
            Indicates the speech result contains (unverified) keyword text.
            Added in 1.3.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedKeyword">
            <summary>
            Indicates that keyword recognition completed recognizing the given keyword.
            Added in 1.3.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.SynthesizingAudioStarted">
            <summary>
            Indicates the speech synthesis is now started.
            Added in 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatingParticipantSpeech">
            <summary>
            Indicates the transcription result contains hypothesis text and its translation(s) for
            other participants in the conversation.
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatedParticipantSpeech">
            <summary>
            Indicates the transcription result contains final text and corresponding translation(s)
            for other participants in the conversation. Speech recognition and translation are now
            complete for this phrase.
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatedInstantMessage">
            <summary>
            Indicates the transcription result contains the instant message and corresponding
            translation(s).
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.TranslatedParticipantInstantMessage">
            <summary>
            Indicates the transcription result contains the instant message for other participants
            in the conversation and corresponding translation(s).
            Added in 1.9.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.EnrollingVoiceProfile">
            <summary>
            Indicates the voice profile is being enrolled and more audio is needed to complete a voice profile.
            Added in 1.12.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.EnrolledVoiceProfile">
            <summary>
            Indicates the voice profile has been enrolled.
            Added in 1.12.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedSpeakers">
            <summary>
            Indicates successful identification of some speakers.
            Added in 1.12.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.RecognizedSpeaker">
            <summary>
            Indicates successful verification of a speaker.
            Added in 1.12.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.ResetVoiceProfile">
            <summary>
            Indicates a voice profile has been reset.
            Added in 1.12.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.DeletedVoiceProfile">
            <summary>
            Indicates a voice profile has been deleted.
            Added in 1.12.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ResultReason.VoicesListRetrieved">
            <summary>
            Indicates the voices list has been retrieved successfully.
            Added in 1.16.0
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.CancellationReason">
            <summary>
            Lists the possible reasons a recognition result might be canceled.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationReason.Error">
            <summary>
            Indicates that an error occurred during speech recognition.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationReason.EndOfStream">
            <summary>
            Indicates that the end of the audio stream was reached.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationReason.CancelledByUser">
            <summary>
            Indicates that request was cancelled by the user.
            Added in 1.14.0
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.CancellationErrorCode">
            <summary>
            Lists error codes possible when <see cref="T:Microsoft.CognitiveServices.Speech.CancellationReason" />
            is <see cref="F:Microsoft.CognitiveServices.Speech.CancellationReason.Error" />.
            Added in 1.1.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.NoError">
            <summary>
            No error.
            If <see cref="T:Microsoft.CognitiveServices.Speech.CancellationReason" />
            is <see cref="F:Microsoft.CognitiveServices.Speech.CancellationReason.EndOfStream" />,
            <see cref="T:Microsoft.CognitiveServices.Speech.CancellationErrorCode" />
            is set to <see cref="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.NoError" />.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.AuthenticationFailure">
            <summary>
            Indicates an authentication error.
            An authentication error occurs if subscription key or authorization token is invalid, expired,
            or does not match the region being used.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.BadRequest">
            <summary>
            Indicates that one or more recognition parameters are invalid or the audio format is not supported.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.TooManyRequests">
            <summary>
            Indicates that the number of parallel requests exceeded the number of allowed concurrent transcriptions for the subscription.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.Forbidden">
            <summary>
            Indicates that the free subscription used by the request ran out of quota.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ConnectionFailure">
            <summary>
            Indicates a connection error.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ServiceTimeout">
            <summary>
            Indicates a time-out error after waiting for response from the Speech service.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ServiceError">
            <summary>
            Indicates that an error is returned by the Speech service.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.ServiceUnavailable">
            <summary>
            Indicates that the Speech service is currently unavailable.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.RuntimeError">
            <summary>
            Indicates an unexpected runtime error.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.CancellationErrorCode.EmbeddedModelError">
            <summary>
            Indicates the embedded speech (SR or TTS) model is not available or corrupted.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionEventArgs">
            <summary>
            Contains payload for recognition events like Speech Start/End Detected.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionEventArgs.Offset">
            <summary>
            Represents the message offset in ticks (100 nanoseconds).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.RecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the recognition event payload.
            </summary>
            <returns>A string that represents the recognition event payload.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionEventType">
            <summary>
            Lists recognition event types.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionResult">
            <summary>
            Contains detailed information about result of a recognition operation.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.ResultId">
            <summary>
            Specifies the result identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Reason">
            <summary>
            Specifies status of speech recognition result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Text">
            <summary>
            Presents the recognized text in the result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Duration">
            <summary>
            Duration of the recognized speech. This does not include trailing or leading silence.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.OffsetInTicks">
            <summary>
            Offset of the recognized speech in ticks. A single tick represents one hundred nanoseconds or one ten-millionth of a second.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.RecognitionResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.RecognitionResult.ToString">
            <summary>
            Returns a string that represents the speech recognition result.
            </summary>
            <returns>A string that represents the speech recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.CancellationDetails">
            <summary>
            Contains detailed information about why a result was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.CancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.RecognitionResult)">
            <summary>
            Creates a CancellationDetails instance for the canceled SpeechRecognitionResult.
            </summary>
            <param name="result">The result that was canceled.</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.CancellationDetails.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.CancellationDetails.ErrorCode">
            <summary>
            The error code of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.CancellationDetails.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in 1.1.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.CancellationDetails.ErrorDetails">
            <summary>
            The error message of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.CancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.CancellationDetails.ToString">
            <summary>
            Returns a string that describes the cancellation.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.NoMatchDetails">
            <summary>
            Contains detailed information for NoMatch recognition results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.NoMatchDetails.FromResult(Microsoft.CognitiveServices.Speech.RecognitionResult)">
            <summary>
            Creates a NoMatchDetails instance when recognition results in <see cref="F:Microsoft.CognitiveServices.Speech.ResultReason.NoMatch"/>.
            </summary>
            <param name="result">The recognition result that was not recognized.</param>
            <returns>The NoMatchDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.NoMatchDetails.Reason">
            <summary>
            The reason the result was not recognized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.NoMatchDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Recognizer">
            <summary>
            Base class that mostly contains common event handlers.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SessionStarted">
            <summary>
            Defines event handler for session started event.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SessionStopped">
            <summary>
            Defines event handler for session stopped event.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SpeechStartDetected">
            <summary>
            Defines event handler for speech start detected event.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Recognizer.SpeechEndDetected">
            <summary>
            Defines event handler for speech end detected event.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.pointerHandle">
            <summary>
            Internal for logging.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.gch">
            <summary>
            GC handle for callbacks for context.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.disposed">
            <summary>
            disposed is a flag used to indicate if object is disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.isDisposing">
            <summary>
            Indicates whether the object is currently being disposed.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Recognizer.recognizerLock">
            <summary>
            recognizerLock is used to synchronize access to objects member variables from multiple threads
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.FireEvent_SetSessionStarted(System.IntPtr,System.IntPtr,System.IntPtr)">
             <summary>
             Define a private methods which raise a C# event when a corresponding callback is invoked from the native layer.
             </summary>
            
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Recognizer.DoAsyncRecognitionAction(System.Action)">
            <summary>
            This methods checks if a recognizer is disposed before performing async recognition action.
            The Action parameter <paramref name="recoImplAction"/> can be any internal async recognition method of Speech, Translation and Intent Recognizer.
            The method is called from all async recognition methods (e.g. <see cref="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StartContinuousRecognitionAsync"/>).
            ObjectDisposedException will be thrown and the action will not be performed if its recognizer is not available anymore.
            The purpose of this method is to prevent possible race condition if async recognitions are not awaited.
            </summary>
            <param name="recoImplAction">Actual implementation.</param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ServicePropertyChannel">
            <summary>
            Lists channels used to pass property settings to service.
            Added in 1.5.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ServicePropertyChannel.UriQueryParameter">
            <summary>
            Uses URI query parameter to pass property settings to service.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.ServicePropertyChannel.HttpHeader">
            <summary>
            Uses HttpHeader to set a key/value in a HTTP header.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SessionEventArgs">
            <summary>
            Contains payload for <see cref="E:Microsoft.CognitiveServices.Speech.Recognizer.SessionStarted" />
            and <see cref="E:Microsoft.CognitiveServices.Speech.Recognizer.SessionStopped" /> events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SessionEventArgs.SessionId">
            <summary>
            Represents the session identifier.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SessionEventArgs.ToString">
            <summary>
            Returns a string that represents the session event.
            </summary>
            <returns>A string that represents the session event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SessionEventType">
            <summary>
            Lists session event types.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SourceLanguageConfig">
            <summary>
            Source Language configuration.
            Added in 1.17.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageConfig.FromLanguage(System.String)">
            <summary>
            Creates a SourceLanguageConfig instance with a source language.
            </summary>
            <param name="language">The source language</param>
            <returns>A new SourceLanguageConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageConfig.FromLanguage(System.String,System.String)">
            <summary>
            Creates a SourceLanguageConfig instance with source language and custom endpoint id. 
            A custom endpoint id corresponds to a custom model. 
            </summary>
            <param name="language">The source language</param>
            <param name="endpointId">The custom endpoint id</param>
            <returns>A new SourceLanguageConfig instance.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer">
            <summary>
            Detects the spoken language on the input audio.
            Added in version 1.17.0
            </summary>
            <remarks>
            See [Language Identification](https://aka.ms/speech/lid?pivots=programming-language-csharp) document.
            </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.Canceled"/> signals that the speech to source language recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig)">
            <summary>
            Creates a new instance of SourceLanguageRecognizer that determines the source language from a different languageId modes and priorities.
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration that specifies the language(s) to look for in the source speech to synthesize</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SourceLanguageRecognizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">An instance that specifies possible source languages in the speech.</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
              
            Note: Your code needs to ensure that the authorization token is valid. Before the authorization token
            expires, your code needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will produce errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer"/>.
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.RecognizeOnceAsync">
             <summary>
             Starts source language recognition as an asynchronous operation.
             </summary>
             <remarks>
             The end of a single utterance is determined by listening for silence at the end, or until a timeout period has elapsed.
             The task returns the recognized speech in **SpeechRecognitionResult.Text**.
               
             You can call **StopContinuousRecognitionAsync** to stop recognition before a phrase has been recognized.
               
             Since this method returns only a single utterance, it is suitable only for single shot recognition like command or query. 
             For long-running multi-utterance recognition, use **StartContinuousRecognitionAsync** instead.
            
             See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
             </remarks>
             <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResult"/> </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts source language recognition on a continuous audio stream, until StopContinuousRecognitionAsync() is called.
            You must subscribe to events to receive recognition results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops a running recognition operation as soon as possible and immediately requests a result based on the
            the input that has been processed so far. This works for all recognition operations, not just continuous
            ones, and facilitates the use of push-to-talk or "finish now" buttons for manual audio endpointing.
            </summary>
            <returns>
            A task that will complete when input processing has been stopped. Result generation, if applicable for the
            input provided, may happen after this task completes and should be handled with the appropriate event.
            </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SourceLanguageRecognizer.Dispose(System.Boolean)">
            <summary>
            Disposes of the object.
            </summary>
            <param name="disposing">True to dispose managed resources.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.SpeakerIdentificationModel">
            <summary>
            Represents speaker identification model.
            Added in 1.12.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerIdentificationModel.FromProfiles(System.Collections.Generic.IEnumerable{Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile})">
            <summary>
            Creates a speaker identification model from the voice profiles.
            </summary>
            <param name="profiles">a collection of voice profiles.</param>
            <returns>The speaker identification model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerIdentificationModel.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult">
            <summary>
            Contains detailed information about result of a speaker recognition operation.
            Added in 1.12.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult.ResultId">
            <summary>
            Specifies the result identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult.Reason">
            <summary>
            Specifies status of speaker recognition result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult.ProfileId">
            <summary>
            Presents the recognized profile id.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult.Score">
            <summary>
            Presents the similarity score of the recognized speaker.
            The score is a float number indicating the similarity between input audio and targeted voice profile.This number is between 0 and 1. A higher number means higher similarity.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult.ToString">
            <summary>
            Returns a string that represents the speaker recognition result.
            </summary>
            <returns>A string that represents the speaker recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails">
            <summary>
            Contains detailed information about why a result was canceled.
            Added in 1.12.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionResult)">
            <summary>
            Contains the detailed information about why a speaker recognition result was canceled.
            </summary>
            <param name="result">The result that was canceled.</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognitionCancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer">
            <summary>
            Verifies or identifies speakers in a conversation by matching unique voice characteristics.
            </summary>
            <remarks>
            See also: [What is Speaker Recognition (Preview)?](/azure/cognitive-services/speech-service/speaker-recognition-overview)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
             <summary>
             Creates a speaker recognizer.
             </summary>
             <param name="speechConfig">The speech configuration to use.</param>
             <param name="audioConfig">The audio config to use.</param>
             <returns></returns>
            
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer.Properties">
            <summary>
            Gets the collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer"/>.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer.RecognizeOnceAsync(Microsoft.CognitiveServices.Speech.Speaker.SpeakerVerificationModel)">
            <summary>
            Verify the speaker in the speaker verification model as an asynchronous operation.
            </summary>
            <param name="model">A SpeakerVerificationModel.</param>
            <returns>An asynchronous operation representing verifying the speaker in the model.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer.RecognizeOnceAsync(Microsoft.CognitiveServices.Speech.Speaker.SpeakerIdentificationModel)">
            <summary>
            Identify the speakers in the speaker identification model as an asynchronous operation.
            </summary>
            <param name="model">A speaker identification model.</param>
            <returns>An asynchronous operation representing identifying the speakers in the model.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerRecognizer.Dispose(System.Boolean)">
             <summary>
            
             </summary>
             <param name="disposeManaged"></param>
             <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.SpeakerVerificationModel">
            <summary>
            Represents speaker verification model.
            Added in 1.12.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerVerificationModel.FromProfile(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile)">
            <summary>
            Creates a speaker verification model from a voice profile.
            </summary>
            <param name="profile">A voice profile.</param>
            <returns>The speaker verification model being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.SpeakerVerificationModel.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile">
            <summary>
            Represents a speaker's unique voice.
            Added in 1.12.0
            </summary>
            <remarks>
            See also: [Get started with Speaker recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile.#ctor(System.String,Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType)">
             <summary>
             Creates a VoiceProfile instance.
             </summary>
             <param name="id">An unique id.</param>
             <param name="type">VoiceProfileType.</param>
            
             <returns></returns>
            
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile.Id">
            <summary>
            Gets a voice profile identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile.Type">
            <summary>
            Gets the voice profile type.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile.Dispose(System.Boolean)">
             <summary>
            
             </summary>
             <param name="disposeManaged"></param>
             <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient">
            <summary>
            Creates and manages voice profiles that represent
            unique, distinguishable human voices.
            These profiles are used to identify individual
            enrolled speakers and customize speech recognition
            experiences.
            Added in 1.12.0
            </summary>
            <remarks>
            See also: [Get started with Speaker Recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
             <summary>
             Creates an instance of VoiceProfileClient.
             </summary>
             <param name="speechConfig">The speech configuration to use.</param>
             <returns></returns>
            
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.Properties">
            <summary>
            Gets the collection of properties and their values defined for this instance of <see cref="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient"/>.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.CreateProfileAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType,System.String)">
            <summary>
            Creates a voice profile as an asynchronous operation.
            </summary>
            <remarks>
            See also: [Text-dependent verification](/azure/cognitive-services/speech-service/get-started-speaker-recognition#text-dependent-verification)
            </remarks>
            <param name="voiceProfileType">A voice profile type.</param>
            <param name="locale">a locale, e.g "en-us"</param>
            <returns>An asynchronous operation representing the result of creating a voice profile.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.GetAllProfilesAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType)">
            <summary>
            Gets all voice profiles of a given profile type.
            </summary>
            <param name="voiceProfileType">A voice profile type.</param>
            <returns>An asynchronous operation representing the result of getting a collection of voice profiles.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.EnrollProfileAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Enrolls a voice profile asynchronously.
            </summary>
            <remarks>
            See also: [Get started with Speaker Recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
            <param name="voiceProfile">A voice profile.</param>
            <param name="audioConfig">An audio config.</param>
            <returns>An asynchronous operation representing the result of enrollment of a voice profile.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.DeleteProfileAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile)">
            <summary>
            Deletes a voice profile asynchronously.
            </summary>
            <param name="voiceProfile">A voice profile.</param>
            <returns> An asynchronous operation representing the result of deleting a voice profile.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.ResetProfileAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile)">
            <summary>
            Resets a voice profile asynchronously.
            </summary>
            <param name="voiceProfile">A voice profile.</param>
            <returns> An asynchronous operation representing the result of reset a voice profile.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.RetrieveEnrollmentResultAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfile)">
            <summary>
            Retrieves an enrollment result associated with a voice profile.
            </summary>
            <param name="voiceProfile">a voice profile object.</param>
            <returns>An asynchronous operation representing the result of enrollment of a voice profile.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.GetActivationPhrasesAsync(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType,System.String)">
            <summary>
            Get voice profile activation phrase result as an asynchronous operation.
            </summary>
            <remarks>
            See also: [Get started with Speaker Recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
            <param name="voiceProfileType">A voice profile type.</param>
            <param name="locale">a locale, e.g "en-us"</param>
            <returns>An asynchronous operation representing the result of creating a voice profile.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileClient.Dispose(System.Boolean)">
             <summary>
            
             </summary>
             <param name="disposeManaged"></param>
             <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult">
            <summary>
            Contains the result of enrolling a voice profile.
            Added in 1.12.0
            </summary>
            <remarks>
            See also: [Get started with Speaker recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.ResultId">
            <summary>
            Specifies the result identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.Reason">
            <summary>
            Specifies reason of the enrollment result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.ProfileId">
            <summary>
            Specifies profile id in the enrollment result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.EnrollmentsCount">
            <summary>
            Number of enrollment audio segments accepted for this profile.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.EnrollmentsLength">
            <summary>
            The total duration of enrollment audio accepted for this profile.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.RemainingEnrollmentsCount">
            <summary>
            Number of enrollment audio segments needed to complete profile enrollment.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.RemainingEnrollmentsSpeechLength">
            <summary>
            The duration of non-silence speech audio still needed to complete profile enrollment.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.EnrollmentsSpeechLength">
            <summary>
            The duration of non-silence speech audio used across all voice profiles used by this client.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.AudioLength">
            <summary>
            Duration of audio in this enrollment.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.AudioSpeechLength">
            <summary>
            Duration of non-silence speech audio in this enrollment.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.CreatedTime">
            <summary>
            A textual representation of the creation time of the voice profile.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.LastUpdatedDateTime">
            <summary>
            A textual representation of the time the voice profile was last updated.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult.ToString">
            <summary>
            Returns a string that represents the enrollment result.
            </summary>
            <returns>A string that represents the enrollment result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails">
            <summary>
            Contains the cancellation details of an enrollment result.
            Added in 1.12.0
            </summary>
            <remarks>
            See also: [Get started with Speaker recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentResult)">
            <summary>
            Create an object that represents the details of a canceled enrollment result.
            </summary>
            <param name="result">A voice profile enrollment result object.</param>
            <returns>A voice profile enrollment cancellation details object.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.Reason">
            <summary>
            The reason the enrollment was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.ErrorCode">
            <summary>
            The error code in case of an unsuccessful enrollment (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful enrollment (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileEnrollmentCancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult">
            <summary>
            Contains the result of requesting voice profile activation phrases.
            Added in 1.18.0
            </summary>
            <remarks>
            See also: [Get started with Speaker recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult.ResultId">
            <summary>
            Specifies the result identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult.Reason">
            <summary>
            Specifies reason of the activation phrase result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult.Phrases">
            <summary>
            Activation Phrases received.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult.ToString">
            <summary>
            Returns a string that represents the activation phrase result.
            </summary>
            <returns>A string that represents the activation phrase result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails">
            <summary>
            Contains the cancellation details of an activation phrase result.
            Added in 1.18.0
            </summary>
            <remarks>
            See also: [Get started with Speaker recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseResult)">
            <summary>
            Create an object that represents the details of a canceled activation phrase result.
            </summary>
            <param name="result">A voice profile phrase result object.</param>
            <returns>A voice profile phrase cancellation details object.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.Reason">
            <summary>
            The reason the activation phrase request was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.ErrorCode">
            <summary>
            The error code in case of an unsuccessful activation phrase request (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.Reason"/> is set to **Error**).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful activation phrase request (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.Reason"/> is set to **Error**).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfilePhraseCancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileResult">
            <summary>
            Contains the result of processing a voice profile.
            Added in 1.12.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileResult.ResultId">
            <summary>
            Specifies the result identifier.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileResult.Reason">
            <summary>
            Specifies the reason of voice profile result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileResult.ToString">
            <summary>
            Returns a string that represents the voice profile result.
            </summary>
            <returns>A string that represents the voice profile result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails">
            <summary>
            Contains detailed information about why a voice profile action was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileResult)">
            <summary>
            Creates an **VoiceProfileCancellationDetails** instance for the canceled **VoiceProfileResult**.
            </summary>
            <param name="result">The result that was canceled.</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.Reason">
            <summary>
            Reason the voice profile action was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.ErrorCode">
            <summary>
            Error code of an unsuccessful voice profile action (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.ErrorDetails">
            <summary>
            Error message of an unsuccessful voice profile action (<see cref="P:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileCancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType">
            <summary>
            List of speaker verification types.
            </summary>
            <remarks>
            See also: [Get started with Speaker recognition](/azure/cognitive-services/speech-service/get-started-speaker-recognition)
            </remarks>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType.TextIndependentIdentification">
            <summary>
            Text independent speaker identification.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType.TextDependentVerification">
            <summary>
             Text dependent speaker verification.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.Speaker.VoiceProfileType.TextIndependentVerification">
            <summary>
            Text independent verification.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.ClassLanguageModel">
            <summary>
            Represents a list of grammars for dynamic grammar scenarios.
            Added in 1.7.0
            </summary>
            <remarks>
            ClassLanguageModels are only usable in specific scenarios and are not generally available.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ClassLanguageModel.FromStorageId(System.String)">
            <summary>
            Creates a class language model from a storage ID.
            </summary>
            <param name="storageId">The persisted storage ID of the language model.</param>
            <returns>The grammar.</returns>
            <remarks>
            Creating a ClassLanguageModel from a storage ID is only usable in specific scenarios and is not generally available.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ClassLanguageModel.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hgrammar">Grammar handle.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.ClassLanguageModel.AssignClass(System.String,Microsoft.CognitiveServices.Speech.Grammar)">
            <summary>
            Adds a single grammar to the current grammar list
            </summary>
            <param name="className">The name of the class the grammar represents.</param>
            <param name="grammar">The grammar to add</param>
            <remarks>
            Currently Class Language Models are the only support grammars to add.
            </remarks>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Grammar">
            <summary>
            Represents base class grammar for customizing speech recognition.
            Added in 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Grammar.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hgrammar">Grammar handle.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Grammar.FromStorageId(System.String)">
            <summary>
            Creates a Grammar from its storage identifier.
            Added in 1.7.0
            </summary>
            <param name="storageId">The storage ID for the grammar.</param>
            <returns>A reference to the grammar</returns>
            <remarks>
            Creating a grammar from a storage ID is only usable in specific scenarios and is not generally available.
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Grammar.NativeHandle">
            <summary>
            Internal native handle property.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.RecognitionFactorScope">
            <summary>
            Lists the scope that a recognition factor applies to.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.RecognitionFactorScope.PartialPhrase">
            <summary>
            A recognition factor will apply to grammars that can be referenced as individual partial phrases. Currently only applies to PhraseListGrammar instances.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.GrammarList">
            <summary>
            Represents a list of grammars for dynamic grammar scenarios.
            Added in 1.7.0
            </summary>
            <remarks>
            GrammarLists are only usable in specific scenarios and are not generally available.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarList.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer)">
            <summary>
            Creates a grammar list for the specified recognizer.
            </summary>
            <param name="recognizer">The recognizer from which to obtain the grammar list.</param>
            <returns>The grammar list associated with the recognizer.</returns>
            <remarks>
            Creating a grammar list from a recognizer is only usable in specific scenarios and is not generally available.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarList.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hgrammar">Grammar handle.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarList.Add(Microsoft.CognitiveServices.Speech.Grammar)">
            <summary>
            Adds a single grammar to the current grammar list
            </summary>
            <param name="grammar">The grammar to add</param>
            <remarks>
            Add support grammar grammars using <see cref="T:Microsoft.CognitiveServices.Speech.ClassLanguageModel" />.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarList.SetRecognitionFactor(System.Double,Microsoft.CognitiveServices.Speech.RecognitionFactorScope)">
            <summary>
            Sets the recognition factor applied to all grammars in a recognizer's **GrammarList**.
            </summary>
            <param name="factor">The recognition factor to apply</param>
            <param name="scope">The scope for the recognition factor being set</param>
            <remarks>
            The recognition factor is a numerical value greater than 0 modifies the default weight applied to supplied grammars.
            Setting the recognition factor to 0 will disable the supplied grammars.
            The default recognition factor is 1.
            </remarks>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.GrammarPhrase">
            <summary>
            Represents a phrase that can be spoken by the user.
            Added in 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarPhrase.From(System.String)">
            <summary>
            Creates a grammar phrase using the specified phrase text.
            </summary>
            <param name="text">The text representing a phrase that can be spoken by the user.</param>
            <returns>A shared pointer to a grammar phrase.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarPhrase.Dispose(System.Boolean)">
            <summary>
            Clean up resources.
            </summary>
            <param name="disposing"></param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.GrammarPhrase.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hphrase">Grammar phrase handle.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.GrammarPhrase.NativeHandle">
            <summary>
            Internal native handle property.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.PhraseListGrammar">
            <summary>
            Identifies known phrases in audio data. Added in 1.5.0
            </summary>
            <remarks>
            Not available in all languages. For a list of languages and sample code, see [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text).
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer)">
            <summary>
            Creates a phrase list grammar for the specified recognizer.
            </summary>
            <remarks>
            Not available in all languages. For a list of languages and sample code, see [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text).
            </remarks>
            <param name="recognizer">The recognizer from which to obtain the phrase list grammar.</param>
            <returns>The phrase list grammar.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.#ctor(System.IntPtr)">
            <summary>
            Internal constructor. Creates a new instance using the provided native handle.
            </summary>
            <param name="hgrammar">Grammar handle.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.AddPhrase(System.String)">
            <summary>
            Adds a simple phrase to listen for in the speech.
            </summary>
            <param name="text">The phrase to be added.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.Clear">
            <summary>
            Clears all phrases from the phrase list.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.PhraseListGrammar.FromRecognizer(Microsoft.CognitiveServices.Speech.Recognizer,System.String)">
            <summary>
            Internal. Creates a phrase list grammar for the specified recognizer, with the specified name.
            </summary>
            <param name="recognizer">The recognizer from which to obtain the phrase list grammar.</param>
            <param name="name">The name of the phrase list grammar to create.</param>
            <returns>The phrase list grammar.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechConfig">
             <summary>
             Information about your subscription, including your key and region, endpoint, host, or authorization token.
             </summary>
             <remarks>
             Calls to the Speech service require a **SpeechConfig** object.
            
             There are a few ways that you can initialize a SpeechConfig:
             * Using <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)" />: pass in a key and the associated region.
             * Using <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)" />: pass in a Speech service endpoint. A key or authorization token is optional.
             * Using <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri,System.String)" />: pass in a host address. A key or authorization token is optional.
            
             See also: [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text)
             </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromSubscription(System.String,System.String)">
             <summary>
             Creates an instance of speech configuration with specified subscription key and region.
            
             See also: [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text?tabs=script%2Cwindowsinstall&amp;pivots=programming-language-csharp)
             </summary>
             <param name="subscriptionKey">The subscription key. To create or find your key and region, see [Find keys and region](/azure/cognitive-services/speech-service/overview#find-keys-and-region).</param>
             <param name="region">Region identifier for the subscription key.</param>
             <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromAuthorizationToken(System.String,System.String)">
            <summary>
            Creates an instance of the speech config with specified authorization token and region.
            </summary>
            <remarks>
            The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            As configuration values are copied when creating a new recognizer, the new token value will not apply to recognizers that have already been created.
            For recognizers that have been created before, you need to set authorization token of the corresponding recognizer
            to refresh the token. Otherwise, the recognizers will encounter errors during recognition.
            </remarks>
            <param name="authorizationToken">The authorization token.</param>
            <param name="region">Region identifier for the authorization token.</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri,System.String)">
            <summary>
            Creates an instance of SpeechConfig with a custom endpoint and subscription key.
            </summary>
            <remarks>
            * This method is only used for a non-standard resource path or parameter overrides. To change the host name with standard resource paths, use **FromHost** instead.
            * The query parameters specified in the endpoint URI are not changed, even if they are set by any other API call.
              For example, if the recognition language is defined in the URI query parameter as "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US", the language set in the URI takes precedence, and "de-DE" remains the expected language.
              Since parameters included in the endpoint URI take priority, only parameters that are not specified in the endpoint URI can be set by other APIs.
            * To use an authorization token with FromEndpoint, use FromEndpoint(System.Uri), and then set the AuthorizationToken property on the new SpeechConfig instance.
            </remarks>
            <param name="endpoint">The service endpoint to connect to.</param>
            <param name="subscriptionKey">The subscription key. To create or find your key and region, see [Find keys and region](/azure/cognitive-services/speech-service/overview#find-keys-and-region).</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromEndpoint(System.Uri)">
            <summary>
            Creates an instance of the speech config with specified endpoint. Added in 1.5.0
            </summary>
            <remarks>
            * This method is only used for a non-standard resource path or parameter overrides. To change the host name with standard resource paths, use **FromHost** instead.
            * The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
              For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US", the language setting in URI takes precedence, and the effective language is "de-DE".
              Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            * If the endpoint requires a subscription key for authentication, use FromEndpoint(System.Uri, string) to pass the subscription key as parameter.
              To use an authorization token with FromEndpoint, use this method to create a SpeechConfig instance, and then set the AuthorizationToken property on the created SpeechConfig instance.
            </remarks>
            <param name="endpoint">The service endpoint to connect to.</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri,System.String)">
            <summary>
            Creates a **SpeechConfig** instance with a specific host and subscription key.
            This method is intended only for users who use a non-default service host. Standard resource path will be assumed.
            For services with a non-standard resource path or no path at all, use **FromEndpoint** instead. Added in 1.8.0
            </summary>
            <remarks>
            * Query parameters are not allowed in the host URI and must be set by other APIs.
            * To use an authorization token with **FromHost**, use **FromHost(System.Uri)**, and then set the **AuthorizationToken** property on the created **SpeechConfig** instance.
            </remarks>
            <param name="host">The service host to connect to. Format is "protocol://host:port" where ":port" is optional.</param>
            <param name="subscriptionKey">The subscription key. To create or find your key and region, see [Find keys and region](/azure/cognitive-services/speech-service/overview#find-keys-and-region).</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri)">
             <summary>
             Creates an instance of the speech config with specified host.
             This method is intended only for users who use a non-default service host. Standard resource path will be assumed.
             For services with a non-standard resource path or no path at all, use FromEndpoint instead. Added in 1.
             </summary>
             <remarks>
             * Query parameters are not allowed in the host URI and must be set by other APIs.
             * If the host requires a subscription key for authentication, use <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.FromHost(System.Uri,System.String)" /> to pass the subscription key as parameter.
             * To use an authorization token with FromHost, use this method to create a SpeechConfig instance, and then set the AuthorizationToken property on the created SpeechConfig instance.
             </remarks>
            
             <param name="host">The service host to connect to. Format is "protocol://host:port" where ":port" is optional.</param>
             <returns>A speech config instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SubscriptionKey">
            <summary>
            Subscription key.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.Region">
            <summary>
            Region.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.AuthorizationToken">
            <summary>
            Gets or sets the authorization token.
            </summary>
            <remarks>
            You must assure that the authorization token is valid. Before the authorization token
            expires, you need to refresh it by calling this setter with a new valid token.
            Configuration values are copied when creating a new recognizer, so the new token value will not apply to recognizers that have already been created.
            For recognizers that have been created before, you need to set the authorization token of the corresponding recognizer
            to refresh the token. Otherwise, the recognizers will trigger errors during recognition.
            Changed in 1.3.0
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechRecognitionLanguage">
            <summary>
            Specifies the name of spoken language to be recognized, in BCP-47 format.
            </summary>
            <remarks>
            See BCP-47 locale values available to speech-to-text at [Speech-to-text](/azure/cognitive-services/speech-service/language-support#speech-to-text).
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.OutputFormat">
             <summary>
             Gets or sets the speech recognition output format: simple or detailed.
             </summary>
             <remarks>
             This output format is for speech recognition results. Use <see cref="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisOutputFormat"/>
             and <see cref="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetSpeechSynthesisOutputFormat(Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat)"/> to get/set synthesized audio output format.
            
             See also: [Customize audio format](/azure/cognitive-services/speech-service/get-started-text-to-speech#customize-audio-format)
             </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.EndpointId">
            <summary>
            Gets or sets the endpoint ID of a custom speech model to use for speech recognition, or a custom voice model for speech synthesis.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisLanguage">
            <summary>
            Gets or sets the speech synthesis language, e.g. en-US.
            Added in 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisVoiceName">
            <summary>
            Gets or sets the speech synthesis voice.
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Standard voices](/azure/cognitive-services/speech-service/language-support#text-to-speech)
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechConfig.SpeechSynthesisOutputFormat">
             <summary>
             Gets the output format of synthesized speech.
             Added in 1.4.0
            
             Example value: `riff-16khz-16bit-mono-pcm`
             </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetSpeechSynthesisOutputFormat(Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat)">
            <summary>
            Sets the speech synthesis output format.
            Added in 1.4.0
            </summary>
            <remarks>
            See also: [Customize audio format](/azure/cognitive-services/speech-service/get-started-text-to-speech#customize-audio-format)
            </remarks>
            <param name="format">The synthesis output format ID (e.g. Riff16Khz16BitMonoPcm).</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32,System.String,System.String)">
             <summary>
             Configures proxy with username-password pair.
             Added in 1.1.0
            
             Note: Proxy functionality is not available on macOS. This method will have no effect on the macOS platform.
             </summary>
             <param name="proxyHostName">The host name of the proxy server, without the protocol scheme (http://)</param>
             <param name="proxyPort">The port number of the proxy server.</param>
             <param name="proxyUserName">The user name of the proxy server.</param>
             <param name="proxyPassword">The password of the proxy server.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProxy(System.String,System.Int32)">
            <summary>
            Configures proxy.
            Added in 1.3.0
            </summary>
            <param name="proxyHostName">The host name of the proxy server.</param>
            <param name="proxyPort">The port number of the proxy server.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProperty(System.String,System.String)">
            <summary>
            Sets a property using a string name.
            </summary>
            <remarks>
            A small number of properties are stored using string names.
            In most cases, you will use **SetProperty(PropertyId, String)** instead of this method.
            </remarks>
            <param name="name">Name of the property</param>
            <param name="value">Value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProperty(Microsoft.CognitiveServices.Speech.PropertyId,System.String)">
            <summary>
            Sets the value of a property specified by a value in the <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId" /> enumeration.
            Added in 1.3.0
            </summary>
            <param name="id">PropertyId of the property</param>
            <param name="value">New value of the property</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.GetProperty(System.String)">
            <summary>
            Searches for the property that has this string name.
            </summary>
            <remarks>
            A small number of properties are stored using string names.
            In most cases, you will use <see cref="M:Microsoft.CognitiveServices.Speech.PropertyCollection.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)" /> instead of this method.
            </remarks>
            <param name="name">Name of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.GetProperty(Microsoft.CognitiveServices.Speech.PropertyId)">
            <summary>
            Searches for the property named with this <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId" /> enumeration.
            Added in 1.3.0
            </summary>
            <param name="id">PropertyId of the property</param>
            <returns>Value of the property</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetServiceProperty(System.String,System.String,Microsoft.CognitiveServices.Speech.ServicePropertyChannel)">
            <summary>
            Enables preview of new service features.
            </summary>
            <remarks>
            Used to configure services that are in Preview, and are not yet Generally Available. 
            This method might appear in some samples of preview services.
            </remarks>
            <param name="name">The property name.</param>
            <param name="value">The property value.</param>
            <param name="channel">The channel used to pass the specified property to service.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.SetProfanity(Microsoft.CognitiveServices.Speech.ProfanityOption)">
            <summary>
            Sets profanity option. The profanity option can remove profane words, or replace their letters with stars.
            Added in 1.5.0
            </summary>
            <param name="profanity">The profanity option to set.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.EnableAudioLogging">
            <summary>
            Enable audio and content logging in the service.
            Added in 1.5.0
            </summary>
            <remarks>
            Audio and content logs are stored either in Microsoft-owned storage, or in your own storage account linked
            to your Cognitive Services subscription (Bring Your Own Storage (BYOS) enabled Speech resource).
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.RequestWordLevelTimestamps">
            <summary>
            Include word-level timestamps. When audio logging is enabled, this method adds
            time details about the start point and duration of each word to the log.
            Added in 1.5.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechConfig.EnableDictation">
            <summary>
            Enable dictation during continuous recognition.
            Added in 1.5.0
            </summary>
            <remarks>
            With dictation enabled, word descriptions of sentence structures are understood.
            For example, ending a statement by saying "question mark" adds a question mark to the sentence when dictation is enabled.
            Only supported for continuous recognition.
            </remarks>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionModel">
            <summary>
            Speech recognition model information.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionModel.Name">
            <summary>
            Gets the model name.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionModel.Locales">
            <summary>
            Gets the locales of the model in BCP-47 format.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionModel.Path">
            <summary>
            Gets the model path (only valid for offline models).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionModel.Version">
            <summary>
            Gets the model version.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionModel.Dispose(System.Boolean)">
            <summary>
            Clean up resources.
            </summary>
            <param name="disposing"></param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResult">
            <summary>
            Contains result of speech recognition.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs">
            <summary>
            Contains payload of speech recognizing/recognized events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs.Result">
            <summary>
            Specifies the recognition result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs">
            <summary>
            Contains payload of speech recognition canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in 1.1.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResultExtensions">
            <summary>
            Extension methods for speech recognition result
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognitionResultExtensions.Best(Microsoft.CognitiveServices.Speech.SpeechRecognitionResult)">
            <summary>
            Returns best possible recognitions for the result if the recognizer
            was created with detailed output format.
            </summary>
            <param name="result">Recognition result.</param>
            <returns>A collection of best recognitions.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechRecognizer">
             <summary>
             Transcribes speech into text. Speech can arrive via microphone, audio file, or other audio input stream.
             </summary>
             <remarks>
             See also: [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text)
             </remarks>
             <example>
             This example uses the speech recognizer from a microphone and listens to events generated by the recognizer.
             <code language="c#">
             public async Task SpeechContinuousRecognitionAsync()
             {
                 // Creates an instance of a speech config with specified subscription key and region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 // Creates a speech recognizer from microphone.
                 using (var recognizer = new SpeechRecognizer(config))
                 {
                     // Subscribes to events.
                     recognizer.Recognizing += (s, e) => {
                         Console.WriteLine($"RECOGNIZING: Text={e.Result.Text}");
                     };
            
                     recognizer.Recognized += (s, e) => {
                         var result = e.Result;
                         Console.WriteLine($"Reason: {result.Reason.ToString()}");
                         if (result.Reason == ResultReason.RecognizedSpeech)
                         {
                                 Console.WriteLine($"Final result: Text: {result.Text}.");
                         }
                     };
            
                     recognizer.Canceled += (s, e) => {
                         Console.WriteLine($"\n    Canceled. Reason: {e.Reason.ToString()}, CanceledReason: {e.Reason}");
                     };
            
                     recognizer.SessionStarted += (s, e) => {
                         Console.WriteLine("\n    Session started event.");
                     };
            
                     recognizer.SessionStopped += (s, e) => {
                         Console.WriteLine("\n    Session stopped event.");
                     };
            
                     // Starts continuous recognition. 
                     // Uses StopContinuousRecognitionAsync() to stop recognition.
                     await recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
            
                     do
                     {
                         Console.WriteLine("Press Enter to stop");
                     } while (Console.ReadKey().Key != ConsoleKey.Enter);
            
                     // Stops recognition.
                     await recognizer.StopContinuousRecognitionAsync().ConfigureAwait(false);
                 }
             }
             </code>
             </example>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Canceled"/> signals that the speech recognition was canceled.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer configured to receive speech from the default microphone.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer using EmbeddedSpeechConfig, configured to receive speech from the default microphone.
            Added in 1.19.0
            </summary>
            <param name="speechConfig">Embedded speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer using HybridSpeechConfig, configured to receive speech from the default microphone.
            </summary>
            <param name="speechConfig">Hybrid speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer configured to receive speech from an audio source specified in an AudioConfig object. 
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer using EmbeddedSpeechConfig, configured to receive speech from an audio source specified in an AudioConfig object. 
            Added in 1.19.0
            </summary>
            <param name="speechConfig">Embedded speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer using HybridSpeechConfig, configured to receive speech from an audio source specified in an AudioConfig object. 
            </summary>
            <param name="speechConfig">Hybrid speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,System.String)">
            <summary>
            Creates a new instance of SpeechRecognizer configured to receive speech in a particular language.
            Added in 1.9.0
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="language">The source language</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,System.String,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer configured to receive speech in a particular language from an audio source specified in an AudioConfig object.
            Added in 1.9.0
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="language">The source language</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.SourceLanguageConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer.
            Added in 1.9.0
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="sourceLanguageConfig">The source language config</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.SourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer.
            Added in 1.9.0
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="sourceLanguageConfig">Language of the source speech, in BCP-47 format.</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer that determines the source language from a list of options.
            Added in 1.9.0
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration that specifies the language(s) to look for in the source speech to recognize</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer, using EmbeddedSpeechConfig, that determines the source language from a list of options.
            Added in 1.20.0
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Embedded speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration for auto-detecting the source language</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer, using HybridSpeechConfig, that determines the source language from a list of options.
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Hybrid speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration for auto-detecting the source language</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer.
            Added in 1.9.0
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">An instance that specifies possible source languages in the speech.</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer, using EmbeddedSpeechConfig, that determines the source language from a list of options.
            Added in 1.20.0
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Embedded speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration for auto-detecting the source language</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechRecognizer, using HybridSpeechConfig, that determines the source language from a list of options.
            </summary>
            <remarks>
            See also: [Automatic language detection for speech to text](/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp)
            </remarks>
            <param name="speechConfig">Hybrid speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">Configuration for auto-detecting the source language</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.EndpointId">
            <summary>
            Gets the endpoint ID of a custom speech model to use for speech recognition.
            </summary>
            <returns>Endpoint ID of a custom speech model</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
              
            Note: Your code needs to ensure that the authorization token is valid. Before the authorization token
            expires, your code needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will produce errors during recognition.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that was set when the recognizer was created.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.OutputFormat">
            <summary>
            Gets the output format setting.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognizer"/>.
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.RecognizeOnceAsync">
             <summary>
             Starts speech recognition as an asynchronous operation.
             </summary>
             <remarks>
             The end of a single utterance is determined by listening for silence at the end, or until a timeout period has elapsed.
             The task returns the recognized speech in **SpeechRecognitionResult.Text**.
               
             You can call **StopContinuousRecognitionAsync** to stop recognition before a phrase has been recognized.
               
             Since this method returns only a single utterance, it is suitable only for single shot recognition like command or query. 
             For long-running multi-utterance recognition, use **StartContinuousRecognitionAsync** instead.
            
             See also: [Get started with speech-to-text](/azure/cognitive-services/speech-service/get-started-speech-to-text)
             </remarks>
             <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.SpeechRecognitionResult"/> </returns>
             <example>
             The following example creates a speech recognizer, and then gets and prints the recognition result.
             <code language="c#">
             public async Task SpeechSingleShotRecognitionAsync()
             {
                 // Creates an instance of a speech config with specified subscription key and region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
             
                 // Creates a speech recognizer using microphone as audio input. Default language: en-us
                 using (var recognizer = new SpeechRecognizer(config))
                 {
                     Console.WriteLine("Say something...");
             
                     // Starts speech recognition, and returns after a single utterance is recognized. 
                     // The end of a single utterance is determined by listening for silence at the end 
                     // or until a timeout period has elapsed.  The task returns the
                     // recognition text as result.
                     //
                     // Note: Since RecognizeOnceAsync() returns only a single utterance, 
                     // it is suitable only for single shot recognition like command or query.
                     // For long-running multi-utterance recognition, 
                     // use StartContinuousRecognitionAsync() instead.
             
                     var result = await recognizer.RecognizeOnceAsync();
             
                     // Checks result.
                     if (result.Reason == ResultReason.RecognizedSpeech)
                     {
                         Console.WriteLine($"RECOGNIZED: Text={result.Text}");
                     }
                     else if (result.Reason == ResultReason.NoMatch)
                     {
                         Console.WriteLine($"NOMATCH: Speech could not be recognized.");
                     }
                     else if (result.Reason == ResultReason.Canceled)
                     {
                         var cancellation = CancellationDetails.FromResult(result);
                         Console.WriteLine($"CANCELED: Reason={cancellation.Reason}");
             
                         if (cancellation.Reason == CancellationReason.Error)
                         {
                             Console.WriteLine($"CANCELED: ErrorCode={cancellation.ErrorCode}");
                             Console.WriteLine($"CANCELED: ErrorDetails={cancellation.ErrorDetails}");
                             Console.WriteLine($"CANCELED: Did you update the subscription info?");
                         }
                     }
                 }
             }
             </code>
             </example>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts speech recognition on a continuous audio stream as an asynchronous operation, until StopContinuousRecognitionAsync() is called.
            You must subscribe to events to receive recognition results.
            </summary>
            <remarks>
            See also: [Continuous recognition](/azure/cognitive-services/speech-service/get-started-speech-to-text?tabs=windowsinstall)
            </remarks>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops a running recognition operation as soon as possible and immediately requests a result based on the
            the input that has been processed so far. This works for all recognition operations, not just continuous
            ones, and facilitates the use of push-to-talk or "finish now" buttons for manual audio endpointing.
            </summary>
            <returns>
            A task that will complete when input processing has been stopped. Result generation, if applicable for the
            input provided, may happen after this task completes and should be handled with the appropriate event.
            </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Configures the recognizer with the given keyword model. After calling this method, the recognizer is listening 
            for the keyword to start the recognition. Call StopKeywordRecognitionAsync() to end the keyword initiated recognition.
            You must subscribe to events to receive recognition results.
            </summary>
            <remarks>
            See also: [Use a keyword model with the SDK](/azure/cognitive-services/speech-service/custom-keyword-basics?pivots=programming-language-csharp#use-a-keyword-model-with-the-sdk)
            </remarks>
            <param name="model">The keyword recognition model that specifies the keyword to be recognized.</param>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.StopKeywordRecognitionAsync">
            <summary>
            Ends the keyword initiated recognition.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechRecognizer.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisBookmarkEventArgs">
            <summary>
            Contains bookmark event in synthesized speech.
            Added in 1.16.0
            </summary>
            <remarks>
            See also: <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.BookmarkReached" />
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisBookmarkEventArgs.ResultId">
            <summary>
            Specifies unique ID of speech synthesis.
            (Added in version 1.25.0)
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisBookmarkEventArgs.AudioOffset">
            <summary>
            Specifies current bookmark's offset in output audio, in ticks (100ns).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisBookmarkEventArgs.Text">
            <summary>
            Specifies the bookmark text.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisBoundaryType">
            <summary>
            Defines the boundary type of speech synthesis boundary event
            Added in version 1.21.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisBoundaryType.Word">
            <summary>
            Word boundary
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisBoundaryType.Punctuation">
            <summary>
            Punctuation boundary
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisBoundaryType.Sentence">
            <summary>
            Sentence boundary
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat">
            <summary>
            Lists synthesis output audio formats.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw8Khz8BitMonoMULaw">
            <summary>
            raw-8khz-8bit-mono-mulaw
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff16Khz16KbpsMonoSiren">
            <summary>
            riff-16khz-16kbps-mono-siren
            Unsupported by the service. Do not use this value.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz16KbpsMonoSiren">
            <summary>
            audio-16khz-16kbps-mono-siren
            Unsupported by the service. Do not use this value.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz32KBitRateMonoMp3">
            <summary>
            audio-16khz-32kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz128KBitRateMonoMp3">
            <summary>
            audio-16khz-128kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz64KBitRateMonoMp3">
            <summary>
            audio-16khz-64kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz48KBitRateMonoMp3">
            <summary>
            audio-24khz-48kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz96KBitRateMonoMp3">
            <summary>
            audio-24khz-96kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz160KBitRateMonoMp3">
            <summary>
            audio-24khz-160kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw16Khz16BitMonoTrueSilk">
            <summary>
            raw-16khz-16bit-mono-truesilk
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff16Khz16BitMonoPcm">
            <summary>
            riff-16khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff8Khz16BitMonoPcm">
            <summary>
            riff-8khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff24Khz16BitMonoPcm">
            <summary>
            riff-24khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff8Khz8BitMonoMULaw">
            <summary>
            riff-8khz-8bit-mono-mulaw
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw16Khz16BitMonoPcm">
            <summary>
            raw-16khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoPcm">
            <summary>
            raw-24khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw8Khz16BitMonoPcm">
            <summary>
            raw-8khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Ogg16Khz16BitMonoOpus">
            <summary>
            ogg-16khz-16bit-mono-opus
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Ogg24Khz16BitMonoOpus">
            <summary>
            ogg-24khz-16bit-mono-opus
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw48Khz16BitMonoPcm">
            <summary>
            raw-48khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff48Khz16BitMonoPcm">
            <summary>
            riff-48khz-16bit-mono-pcm
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio48Khz96KBitRateMonoMp3">
            <summary>
            audio-48khz-96kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio48Khz192KBitRateMonoMp3">
            <summary>
            audio-48khz-192kbitrate-mono-mp3
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Ogg48Khz16BitMonoOpus">
            <summary>
            ogg-48khz-16bit-mono-opus
            (Added in 1.16.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Webm16Khz16BitMonoOpus">
            <summary>
            webm-16khz-16bit-mono-opus
            (Added in 1.16.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Webm24Khz16BitMonoOpus">
            <summary>
            webm-24khz-16bit-mono-opus
            (Added in 1.16.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw24Khz16BitMonoTrueSilk">
            <summary>
            raw-24khz-16bit-mono-truesilk
            (Added in 1.17.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw8Khz8BitMonoALaw">
            <summary>
            raw-8khz-8bit-mono-alaw
            (Added in 1.17.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff8Khz8BitMonoALaw">
            <summary>
            riff-8khz-8bit-mono-alaw
            (Added in 1.17.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Webm24Khz16Bit24KbpsMonoOpus">
            <summary>
            webm-24khz-16bit-24kbps-mono-opus
            Audio compressed by OPUS codec in a WebM container, with bitrate of 24kbps, optimized for IoT scenario.
            (Added in 1.19.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio16Khz16Bit32KbpsMonoOpus">
            <summary>
            audio-16khz-16bit-32kbps-mono-opus
            Audio compressed by OPUS codec without container, with bitrate of 32kbps.
            (Added in 1.20.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz16Bit48KbpsMonoOpus">
            <summary>
            audio-24khz-16bit-48kbps-mono-opus
            Audio compressed by OPUS codec without container, with bitrate of 48kbps.
            (Added in 1.20.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Audio24Khz16Bit24KbpsMonoOpus">
            <summary>
            audio-24khz-16bit-24kbps-mono-opus
            Audio compressed by OPUS codec without container, with bitrate of 24kbps.
            (Added in 1.20.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw22050Hz16BitMonoPcm">
            <summary>
            raw-22050hz-16bit-mono-pcm
            Raw PCM audio at 22050Hz sampling rate and 16-bit depth.
            (Added in 1.22.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff22050Hz16BitMonoPcm">
            <summary>
            riff-22050hz-16bit-mono-pcm
            PCM audio at 22050Hz sampling rate and 16-bit depth, with RIFF header.
            (Added in 1.22.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Raw44100Hz16BitMonoPcm">
            <summary>
            raw-44100hz-16bit-mono-pcm
            Raw PCM audio at 44100Hz sampling rate and 16-bit depth.
            (Added in 1.22.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.Riff44100Hz16BitMonoPcm">
            <summary>
            riff-44100hz-16bit-mono-pcm
            PCM audio at 44100Hz sampling rate and 16-bit depth, with RIFF header.
            (Added in 1.22.0)
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SpeechSynthesisOutputFormat.AmrWb16000Hz">
            <summary>
            amr-wb-16000hz
            AMR-WB audio at 16kHz sampling rate.
            (Added in 1.24.0)
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult">
            <summary>
            Contains detailed information about result of a speech synthesis operation.
            Added in 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.ResultId">
            <summary>
            Specifies unique ID of speech synthesis result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Reason">
            <summary>
            Specifies status of speech synthesis result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.AudioDuration">
            <summary>
            Specifies the time duration of synthesized audio.
            Only valid for completed synthesis.
            Added in version 1.21.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.AudioData">
            <summary>
            Presents the synthesized audio in the result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
            <remarks>
            Property keys appear in <see cref="T:Microsoft.CognitiveServices.Speech.PropertyId"/> and begin with
            SpeechServiceResponse_.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisResult.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails">
            <summary>
            Contains detailed information about why a speech synthesis result was canceled.
            Added in 1.4.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.FromResult(Microsoft.CognitiveServices.Speech.SpeechSynthesisResult)">
            <summary>
            Creates an instance of SpeechSynthesisCancellationDetails object for the canceled SpeechSynthesisResult.
            </summary>
            <param name="result">The result that was canceled.</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.FromStream(Microsoft.CognitiveServices.Speech.AudioDataStream)">
            <summary>
            Creates an instance of SpeechSynthesisCancellationDetails object for the canceled AudioDataStream.
            </summary>
            <param name="stream">The stream that was canceled</param>
            <returns>The CancellationDetails object being created.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.Reason">
            <summary>
            The reason the synthesis was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.ErrorCode">
            <summary>
            The error code in case of an unsuccessful synthesis (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful synthesis (<see cref="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisCancellationDetails.ToString">
            <summary>
            Returns a string that represents the cancellation details.
            </summary>
            <returns>A string that represents the cancellation details.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisEventArgs">
            <summary>
            Contains payload of speech synthesis events.
            Added in 1.4.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisEventArgs.Result">
            <summary>
            Specifies the synthesis result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisEventArgs.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisVisemeEventArgs">
            <summary>
            Contains facial pose events that correspond to time-based offsets in synthesized speech.
            Added in 1.16.0
            </summary>
            <remarks>
            See also:
            * <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.VisemeReceived" />
            * [Get facial pose events](/azure/cognitive-services/speech-service/how-to-speech-synthesis-viseme?pivots=programming-language-csharp)
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisVisemeEventArgs.ResultId">
            <summary>
            Specifies unique ID of speech synthesis.
            (Added in version 1.25.0)
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisVisemeEventArgs.AudioOffset">
            <summary>
            Specifies the time offset of the current viseme in output audio, in ticks (100ns).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisVisemeEventArgs.VisemeId">
            <summary>
            Specifies current viseme ID.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisVisemeEventArgs.Animation">
            <summary>
            Specifies the animation type of the viseme event. Could be svg or other animation format.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs">
            <summary>
            Contains location and length details about words in synthesized speech.
            Added in 1.7.0
            </summary>
            <remarks>
            See also: <see cref="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.WordBoundary" />
            </remarks>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.ResultId">
            <summary>
            Specifies unique ID of speech synthesis.
            (Added in version 1.25.0)
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.AudioOffset">
            <summary>
            Specifies current word's offset in output audio, in ticks (100ns).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.Duration">
            <summary>
            Time duration of the audio.
            Added in version 1.21.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.TextOffset">
            <summary>
            Specifies current word's text offset in input text, in characters.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.WordLength">
            <summary>
            Specifies current word's length, in characters.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.Text">
            <summary>
            The text.
            Added in version 1.21.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.BoundaryType">
            <summary>
            Word boundary type.
            Added in version 1.21.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesisWordBoundaryEventArgs.Dispose(System.Boolean)">
            <summary>
            This method performs cleanup of resources.
            The Boolean parameter <paramref name="disposing"/> indicates whether the method is called from <see cref="M:System.IDisposable.Dispose"/> (if <paramref name="disposing"/> is true) or from the finalizer (if <paramref name="disposing"/> is false).
            Derived classes should override this method to dispose resource if needed.
            </summary>
            <param name="disposing">Flag to request disposal.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechSynthesizer">
            <summary>
            Performs speech synthesis to speaker, file, or other audio output streams, and gets synthesized audio as result.
            Updated in 1.16.0
            </summary>
            <remarks>
            See also: [Get started with text-to-speech](/azure/cognitive-services/speech-service/get-started-text-to-speech)
            </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisStarted">
            <summary>
            Signals that speech synthesis has started.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Synthesizing">
            <summary>
            Signals that speech synthesis is ongoing. This event fires each time the SDK receives an audio chunk from the Speech service.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisCompleted">
            <summary>
            Signals that speech synthesis has completed.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SynthesisCanceled">
            <summary>
            Signals that the speech synthesis was canceled.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.WordBoundary">
            <summary>
            Signals that a word boundary was received.
            Added in 1.7.0
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.VisemeReceived">
            <summary>
            Signals that a viseme event was received.
            Added in 1.16.0
            </summary>
            <remarks>
            See also: [Get viseme events with the Speech SDK](/azure/cognitive-services/speech-service/how-to-speech-synthesis-viseme?pivots=programming-language-csharp#get-viseme-events-with-the-speech-sdk)
            </remarks>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.BookmarkReached">
            <summary>
            Signals that a bookmark was reached.
            Added in 1.16.0
            </summary>
            <remarks>
            See also: [Improve synthesis with Speech Synthesis Markup Language (SSML)](/azure/cognitive-services/speech-service/speech-synthesis-markup)
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer using EmbeddedSpeechConfig.
            Added in 1.19.0
            </summary>
            <param name="speechConfig">Embedded speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer using HybridSpeechConfig.
            </summary>
            <param name="speechConfig">Hybrid speech configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer.
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer using EmbeddedSpeechConfig.
            Added in 1.19.0
            </summary>
            <param name="speechConfig">Embedded speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer using HybridSpeechConfig.
            </summary>
            <param name="speechConfig">Hybrid speech configuration</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a new instance of SpeechSynthesizer.
            Added in 1.13.0
            </summary>
            <param name="speechConfig">Speech configuration</param>
            <param name="autoDetectSourceLanguageConfig">The auto detect source language config</param>
            <param name="audioConfig">Audio configuration</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.SpeechSynthesizer"/>.
            Note: The property collection is only valid until the SpeechSynthesizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.AuthorizationToken">
            <summary>
            Gets/sets authorization token used to communicate with the service.
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the synthesizer will encounter errors while speech synthesis.
            Added in 1.7.0
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SpeakTextAsync(System.String)">
            <summary>
            Synthesize speech from plain text synchronously (returns when done synthesizing).
            </summary>
            <param name="text">Plain text to synthesize.</param>
            <returns>A task representing the completed synthesis operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SpeakSsmlAsync(System.String)">
            <summary>
            Synthesize speech from SSML synchronously (returns when done synthesizing).
            </summary>
            <param name="ssml">The SSML to synthesize.</param>
            <returns>A task representing the started synthesis operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.StartSpeakingTextAsync(System.String)">
            <summary>
            Queue speech synthesis task from plain text as an asynchronous operation.
            </summary>
            <remarks>
            This method is the asynchronous version of <see cref="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SpeakTextAsync(System.String)"/>.
            </remarks>
            <param name="text">The plain text to synthesize.</param>
            <returns>A task representing the started synthesis operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.StartSpeakingSsmlAsync(System.String)">
            <summary>
            Queue speech synthesis task from SSML as an asynchronous operation.
            </summary>
            <remarks>
            This method is the asynchronous version of <see cref="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.SpeakSsmlAsync(System.String)"/>.
            </remarks>
            <param name="ssml">The SSML to synthesize.</param>
            <returns>A task representing the started synthesis operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.StopSpeakingAsync">
            <summary>
            Stop speech synthesis.
            </summary>
            <remarks>
            This method stops audio speech synthesis and discards any unread data in <see cref="T:Microsoft.CognitiveServices.Speech.Audio.PullAudioOutputStream"/>.
            </remarks>
            <returns>A task representing the asynchronous operation that stops the synthesis.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.GetVoicesAsync(System.String)">
            <summary>
            Get the available voices.
            Added in 1.16.0
            </summary>
            <param name="locale">Specify the locale of voices, in BCP-47 format; or leave it empty to get all available voices.</param>
            <returns>A task representing the getting voices list operation.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechSynthesizer.Dispose">
            <summary>
            Dispose of associated resources.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig">
            <summary>
            Speech translation configuration.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromSubscription(System.String,System.String)">
            <summary>
            Creates an instance of speech translation config with specified subscription key and region.
            </summary>
            <param name="subscriptionKey">The subscription key. To create or find your key and region, see [Find keys and region](/azure/cognitive-services/speech-service/overview#find-keys-and-region).</param>
            <param name="region">Region identifier for the given subscription key.</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromAuthorizationToken(System.String,System.String)">
            <summary>
            Creates an instance of the speech translation config with specified authorization token and region.
            </summary>
            <remarks>
            The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, you need to refresh it in the recognizer by passing a new valid token to
            the recognizer.
            </remarks>
            <param name="authorizationToken">The authorization token.</param>
            <param name="region">Region identifier for the given authorization token.</param>
            <returns>A speech config instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromEndpoint(System.Uri,System.String)">
            <summary>
            Creates an instance of the speech translation config with specified endpoint and subscription key.
            </summary>
            <remarks>
            * This method is only used for a non-standard resource path or parameter overrides. To change the host name with standard resource paths, use **FromHost** instead.
            * The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
              For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US", the language setting in URI takes precedence, and the effective language is "de-DE".
              Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            * If the endpoint requires a subscription key for authentication, use FromEndpoint(System.Uri, string) to pass the subscription key as parameter.
              To use an authorization token with FromEndpoint, use this method to create a SpeechTranslationConfig instance, and then set the AuthorizationToken property on the created SpeechTranslationConfig instance.
            </remarks>
            <param name="endpoint">The service endpoint to connect to.</param>
            <param name="subscriptionKey">The subscription key. To create or find your key and region, see [Find keys and region](/azure/cognitive-services/speech-service/overview#find-keys-and-region).</param>
            <returns>A SpeechTranslationConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromEndpoint(System.Uri)">
            <summary>
            Creates an instance of the speech translation config with specified endpoint.
            Added in 1.5.0
            </summary>
            <remarks>
            * This method is only used for a non-standard resource path or parameter overrides. To change the host name with standard resource paths, use **FromHost** instead.
            * The query parameters specified in the endpoint URI are not changed, even if they are set by any other APIs.
              For example, if the recognition language is defined in URI as query parameter "language=de-DE", and the property SpeechRecognitionLanguage is set to "en-US", the language setting in URI takes precedence, and the effective language is "de-DE".
              Only the parameters that are not specified in the endpoint URI can be set by other APIs.
            * If the endpoint requires a subscription key for authentication, use FromEndpoint(System.Uri, string) to pass the subscription key as parameter.
              To use an authorization token with FromEndpoint, use this method to create a SpeechTranslationConfig instance, and then set the AuthorizationToken property on the created SpeechTranslationConfig instance.
            </remarks>
            <param name="endpoint">The service endpoint to connect to.</param>
            <returns>A SpeechTranslationConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromHost(System.Uri,System.String)">
            <summary>
            Creates an instance of the speech translation config with specified host and subscription key. Added in 1.8.0
            </summary>
            <remarks>
            This method is intended only for users who use a non-default service host. Standard resource path will be assumed.
            For services with a non-standard resource path or no path at all, use FromEndpoint instead.
            Notes: 
            * Query parameters are not allowed in the host URI and must be set by other APIs.
            * To use an authorization token with **FromHost**, use **FromHost(System.Uri)**,
            and then set the **AuthorizationToken** property on the created **SpeechTranslationConfig** instance.
            </remarks>
            <param name="host">The service host to connect to. Format is "protocol://host:port" where ":port" is optional.</param>
            <param name="subscriptionKey">The subscription key. To create or find your key and region, see [Find keys and region](/azure/cognitive-services/speech-service/overview#find-keys-and-region).</param>
            <returns>A SpeechTranslationConfig instance.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.FromHost(System.Uri)">
            <summary>
            Creates an instance of the speech translation config with specified host. Added in 1.8.0
            </summary>
            <remarks>
            This method is intended only for users who use a non-default service host. Standard resource path will be assumed.
            For services with a non-standard resource path or no path at all, use **FromEndpoint** instead.
              
            To use an authorization token with **FromHost**, use this method to create a **SpeechTranslationConfig** instance, and then
            set the **AuthorizationToken** property on the created **SpeechTranslationConfig** instance.
              
            Notes:
            * Query parameters are not allowed in the host URI and must be set by other APIs.
            * If the host requires a subscription key for authentication, use **FromHost(System.Uri, string)** to pass the subscription key as parameter.
            </remarks>
            <param name="host">The service host to connect to. Format is `protocol://host:port` where `:port` is optional.</param>
            <returns>A SpeechTranslationConfig instance.</returns>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.TargetLanguages">
            <summary>
            Gets a collection of languages to translate to.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.AddTargetLanguage(System.String)">
            <summary>
            Adds a target languages of translation.
            </summary>
            <remarks>
            When speech synthesis is used and several target languages are specified for translation,
            the speech will be synthesized only for the first language.
            </remarks>
            <param name="language"></param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.RemoveTargetLanguage(System.String)">
            <summary>
            Removes a target languages of translation.
            Added in 1.7.0
            </summary>
            <param name="language"></param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationConfig.VoiceName">
            <summary>
            Specifies the name of voice tag if a synthesized audio output is desired.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SpeechTranslationModel">
            <summary>
            Speech translation model information.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationModel.Name">
            <summary>
            Gets the model name.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationModel.SourceLanguages">
            <summary>
            Gets the source languages that the model supports.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationModel.TargetLanguages">
            <summary>
            Gets the target languages that the model supports.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationModel.Path">
            <summary>
            Gets the model path (only valid for offline models).
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SpeechTranslationModel.Version">
            <summary>
            Gets the model version.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SpeechTranslationModel.Dispose(System.Boolean)">
            <summary>
            Clean up resources.
            </summary>
            <param name="disposing"></param>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.StreamStatus">
            <summary>
            Lists possible status values of an audio data stream.
            Added in 1.4.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.Unknown">
            <summary>
            The audio data stream status is unknown.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.NoData">
            <summary>
            The audio data stream contains no data.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.PartialData">
            <summary>
            The audio data stream contains partial data of a speak request.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.AllData">
            <summary>
            The audio data stream contains all data of a speak request.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.StreamStatus.Canceled">
            <summary>
            The audio data stream was cancelled.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult">
            <summary>
            Contains detailed information about the retrieved synthesis voices list.
            Added in 1.16.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult.Voices">
            <summary>
            Specifies voices list retrieved.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult.ResultId">
            <summary>
            Specifies unique ID of voices list result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult.Reason">
            <summary>
            Specifies status of voices list result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult.ErrorDetails">
            <summary>
            The error details.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult.Properties">
            <summary>
            Contains properties of the results.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.SynthesisVoicesResult.Dispose(System.Boolean)">
             <summary>
            
             </summary>
             <param name="disposeManaged"></param>
             <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.VoiceInfo">
            <summary>
            Contains detailed information about the synthesis voice.
            Updated in 1.17.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.Name">
            <summary>
            Gets the voice name.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.Locale">
            <summary>
            Gets the locale of the voice.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.ShortName">
            <summary>
            Gets the short name of the voice.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.LocalName">
            <summary>
            Gets the local name of the voice.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.Gender">
            <summary>
            Gets the voice gender.
            Added in version 1.17.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.VoiceType">
            <summary>
            Gets the voice type.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.StyleList">
            <summary>
            Gets the style list.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.VoicePath">
            <summary>
            Gets the voice path. (Only valid for offline voices.)
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.VoiceInfo.Properties">
            <summary>
            Gets result properties.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.VoiceInfo.Dispose(System.Boolean)">
             <summary>
            
             </summary>
             <param name="disposeManaged"></param>
             <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SynthesisVoiceGender">
            <summary>
            Lists synthesis voice gender.
            Added in version 1.17.0
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceGender.Unknown">
            <summary>
            Gender unknown.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceGender.Female">
            <summary>
            Female.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceGender.Male">
            <summary>
            Male.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.SynthesisVoiceType">
            <summary>
            Lists synthesis voice types.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceType.OnlineNeural">
            <summary>
            Online neural voice.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceType.OnlineStandard">
            <summary>
            Online standard voice.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceType.OfflineNeural">
            <summary>
            Offline neural voice.
            </summary>
        </member>
        <member name="F:Microsoft.CognitiveServices.Speech.SynthesisVoiceType.OfflineStandard">
            <summary>
            Offline standard voice.
            </summary>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult">
            <summary>
            Contains a translation result.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult.Translations">
            <summary>
            Contains translation results.
            </summary>
            <remarks>
            Each item in the dictionary represents translation results in one target language. The key
            is the name of the target language in BCP-47 format, and the value is the translation text in the specified language.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult.ToString">
            <summary>
            Returns a string that represents the speech recognition result.
            </summary>
            <returns>A string that represents the speech recognition result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer">
             <summary>
             Translates speech input into text and synthesized speech in one or more target languages.
             </summary>
             <remarks>
             See also: [Get started with speech translation](/azure/cognitive-services/speech-service/get-started-speech-translation)
             </remarks>
             <example>
             This example uses the translation recognizer from a microphone and receives events generated by the recognizer.
             <code language="c#">
             public async Task TranslationContinuousRecognitionAsync()
             {
                 // Creates an instance of a speech translation config with your subscription key and region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechTranslationConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 // Sets source and target languages.
                 string fromLanguage = "en-US";
                 config.SpeechRecognitionLanguage = fromLanguage;
                 config.AddTargetLanguage("de");
            
                 // Sets voice name of synthesis output.
                 const string GermanVoice = "Microsoft Server Speech Text to Speech Voice (de-DE, Hedda)";
                 config.VoiceName = GermanVoice;
                 // Creates a translation recognizer using microphone as audio input.
                 using (var recognizer = new TranslationRecognizer(config))
                 {
                     // Subscribes to events.
                     recognizer.Recognizing += (s, e) =&gt;
                     {
                         Console.WriteLine($"RECOGNIZING in '{fromLanguage}': Text={e.Result.Text}");
                         foreach (var element in e.Result.Translations)
                         {
                             Console.WriteLine($"    TRANSLATING into '{element.Key}': {element.Value}");
                         }
                     };
            
                     recognizer.Recognized += (s, e) =&gt;
                     {
                         if (e.Result.Reason == ResultReason.TranslatedSpeech)
                         {
                             Console.WriteLine($"\nFinal result: Reason: {e.Result.Reason.ToString()}, recognized text in {fromLanguage}: {e.Result.Text}.");
                             foreach (var element in e.Result.Translations)
                             {
                                 Console.WriteLine($"    TRANSLATING into '{element.Key}': {element.Value}");
                             }
                         }
                     };
            
                     recognizer.Synthesizing += (s, e) =&gt;
                     {
                         var audio = e.Result.GetAudio();
                         Console.WriteLine(audio.Length != 0
                             ? $"AudioSize: {audio.Length}"
                             : $"AudioSize: {audio.Length} (end of synthesis data)");
                     };
            
                     recognizer.Canceled += (s, e) =&gt;
                     {
                         Console.WriteLine($"\nRecognition canceled. Reason: {e.Reason}; ErrorDetails: {e.ErrorDetails}");
                     };
            
                     recognizer.SessionStarted += (s, e) =&gt;
                     {
                         Console.WriteLine("\nSession started event.");
                     };
            
                     recognizer.SessionStopped += (s, e) =&gt;
                     {
                         Console.WriteLine("\nSession stopped event.");
                     };
            
                     // Starts continuous recognition. 
                     // Uses StopContinuousRecognitionAsync() to stop recognition.
                     Console.WriteLine("Say something...");
                     await recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
            
                     do
                     {
                         Console.WriteLine("Press Enter to stop");
                     } while (Console.ReadKey().Key != ConsoleKey.Enter);
            
                     // Stops continuous recognition.
                     await recognizer.StopContinuousRecognitionAsync().ConfigureAwait(false);
                 }
             }
             </code>
             </example>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognizing"/> signals that an intermediate recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognized">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Recognized"/> signals that a final recognition result is received.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Canceled">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Canceled"/> signals that the speech to text/synthesis translation was canceled.
            </summary>
        </member>
        <member name="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Synthesizing">
            <summary>
            The event <see cref="E:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Synthesizing"/> signals that a translation synthesis result is received.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechTranslationConfig)">
            <summary>
            Creates a translation recognizer using the default microphone input for a specified translation configuration.
            </summary>
            <param name="config">Translation config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig)">
            <summary>
            Creates a translation recognizer using the default microphone input for a specified embedded speech configuration.
            </summary>
            <param name="config">Embedded speech config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig)">
            <summary>
            Creates a translation recognizer using the default microphone input for a specified hybrid speech configuration.
            </summary>
            <param name="config">Hybrid speech config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechTranslationConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a translation recognizer using the specified speech translator and audio configuration.
            </summary>
            <param name="config">Translation config.</param>
            <param name="audioConfig">Audio config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.EmbeddedSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a translation recognizer using the specified embedded speech translator and audio configuration.
            </summary>
            <param name="config">Embedded speech config.</param>
            <param name="audioConfig">Audio config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.HybridSpeechConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a translation recognizer using the specified hybrid speech translator and audio configuration.
            </summary>
            <param name="config">Hybrid speech config.</param>
            <param name="audioConfig">Audio config.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechTranslationConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig)">
            <summary>
            Creates a translation recognizer using the specified speech translator and auto detect source language config
            </summary>
            <param name="config">Translation config.</param>
            <param name="autoDetectSourceLanguageConfig">Configuration that specifies the language(s) to look for in the source speech to synthesize</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.#ctor(Microsoft.CognitiveServices.Speech.SpeechTranslationConfig,Microsoft.CognitiveServices.Speech.AutoDetectSourceLanguageConfig,Microsoft.CognitiveServices.Speech.Audio.AudioConfig)">
            <summary>
            Creates a translation recognizer using the specified speech translator and audio configuration.
            </summary>
            <param name="config">Translation config.</param>
            <param name="autoDetectSourceLanguageConfig">Configuration that specifies the language(s) to look for in the source speech to synthesize</param>
            <param name="audioConfig">Audio config.</param>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.SpeechRecognitionLanguage">
            <summary>
            Gets the language name that was set when the recognizer was created.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.TargetLanguages">
            <summary>
            Gets target languages for translation that were set when the recognizer was created.
            Each language is specified in BCP-47 format. The translation will provide translated text for each of language.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.VoiceName">
            <summary>
            Gets the name of output voice if speech synthesis is used.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Properties">
            <summary>
            The collection of properties and their values defined for this <see cref="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer"/>.
            Note: The property collection is only valid until the recognizer owning this Properties is disposed or finalized.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.AuthorizationToken">
            <summary>
            Gets or sets authorization token used to communicate with the service.
            </summary>
            <remarks>
            Note: The caller needs to ensure that the authorization token is valid. Before the authorization token
            expires, the caller needs to refresh it by calling this setter with a new valid token.
            Otherwise, the recognizer will encounter errors during recognition.
            </remarks>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.AddTargetLanguage(System.String)">
            <summary>
            Adds a target language for translation.
            Added in 1.7.0
            </summary>
            <param name="language">Translation target language to add.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.RemoveTargetLanguage(System.String)">
            <summary>
            Removes a target language for translation.
            Added in 1.7.0
            </summary>
            <param name="language">Translation target language to remove.</param>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.RecognizeOnceAsync">
             <summary>
             Starts speech translation as an asynchronous operation.
             </summary>
             <remarks>
             The end of a single utterance is determined by listening for silence at the end, or until a timeout period has elapsed.
             The task returns the translated version of the recognized speech in **TranslationRecognitionResult.Text**.
               
             You can call **StopContinuousRecognitionAsync** to stop recognition before a phrase has been recognized for translation.
               
             Since this method returns only a single utterance, it is suitable only for single shot recognition like command or query. 
             For long-running multi-utterance recognition, use **StartContinuousRecognitionAsync** instead.
            
             See also: [Get started with speech translation](/azure/cognitive-services/speech-service/get-started-speech-translation)
             </remarks>
             <returns>A task representing the recognition operation. The task returns a value of <see cref="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionResult"/> </returns>
             <example>
             Create a translation recognizer, get and print the recognition result
             <code language="c#">
             public async Task TranslationSingleShotRecognitionAsync()
             {
                 // Creates instance of a speech translation config with specified subscription key and region.
                 // Replace with your own subscription key and service region (e.g., "westus").
                 var config = SpeechTranslationConfig.FromSubscription("YourSubscriptionKey", "YourServiceRegion");
            
                 string fromLanguage = "en-US";
                 config.SpeechRecognitionLanguage = fromLanguage;
                 config.AddTargetLanguage("de");
            
                 // Creates a translation recognizer.
                 using (var recognizer = new TranslationRecognizer(config))
                 {
                     // Starts recognizing.
                     Console.WriteLine("Say something...");
            
                     // Starts translation recognition, and returns after a single utterance is recognized. 
                     // The end of a single utterance is determined by listening for silence at the end or
                     // until a timeout period has elapsed. The task returns the
                     // recognized text as well as the translation.
                     // 
                     // Note: Since RecognizeOnceAsync() returns only a single utterance, 
                     // it is suitable only for single shot recognition, like a command or query.
                     //
                     // For long-running multi-utterance recognition, 
                     // use StartContinuousRecognitionAsync() instead.
                     
                     var result = await recognizer.RecognizeOnceAsync();
            
                     if (result.Reason == ResultReason.TranslatedSpeech)
                     {
                         Console.WriteLine($"\nFinal result: Reason: {result.Reason.ToString()}, recognized text: {result.Text}.");
                         foreach (var element in result.Translations)
                         {
                             Console.WriteLine($"    TRANSLATING into '{element.Key}': {element.Value}");
                         }
                     }
                 }
             }
             </code>
             </example>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StartContinuousRecognitionAsync">
            <summary>
            Starts recognition and translation on a continous audio stream, until StopContinuousRecognitionAsync() is called.
            You must subscribe to events to receive translation results.
            </summary>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StopContinuousRecognitionAsync">
            <summary>
            Stops a running recognition operation as soon as possible and immediately requests a result based on the
            the input that has been processed so far. This works for all recognition operations, not just continuous
            ones, and facilitates the use of push-to-talk or "finish now" buttons for manual audio endpointing.
            </summary>
            <returns>
            A task that will complete when input processing has been stopped. Result generation, if applicable for the
            input provided, may happen after this task completes and should be handled with the appropriate event.
            </returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StartKeywordRecognitionAsync(Microsoft.CognitiveServices.Speech.KeywordRecognitionModel)">
            <summary>
            Configures the recognizer with the given keyword model. After calling this method, the recognizer is listening 
            for the keyword to start the recognition. Call StopKeywordRecognitionAsync() to end the keyword initiated recognition.
            You must subscribe to events to receive recognition results.
            </summary>
            <remarks>
            See also: [Use a keyword model with the SDK](/azure/cognitive-services/speech-service/custom-keyword-basics?pivots=programming-language-csharp#use-a-keyword-model-with-the-sdk)
            </remarks>
            <param name="model">The keyword recognition model that specifies the keyword to be recognized.</param>
            <returns>A task representing the asynchronous operation that starts the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.StopKeywordRecognitionAsync">
            <summary>
            Ends the keyword initiated recognition.
            </summary>
            <returns>A task representing the asynchronous operation that stops the recognition.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognizer.Dispose(System.Boolean)">
            <summary>
            Disposes of the object.
            </summary>
            <param name="disposing">True to dispose managed resources.</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult">
            <summary>
            Contains the voice output of the translated text in the target language.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult.Reason">
            <summary>
            Gets the possible reasons a TranslationSynthesisResult might be generated.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult.GetAudio">
            <summary>
            Voice output of the translated text in the target language.
            </summary>
            <returns>Synthesized audio data.</returns>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisResult.ToString">
            <summary>
            Returns a string that represents the synthesis result.
            </summary>
            <returns>A string that represents the synthesis result.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisEventArgs">
            <summary>
            Contains payload of translation synthesis result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisEventArgs.Result">
            <summary>
            Specifies the translation synthesis result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationSynthesisEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionEventArgs">
            <summary>
            Contains payload of translation recognizing/recognized events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionEventArgs.Result">
            <summary>
            Specifies the recognition result.
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
        <member name="T:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs">
            <summary>
            Contains payload of translation text result recognition canceled result events.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.Reason">
            <summary>
            The reason the recognition was canceled.
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.ErrorCode">
            <summary>
            The error code in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.Reason"/> is set to Error).
            If Reason is not Error, ErrorCode returns NoError.
            Added in 1.1.0
            </summary>
        </member>
        <member name="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.ErrorDetails">
            <summary>
            The error message in case of an unsuccessful recognition (<see cref="P:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.Reason"/> is set to Error).
            </summary>
        </member>
        <member name="M:Microsoft.CognitiveServices.Speech.Translation.TranslationRecognitionCanceledEventArgs.ToString">
            <summary>
            Returns a string that represents the speech recognition result event.
            </summary>
            <returns>A string that represents the speech recognition result event.</returns>
        </member>
    </members>
</doc>
